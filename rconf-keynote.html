<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />



<meta name="date" content="2021-11-07" />

<title>딥러닝</title>

<script src="rconf-keynote_files/header-attrs-2.11/header-attrs.js"></script>
<script src="rconf-keynote_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="rconf-keynote_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="rconf-keynote_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="rconf-keynote_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="rconf-keynote_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="rconf-keynote_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="rconf-keynote_files/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="rconf-keynote_files/tocify-1.9.1/jquery.tocify.js"></script>
<script src="rconf-keynote_files/navigation-1.1/tabsets.js"></script>
<script src="rconf-keynote_files/navigation-1.1/codefolding.js"></script>
<script src="rconf-keynote_files/core-js-2.5.3/shim.min.js"></script>
<script src="rconf-keynote_files/react-17.0.0/react.min.js"></script>
<script src="rconf-keynote_files/react-17.0.0/react-dom.min.js"></script>
<script src="rconf-keynote_files/reactwidget-1.0.0/react-tools.js"></script>
<script src="rconf-keynote_files/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="rconf-keynote_files/reactable-binding-0.2.3/reactable.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>


<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */

.sourceCode .row {
  width: 100%;
}
.sourceCode {
  overflow-x: auto;
}
.code-folding-btn {
  margin-right: -30px;
}
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>







<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">



<!-- <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" /> 
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" type="text/css" href="css/bootstrap/bootstrap.css" />
<link rel="stylesheet" type="text/css" href="css/bootstrap/bootstrap-theme.css" />
<link rel="stylesheet" type="text/css" href="css/swc.css" />  -->
<link rel="alternate" type="application/rss+xml" title="Tidyverse Korea" href="https://www.facebook.com/groups/tidyverse/"/>
<meta charset="UTF-8" />

<!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
<!--[if lt IE 9]>
  <script src="https://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-59802572-23"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-59802572-23');
</script>

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">딥러닝</h1>
<h3 class="subtitle">한국 R 컨퍼런스 - Julia Silge Keynote번역</h3>
<h4 class="author">이광춘</h4>
<address class="author_afil">
<a href="https://www.facebook.com/groups/tidyverse/">Tidyverse Korea</a><br><h4 class="date">2021-11-07</h4>

</div>


<p><img src="fig/rconf_keynote_translation.jpg" /></p>
<div id="audio-extraction" class="section level1" number="1">
<h1 number="1"><span class="header-section-number">1</span> 동영상 → 오디오 추출</h1>
<p>동영상에서 오디오를 추출해서 Speech to Text (STT)가 가능한 <code>.flac</code> 형태로 저장한다. 유튜브 동영상에서 오디오를 추출하는 사례를 <a href="https://statkclee.github.io/deep-learning/r-stt.html">speech-to-text - 음성을 텍스트로 변환</a>에서 자세한 사항 참조한다.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="kw">library</span>(embedr)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a><span class="kw">embed_audio</span>(<span class="st">&quot;data/julia_silge/15_silge.mp3&quot;</span>)</span></code></pre></div>
<p><audio controls> <source src='data/julia_silge/15_silge.mp3' type='audio/mpeg'> Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp </audio></p>
</div>
<div id="audio-extraction-stt" class="section level1" number="2">
<h1 number="2"><span class="header-section-number">2</span> 오디오 → 텍스트 (STT)</h1>
<p><a href="https://cloud.google.com/speech-to-text/">Cloud Speech-to-Text API</a>를 사용해서 오디오에서 텍스트를 추출한다. 하지만 1분이 넘어가는 경우 제약이 있기 때문에 Google Cloud Storage에 앞서 추출한 <code>.flac</code> 파일을 GCS 버킷에 넣어 사용해야만 한다. 추출한 오디오 <code>.flac</code> 파일을 GCS 버킷에 담는 자세한 과정은 <a href="https://statkclee.github.io/deep-learning/r-google-storage.html">Google Cloud Storage - googleCloudStorageR</a>을 참조한다.</p>
<p>구글 STT API를 활용하게 되면 오디오 파일을 입력으로 받아 추출한 텍스트와 시점정보를 함께 반환한다. <code>객체명$transcript</code>에 오디오에서 추출된 텍스트, <code>객체명$timings</code>에 추출된 텍스트 시점정보가 함께 저장되어 있다.</p>
<p>약 45분 오디오를 구글 STT에서 처리하는데 2,820 초가 소요되었다.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="kw">library</span>(googleLanguageR)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>kt_config &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">encoding =</span> <span class="st">&quot;FLAC&quot;</span>,</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>                  <span class="dt">audioChannelCount =</span> <span class="dv">2</span>,</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a>                  <span class="dt">diarizationConfig =</span> <span class="kw">list</span>(</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a>                    <span class="dt">enableSpeakerDiarization =</span> <span class="ot">TRUE</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a>                  ))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true"></a>julia_gcs &lt;-<span class="st"> &quot;gs://julia_silge/15_silge.flac&quot;</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true"></a>julia_tts &lt;-<span class="st">  </span><span class="kw">gl_speech</span>(julia_gcs, <span class="dt">languageCode =</span> <span class="st">&quot;en-US&quot;</span>, <span class="dt">sampleRateHertz =</span> 44100L, <span class="dt">asynch =</span> <span class="ot">TRUE</span>, </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true"></a>                        <span class="dt">customConfig =</span> kt_config)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true"></a>julia_tts_res &lt;-<span class="st"> </span><span class="kw">gl_speech_op</span>(julia_tts)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true"></a><span class="co"># 2021-11-05 15:49:59 -- Asynchronous transcription finished.</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true"></a><span class="co"># 2021-11-05 15:49:59 -- Speech transcription finished. Total billed time: 2820s</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true"></a>julia_stt_tbl &lt;-<span class="st"> </span>julia_tts_res<span class="op">$</span>transcript <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true"></a><span class="st">  </span><span class="kw">as_tibble</span>()</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true"></a>julia_stt_tbl <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true"></a><span class="st">  </span><span class="kw">write_rds</span>(<span class="st">&quot;data/julia_silge/julia_stt_tbl.rds&quot;</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true"></a>julia_stt_timing_list &lt;-<span class="st"> </span>julia_tts_res<span class="op">$</span>timings </span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true"></a>julia_stt_timing_list <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true"></a><span class="st">  </span><span class="kw">write_rds</span>(<span class="st">&quot;data/julia_silge/julia_stt_timing_list.rds&quot;</span>)</span></code></pre></div>
<div id="stt-raw-text" class="section level2" number="2.1">
<h2 number="2.1"><span class="header-section-number">2.1</span> STT 기계추출 → 텍스트</h2>
<p>추출한 텍스트와 전문은 다음과 같다. <a href="data/julia_silge/julia_stt_raw_text.txt">다운로드: STT 원본 TEXT</a></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="kw">library</span>(reactable)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a>julia_stt_tbl &lt;-<span class="st">  </span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a><span class="st">  </span><span class="kw">read_rds</span>(<span class="st">&quot;data/julia_silge/julia_stt_tbl.rds&quot;</span>) </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a>julia_stt_raw_text &lt;-<span class="st"> </span>julia_stt_tbl <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">stt_raw_text =</span> <span class="kw">paste</span>(transcript, <span class="dt">collapse =</span> <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>))</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a><span class="co"># julia_stt_raw_text %&gt;% </span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a><span class="co">#   write_lines(&quot;data/julia_silge/julia_stt_raw_text.txt&quot;)</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true"></a><span class="kw">str_sub</span>(julia_stt_raw_text, <span class="dv">1</span>, <span class="dv">1000</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true"></a><span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true"></a><span class="st">    </span>reactable<span class="op">::</span><span class="kw">reactable</span>(</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true"></a>    <span class="dt">defaultColDef =</span> <span class="kw">colDef</span>(</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true"></a>      <span class="dt">header =</span> <span class="cf">function</span>(value) <span class="kw">gsub</span>(<span class="st">&quot;.&quot;</span>, <span class="st">&quot; &quot;</span>, value, <span class="dt">fixed =</span> <span class="ot">TRUE</span>),</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true"></a>      <span class="dt">cell =</span> <span class="cf">function</span>(value) <span class="kw">format</span>(value, <span class="dt">nsmall =</span> <span class="dv">1</span>),</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true"></a>      <span class="dt">align =</span> <span class="st">&quot;center&quot;</span>,</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true"></a>      <span class="dt">minWidth =</span> <span class="dv">70</span>,</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true"></a>      <span class="dt">headerStyle =</span> <span class="kw">list</span>(<span class="dt">background =</span> <span class="st">&quot;#f7f7f8&quot;</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true"></a>  ),</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true"></a>  <span class="dt">bordered =</span> <span class="ot">TRUE</span>,</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true"></a>  <span class="dt">highlight =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<div id="htmlwidget-e365b1c5ef49ddde5888" class="reactable html-widget" style="width:auto;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-e365b1c5ef49ddde5888">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"value":["hi my name is Julius Elite I'm a data scientist and software engineer at our studio and I'd like to thank of the or the organizers of the our user groups in Korea so much for having me today to speak to you I'm so happy to be speaking specifically today about creating features for machine learning from texting up for a couple of reasons having a better understanding of what we do to take Text data and then to make it appropriate as I'm in put for machine learning algorithm has many benefits both if you are directly getting ready to train a model or if you're at the beginning of some text analysis project I'm or if you are trying to understand the behavior of a model that you're interacting with some way which is something that we do in our work and say the scientist or in are you in our\nhe lives more and more so when we build models for text either super vised or unsupervised we start with something like this this is some example Text data that I'll use a couple of times during the sto"]},"columns":[{"accessor":"value","name":"value","type":"character","cell":["hi my name is Julius Elite I'm a data scientist and software engineer at our studio and I'd like to thank of the or the organizers of the our user groups in Korea so much for having me today to speak to you I'm so happy to be speaking specifically today about creating features for machine learning from texting up for a couple of reasons having a better understanding of what we do to take Text data and then to make it appropriate as I'm in put for machine learning algorithm has many benefits both if you are directly getting ready to train a model or if you're at the beginning of some text analysis project I'm or if you are trying to understand the behavior of a model that you're interacting with some way which is something that we do in our work and say the scientist or in are you in our\nhe lives more and more so when we build models for text either super vised or unsupervised we start with something like this this is some example Text data that I'll use a couple of times during the sto"],"header":"value","minWidth":70,"align":"center","headerStyle":{"background":"#f7f7f8"}}],"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"highlight":true,"bordered":true,"dataKey":"ddeb56b07f52c4f57472a25fafe0ba62","key":"ddeb56b07f52c4f57472a25fafe0ba62"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
</div>
<div id="stt-raw-text-timing" class="section level2" number="2.2">
<h2 number="2.2"><span class="header-section-number">2.2</span> STT 기계추출 → 시간</h2>
<p>다음으로 중요한 것은 시점정보다. 이를 위해서 총 45분 강연이라 5분 단위로 짤라 9개로 쪼개 추후 자연어 처리가 가능하도록 조치한다.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a>julia_stt_timing_list &lt;-<span class="st"> </span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">read_rds</span>(<span class="st">&quot;data/julia_silge/julia_stt_timing_list.rds&quot;</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a>julia_stt_timing_tbl &lt;-<span class="st"> </span><span class="kw">map_df</span>(julia_stt_timing_list, rbind) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>speakerTag)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true"></a>julia_stt_timing_five_tbl &lt;-<span class="st"> </span>julia_stt_timing_tbl <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">startTime =</span> <span class="kw">parse_number</span>(startTime),</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true"></a>         <span class="dt">endTime   =</span> <span class="kw">parse_number</span>(endTime)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true"></a><span class="st">  </span><span class="co"># 45 분 / 9 구간 = 5 분/구간</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">nine_interval =</span> <span class="kw">cut</span>( startTime, </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true"></a>                              <span class="dt">breaks =</span> <span class="kw">unique</span>(<span class="kw">quantile</span>(startTime, <span class="dt">probs =</span> <span class="kw">seq.int</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">by =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">9</span>))), </span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true"></a>                                              <span class="dt">include.lowest=</span><span class="ot">TRUE</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true"></a><span class="st">  </span><span class="kw">group_by</span>(nine_interval) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true"></a><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">five_min_text =</span> <span class="kw">paste</span>(word, <span class="dt">collapse =</span> <span class="st">&quot; &quot;</span>))</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true"></a>julia_stt_timing_five_tbl <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true"></a><span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true"></a><span class="st">  </span>reactable<span class="op">::</span><span class="kw">reactable</span>(</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true"></a>    <span class="dt">defaultColDef =</span> <span class="kw">colDef</span>(</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true"></a>      <span class="dt">header =</span> <span class="cf">function</span>(value) <span class="kw">gsub</span>(<span class="st">&quot;.&quot;</span>, <span class="st">&quot; &quot;</span>, value, <span class="dt">fixed =</span> <span class="ot">TRUE</span>),</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true"></a>      <span class="dt">cell =</span> <span class="cf">function</span>(value) <span class="kw">format</span>(value, <span class="dt">nsmall =</span> <span class="dv">1</span>),</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true"></a>      <span class="dt">align =</span> <span class="st">&quot;center&quot;</span>,</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true"></a>      <span class="dt">minWidth =</span> <span class="dv">70</span>,</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true"></a>      <span class="dt">headerStyle =</span> <span class="kw">list</span>(<span class="dt">background =</span> <span class="st">&quot;#f7f7f8&quot;</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true"></a>  ),</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true"></a>  <span class="dt">columns =</span> <span class="kw">list</span>(</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true"></a>    <span class="dt">nine_interval =</span> <span class="kw">colDef</span>(<span class="dt">minWidth =</span> <span class="dv">20</span>),</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true"></a>    <span class="dt">five_min_text =</span> <span class="kw">colDef</span>(<span class="dt">minWidth =</span> <span class="dv">140</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true"></a>  ),</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true"></a>  <span class="dt">bordered =</span> <span class="ot">TRUE</span>,</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true"></a>  <span class="dt">highlight =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<div id="htmlwidget-d43bf9d332fb5c8a26a9" class="reactable html-widget" style="width:auto;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-d43bf9d332fb5c8a26a9">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"nine_interval":["[0.9,311]"],"five_min_text":["hi my name is Julius Elite I'm a data scientist and software engineer at our studio and I'd like to thank of the or the organizers of the our user groups in Korea so much for having me today to speak to you I'm so happy to be speaking specifically today about creating features for machine learning from texting up for a couple of reasons having a better understanding of what we do to take Text data and then to make it appropriate as I'm in put for machine learning algorithm has many benefits both if you are directly getting ready to train a model or if you're at the beginning of some text analysis project I'm or if you are trying to understand the behavior of a model that you're interacting with some way which is something that we do in our work and say the scientist or in are you in our he lives more and more so when we build models for text either super vised or unsupervised we start with something like this this is some example Text data that I'll use a couple of times during the stocks that describes some animals some animals I'm using some Text data to me as an English speaker looks familiar I got my eye on someone who uses a human language so I look at this and I can read it I could speak out loud and I understand I can interpret it what it means do this kind of data to sort of natural-language data is being generated all the time in all kinds of languages in all kinds of contacts so what are you work in healthcare I'm in Tech in finance basically any kind of organization to sort of Text data is being generated by by by customers by by clients by internal stakeholders inside of Business by people taking surveys by social media business processes and in all this natural language there's information latent in that text that can be used to make better decisions however computers are not great at at you know looking at this and doing math on language as a Representatives like this and instead language has to be transformed magically transformed to some kind of machine readable numeric representation that looks more like this what I'm showing her on the screen to be ready for almost any kind of model and so I spent a fair amount of time working on software for people to be able to do exploratory data analysis visualization summarization pass like that with texting apps in a Thai tea format where we have one observation per row and I love using tidy data principles for text analysis especially during those exploratory phases of analysis when it comes time to build a model often what the underlying mathematical implementation really needs is typically something like this which is a way to do this particular representation is called the documents her Matrix exact representation May differ from what I shown here what I have here is we're waiting things by counts so each row in this Matrix is a document each column is what is a word a token and the numbers represent counts how many times of each document used each word you could wait it in a different way using say tf-idf instead of cows or you might teach sequence information if you are interested in building a deep learning model but basically for all kinds of text modeling from simpler models like naive Bayes models which work well for tax to word embeddings to really the most state-of-the-art kind of work that's happening today like a Transformers 4 Text data we have two heavily feature engineer and process language to get it to some kind of representation if it's suitable for machine learning algorithms do I work on an open-source framework in our for modeling and machine learning this called the models and the examples that I'll be showing today used Heidi model code some of the specifics of the project are to provide a consistent flexible framework for real world hi my name is Julius Elite I'm a data scientist and software engineer at our studio and I'd like to thank of the or the organizers of the our user groups in Korea so much for having me today to speak to you I'm so happy to be speaking specifically today about creating features for machine learning from texting up for a couple of reasons having a better understanding of what we do to take Text data and then to make it appropriate as I'm in put for machine learning algorithm has many benefits both if you are directly getting ready to train a model or if you're at the beginning of some text analysis project I'm or if you are trying to understand the behavior of a model that you're interacting with some way which is something that we do in our work and say the scientist or in are you in our he lives more and more so when we build models for text either super vised or unsupervised we start with something like this this is some example Text data that I'll use a couple of times during the stocks that describes some animals some animals I'm using some Text data to me as an English speaker looks familiar I got my eye on someone who uses a human language so I look at this and I can read it I could speak out loud and I understand I can interpret it what it means do this kind of data to sort of natural-language data is being generated all the time in all kinds of languages in all kinds of contacts so what are you work in healthcare I'm in Tech in finance basically any kind of organization to sort of Text data is being generated by by by customers by by clients by internal stakeholders inside of Business by people taking surveys by social media business processes and in all this natural language there's information latent in that text that can be used to make better decisions however computers are not great at at you know looking at this and doing math on language as a Representatives like this and instead language has to be transformed magically transformed to some kind of machine readable numeric representation that looks more like this what I'm showing her on the screen to be ready for almost any kind of model and so I spent a fair amount of time working on software for people to be able to do exploratory data analysis visualization summarization pass like that with texting apps in a Thai tea format where we have one observation per row and I love using tidy data principles for text analysis especially during those exploratory phases of analysis when it comes time to build a model often what the underlying mathematical implementation really needs is typically something like this which is a way to do this particular representation is called the documents her Matrix exact representation May differ from what I shown here what I have here is we're waiting things by counts so each row in this Matrix is a document each column is what is a word a token and the numbers represent counts how many times of each document used each word you could wait it in a different way using say tf-idf instead of cows or you might teach sequence information if you are interested in building a deep learning model but basically for all kinds of text modeling from simpler models like naive Bayes models which work well for tax to word embeddings to really the most state-of-the-art kind of work that's happening today like a Transformers 4 Text data we have two heavily feature engineer and process language to get it to some kind of representation if it's suitable for machine learning algorithms do I work on an open-source framework in our for modeling and machine learning this called the models and the examples that I'll be showing today used Heidi model code some of the specifics of the project are to provide a consistent flexible framework for real world"]},"columns":[{"accessor":"nine_interval","name":"nine_interval","type":"factor","cell":["[0.9,311]"],"header":"nine_interval","minWidth":20,"align":"center","headerStyle":{"background":"#f7f7f8"}},{"accessor":"five_min_text","name":"five_min_text","type":"character","cell":["hi my name is Julius Elite I'm a data scientist and software engineer at our studio and I'd like to thank of the or the organizers of the our user groups in Korea so much for having me today to speak to you I'm so happy to be speaking specifically today about creating features for machine learning from texting up for a couple of reasons having a better understanding of what we do to take Text data and then to make it appropriate as I'm in put for machine learning algorithm has many benefits both if you are directly getting ready to train a model or if you're at the beginning of some text analysis project I'm or if you are trying to understand the behavior of a model that you're interacting with some way which is something that we do in our work and say the scientist or in are you in our he lives more and more so when we build models for text either super vised or unsupervised we start with something like this this is some example Text data that I'll use a couple of times during the stocks that describes some animals some animals I'm using some Text data to me as an English speaker looks familiar I got my eye on someone who uses a human language so I look at this and I can read it I could speak out loud and I understand I can interpret it what it means do this kind of data to sort of natural-language data is being generated all the time in all kinds of languages in all kinds of contacts so what are you work in healthcare I'm in Tech in finance basically any kind of organization to sort of Text data is being generated by by by customers by by clients by internal stakeholders inside of Business by people taking surveys by social media business processes and in all this natural language there's information latent in that text that can be used to make better decisions however computers are not great at at you know looking at this and doing math on language as a Representatives like this and instead language has to be transformed magically transformed to some kind of machine readable numeric representation that looks more like this what I'm showing her on the screen to be ready for almost any kind of model and so I spent a fair amount of time working on software for people to be able to do exploratory data analysis visualization summarization pass like that with texting apps in a Thai tea format where we have one observation per row and I love using tidy data principles for text analysis especially during those exploratory phases of analysis when it comes time to build a model often what the underlying mathematical implementation really needs is typically something like this which is a way to do this particular representation is called the documents her Matrix exact representation May differ from what I shown here what I have here is we're waiting things by counts so each row in this Matrix is a document each column is what is a word a token and the numbers represent counts how many times of each document used each word you could wait it in a different way using say tf-idf instead of cows or you might teach sequence information if you are interested in building a deep learning model but basically for all kinds of text modeling from simpler models like naive Bayes models which work well for tax to word embeddings to really the most state-of-the-art kind of work that's happening today like a Transformers 4 Text data we have two heavily feature engineer and process language to get it to some kind of representation if it's suitable for machine learning algorithms do I work on an open-source framework in our for modeling and machine learning this called the models and the examples that I'll be showing today used Heidi model code some of the specifics of the project are to provide a consistent flexible framework for real world hi my name is Julius Elite I'm a data scientist and software engineer at our studio and I'd like to thank of the or the organizers of the our user groups in Korea so much for having me today to speak to you I'm so happy to be speaking specifically today about creating features for machine learning from texting up for a couple of reasons having a better understanding of what we do to take Text data and then to make it appropriate as I'm in put for machine learning algorithm has many benefits both if you are directly getting ready to train a model or if you're at the beginning of some text analysis project I'm or if you are trying to understand the behavior of a model that you're interacting with some way which is something that we do in our work and say the scientist or in are you in our he lives more and more so when we build models for text either super vised or unsupervised we start with something like this this is some example Text data that I'll use a couple of times during the stocks that describes some animals some animals I'm using some Text data to me as an English speaker looks familiar I got my eye on someone who uses a human language so I look at this and I can read it I could speak out loud and I understand I can interpret it what it means do this kind of data to sort of natural-language data is being generated all the time in all kinds of languages in all kinds of contacts so what are you work in healthcare I'm in Tech in finance basically any kind of organization to sort of Text data is being generated by by by customers by by clients by internal stakeholders inside of Business by people taking surveys by social media business processes and in all this natural language there's information latent in that text that can be used to make better decisions however computers are not great at at you know looking at this and doing math on language as a Representatives like this and instead language has to be transformed magically transformed to some kind of machine readable numeric representation that looks more like this what I'm showing her on the screen to be ready for almost any kind of model and so I spent a fair amount of time working on software for people to be able to do exploratory data analysis visualization summarization pass like that with texting apps in a Thai tea format where we have one observation per row and I love using tidy data principles for text analysis especially during those exploratory phases of analysis when it comes time to build a model often what the underlying mathematical implementation really needs is typically something like this which is a way to do this particular representation is called the documents her Matrix exact representation May differ from what I shown here what I have here is we're waiting things by counts so each row in this Matrix is a document each column is what is a word a token and the numbers represent counts how many times of each document used each word you could wait it in a different way using say tf-idf instead of cows or you might teach sequence information if you are interested in building a deep learning model but basically for all kinds of text modeling from simpler models like naive Bayes models which work well for tax to word embeddings to really the most state-of-the-art kind of work that's happening today like a Transformers 4 Text data we have two heavily feature engineer and process language to get it to some kind of representation if it's suitable for machine learning algorithms do I work on an open-source framework in our for modeling and machine learning this called the models and the examples that I'll be showing today used Heidi model code some of the specifics of the project are to provide a consistent flexible framework for real world"],"header":"five_min_text","minWidth":140,"align":"center","headerStyle":{"background":"#f7f7f8"}}],"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"highlight":true,"bordered":true,"dataKey":"8d679f76c366eb662cc7d3222cd5dfb7","key":"8d679f76c366eb662cc7d3222cd5dfb7"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<div id="srt-file" class="section level1" number="3">
<h1 number="3"><span class="header-section-number">3</span> <code>.srt</code> 파일</h1>
<p>구글 STT의 경우 상세한 시점정보가 제공되나 단어 STT 시간과 거의 1분 단위로 기계적으로 쪼개진 정보만 제공되고 있다. <a href="https://statkclee.github.io/ingest-data/ingest-srt.html">데이터 가져오기 - 유튜브, 영화자막</a>을 참고하여 자막정보를 만들어보자.</p>
<p>자막 <code>.srt</code> 파일을 제작하는 다른 방식은 유튜브 자막 자동생성 기능을 활용하는 것이다. 일단 동영상을 업로드하고 공개로 설정을 해주면 기계가 자동으로 STT 를 수행하여 자막을 추출할 수 있게 도움을 준다.</p>
<p><img src="fig/youtube-auto-cc.jpg" width="394" /></p>
</div>
<div id="srt-file-proof-reading" class="section level1" number="4">
<h1 number="4"><span class="header-section-number">4</span> <code>.srt</code> 파일 교정</h1>
<p><a href="https://github.com/SubtitleEdit/subtitleedit/releases">Subtitle Edit</a> 최신 버전을 설치하여 구글 STT 기능으로 추출된 텍스트를 교정한다. 구글 STT는 아무래도 R 데이터 과학 전문용어에는 한계가 있기 때문에 <code>RStudio</code>, <code>dplyr</code>와 같은 용어오 <code>stopwords</code>, <code>n-gram</code>와 같은 NLP 전문용어 그리고 <code>um</code> 의성어 등도 있는 그대로 텍스트로 떨구기 때문에 이런 용어를 교정할 필요가 있다. <a href="data/julia_silge/15_silge_english.srt">교정한 영문 SRT 파일</a></p>
<p><img src="fig/youtube-subtitleedit.jpg" /></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="kw">library</span>(srt)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a>srt_eng_raw &lt;-<span class="st"> </span><span class="kw">read_lines</span>(<span class="st">&quot;data/julia_silge/15_silge_english.srt&quot;</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a>srt_eng_text &lt;-<span class="st"> </span>srt_eng_raw[srt_eng_raw <span class="op">!=</span><span class="st"> &quot;&quot;</span>]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a>srt_eng_text[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</span></code></pre></div>
<pre><code> [1] &quot;1&quot;                                                                    
 [2] &quot;00:00:01,120 --&gt; 00:00:08,639&quot;                                        
 [3] &quot;Hi my name is julia silge. &quot;                                          
 [4] &quot;I&#39;m a data scientist and software engineer at RStudio.&quot;               
 [5] &quot;2&quot;                                                                    
 [6] &quot;00:00:06,080 --&gt; 00:00:13,519&quot;                                        
 [7] &quot;and I&#39;d like to thank the the organizers of the R user group in Korea&quot;
 [8] &quot;3&quot;                                                                    
 [9] &quot;00:00:11,200 --&gt; 00:00:18,080&quot;                                        
[10] &quot;so much for having me. &quot;                                              </code></pre>
<p><code>read_srt()</code> 함수를 사용하여 전체 내용을 살펴보면 다음과 같다.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a>(keynote &lt;-<span class="st"> </span><span class="kw">read_srt</span>(<span class="dt">path =</span> <span class="st">&quot;data/julia_silge/15_silge_english.srt&quot;</span>, <span class="dt">collapse =</span> <span class="st">&quot; &quot;</span>))</span></code></pre></div>
<pre><code># A tibble: 885 x 4
       n start   end subtitle                                                   
   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                                                      
 1     1  1.12  8.64 Hi my name is julia silge.  I&#39;m a data scientist and softw~
 2     2  6.08 13.5  and I&#39;d like to thank the the organizers of the R user gro~
 3     3 11.2  18.1  so much for having me.  Today speak to you                 
 4     4 15.5  24    I am so happy to be speaking specifically today about crea~
 5     5 21.1  29.4  features for machine learning from text data for a couple ~
 6     6 27.2  34.5  Having a better understanding of what we do to take text d~
 7     7 31.7  36.9  and then to make it appropriate                            
 8     8 34.5  42.9  as an input for machine learning algorithms has many benef~
 9     9 39.7  45.0  both if you are directly getting ready                     
10    10 42.9  50.7  to train a model or if you&#39;re at the beginning of some tex~
# ... with 875 more rows</code></pre>
</div>
<div id="srt-file-translation" class="section level1" number="5">
<h1 number="5"><span class="header-section-number">5</span> 영문 <code>.srt</code> 파일 번역</h1>
<div id="full-text-translation" class="section level2" number="5.1">
<h2 number="5.1"><span class="header-section-number">5.1</span> 전체 통번역</h2>
<p>먼저 텍스트를 하나로 묶어 이를 전체 문자 벡터로 만든 후에 이를 구글 번역기에 넣어 한글로 번역한다.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a><span class="kw">library</span>(googleLanguageR)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true"></a>eng_plain_text &lt;-<span class="st"> </span>keynote <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true"></a><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">eng_text =</span> <span class="kw">paste</span>(subtitle, <span class="dt">collapse =</span> <span class="st">&quot; &quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">pull</span>()</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true"></a>kor_translated_text &lt;-<span class="st"> </span>googleLanguageR<span class="op">::</span><span class="kw">gl_translate</span>(eng_plain_text, <span class="dt">target =</span> <span class="st">&quot;ko&quot;</span>, <span class="dt">source =</span> <span class="st">&quot;en&quot;</span>, <span class="dt">model =</span> <span class="st">&quot;nmt&quot;</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true"></a><span class="co"># 2021-11-07 10:53:00 -- Translating text: 33505 characters - </span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true"></a>kor_translated_text <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true"></a><span class="st">  </span><span class="kw">write_rds</span>(<span class="st">&quot;data/julia_silge/full_ko_translation.rds&quot;</span>)</span></code></pre></div>
<p>영어 원문과 구글 번역기를 돌려 나온 결과물을 나란히 놓고 비교해보자.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true"></a>kor_translated_text &lt;-<span class="st"> </span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">read_rds</span>(<span class="st">&quot;data/julia_silge/full_ko_translation.rds&quot;</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true"></a>kor_translated_text <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">select</span>(text, translatedText) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true"></a><span class="st">  </span>reactable<span class="op">::</span><span class="kw">reactable</span>(</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true"></a>    <span class="dt">defaultColDef =</span> <span class="kw">colDef</span>(</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true"></a>      <span class="dt">header =</span> <span class="cf">function</span>(value) <span class="kw">gsub</span>(<span class="st">&quot;.&quot;</span>, <span class="st">&quot; &quot;</span>, value, <span class="dt">fixed =</span> <span class="ot">TRUE</span>),</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true"></a>      <span class="dt">cell =</span> <span class="cf">function</span>(value) <span class="kw">format</span>(value, <span class="dt">nsmall =</span> <span class="dv">1</span>),</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true"></a>      <span class="dt">align =</span> <span class="st">&quot;center&quot;</span>,</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true"></a>      <span class="dt">minWidth =</span> <span class="dv">70</span>,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true"></a>      <span class="dt">headerStyle =</span> <span class="kw">list</span>(<span class="dt">background =</span> <span class="st">&quot;#f7f7f8&quot;</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true"></a>  ),</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true"></a>  <span class="co"># columns = list(</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true"></a>  <span class="co">#   translatedText = colDef(minWidth = 100),</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true"></a>  <span class="co">#   text = colDef(minWidth = 100)</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true"></a>  <span class="co"># ),</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true"></a>  <span class="dt">bordered =</span> <span class="ot">TRUE</span>,</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true"></a>  <span class="dt">highlight =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<div id="htmlwidget-06dce0e1fd361bc3d118" class="reactable html-widget" style="width:auto;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-06dce0e1fd361bc3d118">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"text":["Hi my name is julia silge.  I'm a data scientist and software engineer at RStudio. and I'd like to thank the the organizers of the R user group in Korea so much for having me.  Today speak to you I am so happy to be speaking specifically today about creating features for machine learning from text data for a couple of reasons Having a better understanding of what we do to take text data and then to make it appropriate as an input for machine learning algorithms has many benefits both if you are directly getting ready to train a model or if you're at the beginning of some text analysis project or if you are trying to understand the behavior of a model that you're interacting with some way which is something that we do in our work as data scientists or in our in our daily lives more and more so when we build models for text either supervised or unsupervised we start with something like this this is some example text data that I'll use a couple of times during this talk that describes some animals I'm using some text data so you know to me as an english speaker looks familiar like I am as someone who uses a human language so I look at this and I can read it I could speak it aloud and I understand I can interpret it what it means so this kind of data this sort of natural language data is being generated all the time in all kinds of languages in all kinds of contexts so whether you work in healthcare in tech in finance basically any kind of organization this sort of text data is being generated by customers by clients by internal stakeholders inside of a business by people taking surveys via social media via business processes and in all this natural language there's information latent in that text data that can be used to make better decisions However, computers are not great at looking at this and doing math on language as it's represented like this and instead language has to be dramatically transformed to some kind of machine readable numeric representation that looks more like this what I'm showing here on the screen to be ready for almost any kind of model so I spent a fair amount of time working on software for people to be able to do exploratory data analysis, visualization, summarization tasks like that with text data in a tidy format where we have one observation per row and I love using tidy data principles for text analysis especially during those exploratory phases of an analysis when it comes time to build a model often what the underlying mathematical implementation really needs is typically something like this which is a way to this particular representation is called the document term matrix so the exact representation may differ from what I've shown here what I have here is we're weighting things by counts so each row in this matrix is a document each column is a is a word. A token and the numbers represent counts how many times does each document use each word you could weight it in a different way using say TF-IDF instead of counts or you might keep sequence information if you're interested in building a deep learning model but basically for all kinds of text modeling from simpler models like Naive Bayes models which work well for text to word embeddings to really the most state-of-the-art kind of work that's happening today like transformers for text data we have to heavily feature engineer and process language to get it to some kind of representation that's suitable for machine learning algorithms so I work on an open source framework in R for modeling and machine learning that's called Tidymodels and the examples that I'll be showing today use Tidymodels code some of the specific goals of the Tidymodels project are to provide a consistent flexible framework for real world modeling practitioners people who are you know doing that are dealing with real world data those who are just starting out to those who are very experienced in modeling and the goal is to harmonize the heterogeneous interfaces that exist within R and to encourage good statistical practice I'm glad to get to show you some of what I work on and build and how we apply it to text modeling but a lot of what I will talk about today isn't very specific to Tidymodels or even to R. I know this is an R user group but what we're going to talk about and focus on is a little more conceptual and basic how do we transform text into predictors for machine learning I am excited though to talk about Tidymodels and Tidymodels if you have not used it before is a meta package in a similar way that the Tidyverse is a meta package so if you've ever typed library Tidyverse and then you've used ggplot2 for visualization dplyr for data manipulation Tidymodels works in a similar way there are different packages inside of it that are used for different purposes so the pre-processing or the feature engineering is part of a broader model process you know it that process starts really with with exploratory data analysis that helps us decide what kind of model we will build and then it comes to completion I think I would argue with model evaluation when you measure how well your model performed Tidymodels as a piece of software is made up of our packages each of which has a specific focus like our sample is for re-sampling data to be able to create bootstrap resamples cross-validation resamples all different kinds of resamples you might want to use to train and evaluate models the tune package is for hyper parameter tuning as you might guess from the name one of these packages is for feature engineering for a data preprocessing feature engineering and it is the one that is called recipes so in Tidymodels we capture this idea of data pre-processing and feature engineering in the concept of a pre-processing recipe that has steps so you choose ingredients or variables that you're going to use then you define the steps that go into your recipe then you prepare them using training data and then you can apply that to any data set like testing data or new data at prediction time so the variables or ingredients that we use in modeling come in all kinds of shapes and sizes including text data so some of the techniques and approaches that we use for pre-processing text data are the same um as for any other kind of data that you might use like non-text data numeric data categorical data some for some of it is the same but some of what you need to know to be able to do a good job in this process for text is different and is specific to the nature of what language data is like and so I've written a book with my co-author Emile Hvitfeldt on supervised machine learning for text analysis and R and fully the first third of the book focuses on how we transform the natural language that we have in text data into features for modeling the middle section is about how we use these features in simpler or more traditional machine learning models like regularized regression or support vector machines and then the last third of the book talks about how we use deep learning models with text data so deep learning models still require these kinds of transformations from natural language into features as input for these kinds of models but deep learning models are often able to inherently learn structure of features from text in ways that those more traditional or simpler machine learning models are not so this book is now complete and available as of this month as of november folks are getting their first paper copies and also this book is available in its entirety at smalltar.com so if you're new to dealing with text data understanding these fundamental pre-processing approaches for text will set you up for being able to train effective models if you're really experienced with text data if you've dealt with it a lot already you've probably noticed like we have that the existing you know resources or literature whether that's books or tutorials or blog posts is quite sparse when it comes to detailed thoughtful explorations of how these pre-processing steps work and how choices made in these feature engineering steps impact our model output so let's walk through several of some of these like basic feature engineering approaches and how they work and what they do let's start out with tokenization so typically one of the first steps in transfer information from natural language to machine learning feature for really any kind of text analysis including exploratory data analysis or building a model. Anything is tokenization in tokenization we take an input some string some character vector and some kind of token type some meaningful unit of text we're interested in a word and we split the input pieces into tokens that correspond to the type we're interested in so most commonly the meaningful unit or type of token that we want to split text into units of is a word so this might seem straightforward or obvious but it turns out it's difficult to clearly define what a word is for many or even most languages so many languages do not use white space between words at all which presents a challenge for tokenization even languages that do use white space like english and korean often have particular examples that are ambiguous like contractions in english like didn't which should be you know maybe more accurately considered two words the way particles are used in Korean and how pronouns and negation words are written in romance languages like italian and french where they're stuck together and really maybe they should be considered two words once you have figured out what you're going to do and you make some choices and you tokenize your text then it's on its way to being able to be used in exploratory data analysis or unsupervised algorithms or as features for predictive modeling which is what we're talking about here and what these results show here so these results are from a regression model trained on descriptions of media from artwork in the Tate collection in the UK so what we're predicting in what we are predicting is when what year was a piece of art created based on the the medium that the artwork was created with and the medium is described with a little bit of text so we see here that artwork created using graphite watercolor and engraving was more likely to be created earlier that though that is more likely to come from older art and artwork that is created using photography screen point or sorry screen print screen printing and and dung and glitter are more likely to be created later. there this is more likely to come from contemporary art modern art so the the way that we tokenize this text you know we started with natural human generated texts of people writing out the descriptions of the the media that these art pieces of art were created with and the way we tokenized that natural human generated text that we started with has a big impact on what we learned from it if we tokenized in a different way we would have gotten different results in terms of performance like how accurately we were able to predict predict the year and also in terms of how we interpret the model like what is it that we're able to learn from it so this is one kind of tokenization to the single word but we also we all another way to tokenize instead of breaking up into single words or unigrams we can tokenize to n-grams so an n-gram is a continuous sequence of N items from a given sequence of texts so this shows that same piece of little bit of text i'm describing this animal divided up into bi-grams or n-grams of two tokens so notice how the words in the bi-grams overlap so the word collard appears in both of the first bigrams the collared collared peccary peccary also referred to so n-gram tokenization slides along the text to create overlapping sets of tokens this shows tri-grams for the same thing so using uni-grams one word is faster and more efficient but we don't capture information about word order I'm using a higher value two or three or even more keeps more complex information about word order and concepts that are described in multi-word phrases but the vector space of tokens increases dramatically that corresponds to a reduction in token counts we don't count each token as very many times and that means depending on your particular data set you might not be able to get good results so combining different degrees of n-grams can allow you to extract different levels of detail from text so uni-grams can tell you which individual words have been used a lot of times some of those words might be overlooked in bi-gram or tri-gram crowns if they don't co-appear with other words as often this plot compares model performance for a Lasso regression model predicting the year of supreme court opinions the United States supreme court opinions with three different degrees of n-grams what we're doing here is we are taking the text of the writings of the United States supreme court and we're predicting when did it when was that text written so can we predict how old a piece of text is from the contents of the text so holding the number of tokens constant at a thousand using uni-grams alone performs best for this corpus of opinions from the United States supreme court this is not always the case depending on the kind of model you use the data set itself we might see the best performance combining uni-grams and bi-grams or maybe some other option in this case if we wanted to incorporate some of that more complex information that we have in the bi-grams and the tri-grams we probably would need to increase the number of tokens in the model quite a bit so keep in mind when you look at results like these that identifying n-grams is computationally expensive this is especially compared to the amount of like a model the improvement in model performance that we often see like if we if we see some you know modest improvement by adding in bigrams it's important to keep in mind how much improvement we see relative to how long it takes to identify bi-grams and then train that model so for example for this data set of supreme court opinions where we held the number of tokens constant so the model training had the same number of tokens in it using bi-grams plus uni-grams takes twice as long to train to do the feature engineering and the training than only uni-grams and adding in tri-grams as well takes almost five times as long as training on uni-grams alone so this is a computationally expensive thing to do going in the other direction we can tokenize to units smaller than words so like these are what are called character shingles so we take words the collared peccary and we can instead of looking at words we can go down and look at sub word information there's multiple different ways to break words up into sub words that are appropriate for machine learning and often these kinds of approaches or algorithms have the benefit of being able to encode unknown or new words at prediction time so when it when it's time to make a prediction on new data it's not it's not uncommon for there to be new vocabulary words at that time and if we didn't see them in the training data you know what are we going to do about those new words when we train using subword information often we can handle those new words if we saw the subword in our training data set so using this kind of subword information is a way to incorporate morphological sequences into our models of you know various kinds of this is something that applies to various languages not just english so these results are for a classification model with a data set of very short texts it's just the names of post offices in the United States so super short and the goal of the model was to predict the post office located in hawaii in the middle of the pacific ocean or it located in the rest of the united states so I created features for the model that are subwords of these post office names and we end up learning that the names that start with h and p or contain that ale sub word are more likely to be in hawaii and the sub words a and d and ri and ing are are more likely to come from the post office that are outside of hawaii so this is an example of how we tokenized differently and we're able to learn something new we're able to learn something different so in Tidymodels we collect all these kinds of decisions about tokenization and code that looks like this so we start with a recipe that specifies what variables or ingredients that we'll use and then we define these preprocessing steps so even at this first and arguably you know simple and basic step the choices that we make affect our modeling results in a big way the next pre-processing um step that I want to talk about is stop words so once we have split text into tokens we often find that not all words carry the same amount of information if maybe any information at all actually for a machine learning task so common words that carry little or perhaps no meaningful information are called stopwords so this is one of the stopword lists that's available for Korean so it's common advice and practice to say hey just remove just remove remove these stopwords for a lot of natural language processing tasks what I'm showing here is the entirety of one of the shorter english stopword lists that's used really broadly so you know it's words like I me my pronouns conjunctions and of the and these are very common words that are not considered super important the decision though to just remove stopwords is often more involved and perhaps more fraught than what you'll than what you'll find reflected in a lot of resources that are out there so almost all the time real world NLP practitioners use pre-made stopword lists so this plot visualizes set intersections for three common stopword lists in english in what is called an upset plot so the three lists are called the snowball list smart and the iso list so you can see the the lengths of the list are represented by the length of the bars and then we see the intersections which words are in common on these lists by the by the vertical bars so the lengths of the list are quite different and also notice they don't all contain the same sets of words the important thing to remember about stopword lexicons is that they are not created in some neutral perfect setting but instead they are they are context specific they they can be biased both of these things are true because they are lists created from large data sets of language so they reflect the characteristics of the data used in their creation so this is the ten words that are in the english language smart lexicon but not in the English snowball lexicon so notice that they're all contractions but that's not because the snowball exchange doesn't include contractions it has a lot of them also notice that it has that she's is on this list and so that means that that list has he's but it does not have the list she's so this is an example of that The bias I mentioned that occurs because these lists are created from large data sets of text lexicon creators look at the most frequent words in some big corpus of language they make a cut off and then some decisions about what to include or exclude you know based on the list that they have and you end up here so because in many large data sets of language you have more representation of men you end up with a situation like this where a stopword list will have he's but not she's so many decisions when it comes to modeling or analysis with language we as practitioners have to decide what is appropriate for our particular domain it turns out this is even true when it comes to picking a stopword list so in Tidymodels we can implement a pre-processing step like removing stopwords by adding an additional step to our recipe so first we specified what variables we would use then we tokenized the text and now we are removing stopwords here using just the default step since we are not passing in any other arguments we could though use a non-default step or even a custom list if that was most appropriate to our domain this plot compares the model performance for predicting the year of that same data set of supreme court opinions with three different stopword lexicons of different lengths so the snowball lexicon contains the smallest number of words and in this case it results in the best performance so removing fewer stopwords results in the best performance here so this specific result is not generalizable to all data sets and contexts but the fact that removing different sets of stopwords can have noticeably different effects on your model that is quite transferable so the only way to know what is the best thing to do is to try several options and see so machine learning in general. this is an empirical field right like we don't know we don't often have reasons a priori to know what will be the best thing to do and so typically we have to try a different option to see what will be the best thing all right. then the the third pre-processing step that I want to talk about for text is stemming so when we deal with text often documents contain different versions of one base word often called a stem so what if say for an english example if we aren't interested in the difference between animals plural and animal singular and we want to treat them both together so that idea is at the heart of stemming so there's no one right way or correct way to stem text so this plot shows three approaches for stemming in English starting from hey let's just remove a final s to more complex rules about plural handling plural endings that middle one it is called the s stemmer it's a set of it's like a little set of rules and that last one is the best known one probably the best-known implementation of stemming in English called the Porter algorithm so you can see here that Porter stemming is the most different from the other two in the top 20 words here from the data set of animal descriptions that I've been using we see how the word species was treated differently animal predator this sort of collection of words live living life lives that was treated differently so practitioners are typically interested in stemming text data because it buckets tokens together that we believe belong together in in a way that we understand that as human users of language so we can use approaches like this which are pretty like step-by-step rules based this is typically called stemming or and it's fairly algorithmic in nature like first do this then do this then do this or you can use lemmatization which is usually based on large dictionaries of words and it incorporates like a linguistic understanding of what words belong together so most of the existing approaches for this kind of task in Korean are are limited lemmatizers based on these dictionaries and that are trained using large data sets of language so this seems like it's going to be a helpful thing to do when you hear about this you're like oh yeah sounds good,  sounds smart especially because with text data we are typically overwhelmed with features with numbers of tokens this is typically the situation when we're dealing with text data so here we have these animal description data and I made a matrix representation of it like we would typically use in some machine learning algorithm and look how many features there are 16,000 almost 17,000 features that's the number of features that would be going into the model look at the sparsity 98 percent sparse that's high very sparse data so this is the sparsity of the data that will go into the machine learning algorithm to build our supervised machine learning model if we stem the words if I use here an approach for stemming we reduce the number of word features by many thousands the sparsity unfortunately did not change as much but we reduced the number of features by a lot by bucketing those words together that our stemming algorithm belong together so you know common sense says reducing the number of words features so dramatically is going to perform improve the performance of our machine learning model but that is that does assume that we have not lost any important information by by stemming and it turns out that stemming or lemmatization can often be very helpful in some contexts but the typical algorithms used for these are somewhat aggressive and they have been built to favor sensitivity or recall or the true positive rate and this is at the expense of the specificity or the precision or the true negative rate so in a supervised machine learning context what this does is this affects a model's positive predictive value the precision or its ability to to not incorrectly label true negatives as positive I hope I got that right so you know to make this more concrete stemming can increase a model's ability to find the positive examples of say the animal descriptions that are associated with say a certain diet if that's what we're modeling however if text is over stemmed the resulting model loses its ability to label the negative examples say the descriptions that are not about that diet that's what we're looking for and this can be a real challenge when training models with text data kind of finding that that balance there because often we don't have a dial that we can change on these stemming on these stemming algorithms so even just very basic pre-processing for text like what I'm showing here in this feature engineering recipe can be computationally expensive and the choices that a practitioner makes like whether or not to remove stopwords or to stem text can have dramatic impact on how machine learning models of all kinds perform whether those are simpler models more traditional machine learning models or deep learning models what this means is that the price the prioritization that we as practitioners give to like learning teaching and writing about feature engineering steps for text really contributes to better more robust statistical practice in our field I mentioned before the sparsity of text data and I want to come back to that because it is one of text data's really defining characteristics because of just how language works we use a few words a lot of times and then a lot of words only just a couple of times only a few a few times and with a real set of natural language you end up with relationships that look like this that look like these plots in terms of how the sparsity changes as you add more documents and more unique words to a corpus so the sparsity goes up real fast as you add more unique words and the memory that is required to handle this set of documents goes up very fast so even if you use specialized data structures meant to store sparse data like sparse matrices you still end up growing the memory required to handle these data sets in a very non-linear way it still grows up very fast so this means it can take a very long time to train your model or even that you outgrow the memory available on your machine you have to go to the cloud to an expensive big memory situation this can be a real challenge and this challenge it is what has behind the motivating of vector languages for models so linguists have worked for a long time on vector languages for models that can reduce the number of dimensions representing text data based on how people use language so this quote here goes all the way back to 1957. so the idea here is that we use like the data is very sparse but we don't use words randomly it's not independent the words are not used independently of each other but rather there's relationships that exist between how words are used together and we can use those relationships to create to transform our sparse high dimensional space into a special dense low dimensional space lower we still has like 100 dimensions but much lower than the many thousands hundreds tens hundreds of thousands of space so the idea here we use statistical modeling maybe just word counts plus matrix factorization maybe fancier math that involves neural networks to take this really high dimensional space and we create a new lower dimensional lower dimensional space that is special because the new space is created based on vectors that incorporate information about which words are used together so you shall know a word by the company it keeps so you need a big data set of text to create or learn these kinds of word vectors or word embeddings so this table that I'm showing right now it's from a set of embeddings that I created using a data set or a corpus of complaints complaints to the United States consumer financial protection bureau so this is a government body in the United States where people can complain and say what is wrong with something to do with a financial product like a credit card a mortgage a student loan something to do with like a financial product they're like something went wrong with my credit card something went wrong with my mortgage that company is not being fair so you come and you complain to it so I took all those complaints and built it's our high dimensional space and build a low dimensional space and we can look in that space and understand what words are related to each other in this space so in the new space defined by the embeddings the word month is closest to words like year months plural monthly installments payment so these are words that are that makes sense in the context of financial products like credit cards or mortgages in the new space defined by these embeddings the word error is closest to the words like mistake clerical like a clerical mistake problem glitch or there was a glitch on my mortgage statement so we see these kinds of or miscommunication misunderstanding you know like these are these are words that are used in similar ways so you don't have to create embeddings yourself because it requires quite a lot of data to make them so you can use word embeddings that are pre-trained i.e created by someone else based on some huge corpus of data that they have access to and you probably don't so let's look at one of those data sets let's look at this table shows the results for the same word error but for the glove embeddings so the glove embeddings are a set of pre-trained embeddings that are created based on a very large data set that's like all of wikipedia all of the google news data set just like huge swaths of the internet have been fed in to create these embeddings so some of the closest words here are similar to those that are before but we no longer have some of that domain specific flavor like clerical discrepancy and now we have like miscommunication you know but and now we have calculation and probability which people were not talking about with their financial product complaints so this really highlights how these how these work here before we we created our own and we were able to learn relationships that were specific to this context and here we go to a more general set that that was learned somewhere else so embeddings are trained or learned from a large corpus of text data and the characteristics of that corpus become part of the embeddings so machine learning in general you know is exquisitely sensitive to whatever it is that's in your training data and this is never more obvious than when dealing with text data and perhaps with word embeddings is just like one of these classic examples where this is true it turns out that this shows up in how any human prejudice or bias in the corpus becomes imprinted into the embeddings so in fact when we look at some of these most commonly available embeddings that are out there bias is we we see that african-american first names that are more common for african americans in the United States they're associated with more unpleasant feelings than European American first names in these embedding spaces women's first names are more associated with family and men's first names are more associated with career and terms associated with women are more associated with the arts and terms associated with men are more associated with science so it turns out actually bias is so ingrained in word embeddings that the word embeddings themselves can be used to quantify change in social attitudes over time so word embeddings are maybe an exaggerated or extreme example but it turns out that all the feature engineering decisions that we make when it comes to text data have a significant effect on our results both in terms of the model performance that we see and also in terms of how appropriate or fair our models are so given all that when it comes to pre-processing your text data creating these features that you need you have a lot of options and quite a bit of responsibility so my advice is always start with simpler models that you can understand quite deeply be sure to adopt good statistical practices as you train and tune your models so you aren't fooled about model performance improvements when you try different approaches and also to use model explainability tools and frameworks so you can understand any less straightforward models that you try so my co-workers and I have written about all of these topics and how to use them with Tidymodels if that's what you like to use and we will continue to do so with that i will say thank you so very much and I want to be sure to again thank the organizers of the R user group in Korea I want to thank my teammates on the Tidymodels team at Rstudio as well as my co-author EMIL HVITFELDT."],"translatedText":["안녕하세요 제 이름은 줄리아 실지입니다. 저는 RStudio의 데이터 과학자이자 소프트웨어 엔지니어입니다. 그리고 저를 허락해주신 한국 R 유저 그룹의 주최측에 감사드립니다. 오늘은 몇 가지 이유로 텍스트 데이터에서 기계 학습을 위한 기능을 생성하는 방법에 대해 오늘 구체적으로 이야기하게 되어 매우 기쁩니다. 학습 알고리즘은 모델을 직접 훈련할 준비를 하거나 일부 텍스트 분석 프로젝트를 시작하는 경우 또는 어떤 방식으로든 상호 작용하는 모델의 동작을 이해하려는 경우 모두 많은 이점이 있습니다. 우리가 데이터 과학자로서 일하거나 일상 생활에서 점점 더 많이 하는 것입니다. 그래서 감독 또는 감독되지 않은 텍스트에 대한 모델을 구축할 때 다음과 같이 시작합니다. 다음은 제가 사용할 텍스트 데이터의 예입니다. 동물에 대해 설명하는 이 강연에서 몇 번이고 텍스트 데이터를 사용하고 있습니다. 그래서 영어 사용자인 저에게 친숙한 사람처럼 보이기 때문에 이것을 보고 읽을 수 있습니다. 큰 소리로 말하고 이해합니다. 의미를 해석할 수 있습니다. 그래서 이런 종류의 데이터 이런 종류의 자연어 데이터는 모든 종류의 컨텍스트에서 모든 종류의 언어로 항상 생성되고 있으므로 기술 분야의 의료 분야에서 일하든 금융 기본적으로 모든 종류의 조직 이러한 종류의 텍스트 데이터는 비즈니스 프로세스를 통해 소셜 미디어를 통해 설문 조사에 참여하는 사람들이 비즈니스 내부의 내부 이해 관계자에 의해 고객에 의해 고객에 의해 생성되며 이 모든 자연 언어에는 해당 텍스트 데이터에 잠재된 정보가 있습니다. 더 나은 결정을 내리는 데 사용되어야 합니다. 그러나 컴퓨터는 이것을 보고 언어에 대해 수학을 수행하는 데 능숙하지 않습니다. 이렇게 표현되기 때문에 대신 언어는 기계가 읽을 수 있는 일종의 숫자 표현으로 극적으로 변환되어야 합니다. m 거의 모든 종류의 모델에 사용할 수 있도록 화면에 표시되므로 사람들이 ex를 할 수 있도록 소프트웨어 작업에 상당한 시간을 할애했습니다. 행당 하나의 관찰이 있는 깔끔한 형식의 텍스트 데이터와 같은 탐색적 데이터 분석, 시각화, 요약 작업 특히 모델을 구축할 때 분석의 탐색 단계에서 텍스트 분석을 위해 깔끔한 데이터 원칙을 사용하는 것을 좋아합니다. 종종 기본 수학적 구현이 실제로 필요로 하는 것은 일반적으로 이와 같은 것입니다. 이 특정 표현에 대한 방법은 문서 용어 행렬이라고 하므로 정확한 표현은 여기에 표시된 것과 다를 수 있습니다. 여기에 있는 것은 가중치를 적용한다는 것입니다 이 행렬의 각 행은 문서이므로 각 열은 단어입니다. 토큰과 숫자는 각 문서가 각 단어를 몇 번 사용하는지를 나타냅니다. 카운트 대신 TF-IDF를 사용하여 다른 방식으로 가중치를 부여하거나 딥 러닝 모델을 구축하는 데 관심이 있는 경우 시퀀스 정보를 유지할 수 있습니다. 기본적으로 텍스트에서 단어로의 임베딩에 잘 작동하는 Naive Bayes 모델과 같은 단순한 모델에서 텍스트 데이터용 변환기와 같이 오늘날 일어나고 있는 가장 최첨단 작업에 이르기까지 모든 종류의 텍스트 모델링을 위해 많은 기능을 엔지니어링해야 합니다. 기계 학습 알고리즘에 적합한 일종의 표현을 얻기 위해 처리 언어를 사용합니다. 그래서 저는 Tidymodels라고 하는 모델링 및 기계 학습을 위한 R의 오픈 소스 프레임워크에서 작업하고 있으며 오늘 보여드릴 예제에서는 Tidymodels 코드 중 일부를 사용합니다. Tidymodels 프로젝트의 특정 목표는 실제 작업을 처리하는 실제 모델링 실무자에게 일관되고 유연한 프레임워크를 제공하는 것입니다. 세계 데이터 모델링을 시작한지 얼마 되지 않은 사람들, 그리고 목표는 R 내에 존재하는 이기종 인터페이스를 조화시키고 좋은 통계 관행을 장려하는 것입니다. 제가 작업하고 있는 몇 가지를 보여드리게 되어 기쁩니다. 빌드하고 텍스트 모델링에 적용하는 방법에 대해 설명하지만 오늘 이야기할 내용은 Tidymodels 또는 R에만 국한된 것이 아닙니다. 이것이 R 사용자 그룹이라는 것을 알고 있지만 우리가 이야기할 내용과 초점은 좀 더 개념적이고 기본적입니다. 기계 학습을 위해 텍스트를 예측자로 변환하는 방법 이전에 사용하지 않은 경우 Tidymodels 및 Tidymodels에 대해 이야기하게 되어 기쁩니다. Tidyverse가 메타 패키지인 것과 유사한 방식으로 메타 패키지입니다. 패키지이므로 Tidyverse 라이브러리를 입력한 다음 시각화를 위해 ggplot2를 사용한 경우 데이터 조작을 위한 dplyr Tidymodels는 유사한 방식으로 작동합니다. 피쳐 엔지니어링은 더 광범위한 모델 프로세스의 일부입니다. 프로세스는 실제로 어떤 종류의 모델을 구축할지 결정하는 데 도움이 되는 탐색적 데이터 분석으로 시작하여 완성됩니다. 잘 수행된 모델 소프트웨어의 한 조각으로서 Tidymodels는 우리의 패키지로 구성되어 있습니다. 각 패키지는 우리의 샘플과 같이 특정 초점을 가지고 있습니다. 부트스트랩 재샘플을 생성할 수 있도록 데이터를 재샘플링하는 것입니다. 모델을 훈련하고 평가하는 데 사용하고 싶은 튜닝 패키지는 이름에서 짐작할 수 있듯이 하이퍼 매개변수 튜닝을 위한 것입니다. 이 패키지 중 하나는 데이터 전처리 기능 엔지니어링을 위한 기능 엔지니어링을 위한 것이며 Tidymodels에서 우리는 레시피라고 하는 것입니다. 단계가 있는 사전 처리 레시피의 개념에서 데이터 사전 처리 및 기능 엔지니어링에 대한 아이디어를 캡처하여 원하는 재료 또는 변수를 선택합니다. 다시 사용할 것입니다. 그런 다음 레시피에 들어가는 단계를 정의한 다음 훈련 데이터를 사용하여 준비한 다음 예측 시간에 테스트 데이터 또는 새 데이터와 같은 모든 데이터 세트에 적용할 수 있으므로 모델링에 사용하는 변수 또는 성분 텍스트 데이터를 포함한 모든 종류의 모양과 크기로 제공되므로 텍스트 데이터를 사전 처리하는 데 사용하는 일부 기술과 접근 방식은 텍스트가 아닌 데이터와 같이 사용할 수 있는 다른 종류의 데이터와 동일합니다. 숫자 데이터 범주형 데이터 중 일부는 동일하지만 텍스트에 대한 이 프로세스에서 좋은 작업을 수행하기 위해 알아야 할 일부는 다르며 언어 데이터의 특성에 따라 다릅니다. 공동 저자인 Emile Hvitfeldt와 함께 텍스트 분석 및 R을 위한 감독된 기계 학습에 관한 책과 이 책의 1/3은 완전히 텍스트 데이터에 있는 자연어를 중간 섹션을 모델링하기 위한 기능으로 변환하는 방법에 초점을 맞추고 있습니다. 우리는 t를 사용합니다 정규화된 회귀 또는 지원 벡터 머신과 같은 더 단순하거나 더 전통적인 머신 러닝 모델의 이러한 기능과 책의 마지막 3분의 1은 텍스트 데이터와 함께 딥 러닝 모델을 사용하는 방법에 대해 설명하므로 딥 러닝 모델은 여전히 자연어에서 이러한 종류의 변환이 필요합니다. 기능은 이러한 종류의 모델에 대한 입력으로 사용되지만 딥 러닝 모델은 더 전통적이거나 더 간단한 기계 학습 모델이 아닌 방식으로 텍스트에서 기능 구조를 본질적으로 학습할 수 있으므로 이 책은 현재 이달 현재 완성되어 사용할 수 있습니다. 11월 사람들은 첫 종이 사본을 받고 있으며 이 책 전체는 smalltar.com에서 구할 수 있습니다. 따라서 텍스트 데이터를 처음 다루는 경우 텍스트에 대한 이러한 기본적인 사전 처리 접근 방식을 이해하면 이미 텍스트 데이터를 많이 다루었다면 텍스트 데이터에 대한 경험이 있는 경우 효과적인 모델이 될 것입니다. 기존 리소스나 문헌은 책이든 튜토리얼이든 블로그 게시물이든 이러한 사전 처리 단계가 작동하는 방식과 이러한 기능 엔지니어링 단계에서 선택한 사항이 모델 출력에 미치는 영향에 대한 자세한 사려 깊은 탐구와 관련하여 매우 희소합니다. 기본적인 기능 엔지니어링 접근 방식과 작동 방식 및 수행하는 작업과 같은 몇 가지를 통해 토큰화부터 시작하겠습니다. 따라서 일반적으로 탐색을 포함한 모든 종류의 텍스트 분석을 위해 자연어에서 기계 학습 기능으로 정보를 전송하는 첫 번째 단계 중 하나입니다. 데이터 분석 또는 모델 구축. 토큰화에서 모든 것은 토큰화입니다. 우리는 입력을 일부 문자열, 일부 문자 벡터 및 일종의 토큰 유형으로 우리가 관심 있는 단어에 관심이 있는 의미 있는 텍스트 단위를 취하고 입력 조각을 우리가 관심 있는 유형에 해당하는 토큰으로 분할합니다. 텍스트를 단위로 분할하려는 가장 일반적으로 의미 있는 단위 또는 토큰 유형은 단어입니다. 언어는 단어 사이에 공백을 전혀 사용하지 않아 토큰화에 어려움을 겪습니다. 심지어 영어와 한국어와 같이 공백을 사용하는 언어에도 종종 영어의 축약형처럼 모호한 특정 예가 있습니다. 한국어에서 입자가 사용되는 방식과 이탈리아어, 프랑스어와 같은 로맨스 언어에서 대명사와 부정 단어가 서로 붙어있는 방식으로 쓰여지는 두 단어로 간주됩니다. 그리고 실제로 무엇을 할 것인지 파악하고 몇 가지 선택을 하고 텍스트를 토큰화하면 두 단어로 간주되어야 합니다. 그러면 탐색적 데이터 분석이나 감독되지 않은 알고리즘 또는 예측 모델링을 위한 기능으로 여기에서 우리가 이야기하고 이러한 결과가 여기에 표시되는 것이므로 이러한 결과는 영국의 Tate 컬렉션에 있는 예술 작품의 미디어 설명에 대해 훈련된 회귀 모델에서 나온 것이므로 우리가 무엇에서 예측하고 있는지 작품을 만든 매체와 매체가 약간의 텍스트로 설명되어 있어 흑연 수채화와 판화를 사용하여 만든 작품이 몇 년도에 만들어졌는지 예측하고 있습니다. 사진 스크린 포인트 또는 유감 스크린 인쇄 스크린 인쇄를 사용하여 만든 오래된 예술 및 예술 작품에서 나올 가능성이 더 높지만 똥과 반짝이는 mor e 나중에 생성될 가능성이 있습니다. 거기에서 이것은 현대 미술 현대 미술에서 나올 가능성이 더 높기 때문에 이 텍스트를 토큰화하는 방법은 이 예술 작품이 만들어지는 미디어에 대한 설명을 작성하는 사람들의 자연스러운 인간 생성 텍스트로 시작했습니다. 우리가 시작한 자연적인 인간 생성 텍스트를 토큰화한 방식은 우리가 다른 방식으로 토큰화했다면 우리가 예측할 수 있었던 정확도와 같은 성능 측면에서 다른 결과를 얻었을 것입니다. 연도 및 또한 모델을 해석하는 방법의 측면에서 우리가 배울 수 있는 것은 무엇입니까? 이것은 단일 단어에 대한 일종의 토큰화이지만 우리는 또한 단일 단어로 분해하는 대신 토큰화하는 또 다른 방법입니다. 단어 또는 유니그램을 n-그램으로 토큰화할 수 있으므로 n-그램은 주어진 텍스트 시퀀스에서 N개 항목의 연속 시퀀스이므로 이 동물을 설명하는 동일한 텍스트 조각이 다음으로 나뉩니다. bi-grams 또는 n-grams of two tokens 그래서 bi-grams의 단어가 어떻게 겹치는지 확인하여 collard라는 단어가 첫 번째 bigrams 모두에 나타납니다. collared collared peccary라고도 하는 n-gram 토큰화가 텍스트를 따라 슬라이드 겹치는 토큰 세트 생성 이것은 동일한 것에 대한 트라이그램을 보여주므로 유니그램을 사용하면 한 단어가 더 빠르고 효율적이지만 단어 순서에 대한 정보는 캡처하지 않습니다. 더 높은 값을 사용하고 있습니다. 여러 단어로 된 구로 설명된 단어 순서 및 개념에 대한 더 복잡한 정보이지만 토큰의 벡터 공간은 토큰 수의 감소에 따라 극적으로 증가합니다. 이는 각 토큰을 여러 번 계산하지 않으며 이는 귀하의 특정 항목에 따라 달라짐을 의미합니다. 데이터 세트를 사용하면 좋은 결과를 얻지 못할 수 있으므로 서로 다른 정도의 n-gram을 결합하면 텍스트에서 다양한 수준의 세부 정보를 추출할 수 있으므로 uni-gram은 어떤 개별 단어가 많이 사용되었는지 알려줄 수 있습니다. 이러한 단어 중 일부는 다른 단어와 함께 나타나지 않는 경우 바이그램 또는 트라이그램 크라운에서 간과될 수 있습니다. 이 플롯은 대법원 판결의 연도를 예측하는 올가미 회귀 모델의 모델 성능을 비교합니다. 미국 대법원 3가지 다른 등급의 n-gram을 사용하는 법원 의견 우리가 여기서 하는 것은 우리가 미국 대법원의 문서를 가져오는 것입니다. 그리고 우리는 그 문서가 언제 작성되었는지 예측하고 있으므로 얼마나 오래되었는지 예측할 수 있습니다. 텍스트의 일부는 텍스트의 내용에서 가져온 것이므로 유니그램만 사용하여 토큰의 수를 1000으로 일정하게 유지하는 것이 미국 대법원의 의견 모음에 가장 적합합니다. 모델 자체를 사용하여 데이터 세트 자체를 사용하면 유니그램과 바이그램을 결합한 최상의 성능을 볼 수 있으며, 이 경우 바이그램 및 트라이그램의 경우 모델의 토큰 수를 상당히 늘려야 할 수도 있으므로 이러한 결과를 볼 때 n-그램을 식별하는 것은 계산적으로 비용이 많이 든다는 점을 염두에 두십시오. 이는 특히 모델과 같은 양과 비교됩니다. 우리가 흔히 볼 수 있는 모델 성능의 개선 빅그램을 추가하여 약간의 개선을 알면 바이그램을 식별한 다음 해당 모델을 훈련하는 데 걸리는 시간에 비해 얼마나 개선되었는지를 염두에 두는 것이 중요합니다. 예를 들어 이 데이터 세트의 경우 토큰 수를 일정하게 유지하여 모델 교육에 동일한 수의 토큰이 포함되어 바이그램과 유니그램을 사용하여 기능 엔지니어링을 수행하는 데 두 배의 시간이 걸립니다. 유니그램보다 훈련하고 트라이그램을 더하는 것 역시 유니그램만 훈련하는 것보다 거의 5배나 오래 걸리므로 토큰화할 수 있는 다른 방향으로 이동하는 데 계산적으로 많은 비용이 드는 일입니다. 단어보다 작은 단위로 이렇게 하는 것이 문자 대상 포진이라고 하는 것입니다. 그래서 우리는 칼라가 있는 페커리 단어를 사용합니다. 단어를 보는 대신 아래로 내려가서 하위 단어 정보를 볼 수 있습니다. 단어를 하위 단어로 나누는 여러 다른 방법이 있습니다. 머신 러닝에 적합하고 종종 이러한 종류의 접근 방식이나 알고리즘은 예측 시간에 알려지지 않은 단어나 새로운 단어를 인코딩할 수 있다는 이점이 있습니다. 그 당시에 새로운 단어가 있었고 훈련 데이터에서 그 단어를 보지 못했다면 하위 단어 정보를 자주 사용하여 훈련할 때 새로운 단어에 대해 무엇을 할 것인지 알 수 있습니다. 우리의 훈련 데이터 세트이므로 이러한 종류의 하위 단어 정보를 사용하는 것은 형태학적 시퀀스를 우리의 모델에 통합하는 방법입니다. 영어이므로 이 결과는 매우 짧은 텍스트의 데이터 세트가 있는 분류 모델에 대한 것입니다. 미국의 우체국 이름일 뿐이므로 매우 짧고 모델의 목표는 하와이 한가운데에 위치한 우체국을 예측하는 것이었습니다. 태평양 또는 미국의 나머지 지역에 위치하므로 이 우체국 이름의 하위 단어인 모델에 대한 기능을 만들었으며 결국 h와 p로 시작하거나 해당 ale 하위 단어를 포함하는 이름이 더 많다는 것을 알게 되었습니다. 하와이에 있을 가능성이 있고 하위 단어 a 및 d 및 ri 및 ing은 하와이 외부에 있는 우체국에서 올 가능성이 더 높으므로 이것은 우리가 어떻게 다르게 토큰화하고 새로운 것을 배울 수 있는지에 대한 예입니다 우리는 다른 것을 배울 수 있으므로 Tidymodels에서 토큰화 및 이와 같은 코드에 대한 모든 종류의 결정을 수집하므로 사용할 변수 또는 구성 요소를 지정하는 레시피로 시작한 다음 이러한 사전 절차를 정의합니다. 첫 번째 단계에서도 간단하고 기본적인 단계를 선택하면 모델링 결과에 큰 영향을 미칩니다. 제가 이야기하고 싶은 다음 사전 처리 단계는 단어를 중지하는 것이므로 텍스트를 분할하면 토큰으로 우리는 종종 모든 단어가 같은 양의 정보를 전달하는 것은 아니라는 사실을 발견합니다. 실제로 기계 학습 작업에 대한 정보가 전혀 없다면 의미 있는 정보가 거의 또는 전혀 전달되지 않는 일반적인 단어를 불용어라고 하므로 이것이 불용어 목록 중 하나입니다. 한국어로 사용할 수 있으므로 많은 자연어 처리 작업을 위해 이 불용어를 제거하십시오. I me와 같은 단어라는 것을 알고 있습니다. 내 대명사 접속사 및 of and 이것들은 매우 중요하지 않은 매우 일반적인 단어이지만 불용어를 제거하는 것이 더 자주 결정됩니다. 많은 리소스에 반영되어 있는 것보다 더 복잡하고 복잡할 수 있습니다. 그래서 거의 항상 실제 NLP 실무자들은 미리 만들어진 불용어 목록을 사용하므로 이 플롯은 세 가지 일반적인 영어로 된 불용어 목록은 업셋 플롯이라고 하므로 세 개의 목록을 눈덩이 목록 smart라고 하고 iso 목록이라고 하여 목록의 길이가 막대의 길이로 표시되는 것을 볼 수 있습니다. 그런 다음 교차점을 봅니다. 단어는 수직 막대로 이러한 목록에서 공통적이므로 목록의 길이가 상당히 다르며 모두 동일한 단어 집합을 포함하지 않는다는 점에 유의하십시오. 불용어 사전에 대해 기억해야 할 중요한 점은 해당 단어가 생성되지 않는다는 것입니다 일부 중립적인 완벽한 설정에서는 대신 문맥에 따라 다르며 편향될 수 있습니다. 이 두 가지 모두 사실은 언어의 큰 데이터 세트에서 생성된 목록이므로 문자를 반영하기 때문입니다. 생성에 사용된 데이터의 ristics 그래서 이것은 영어 스마트 어휘집에는 있지만 영어 눈덩이 어휘집에는 없는 10개의 단어입니다. 그래서 그것들이 모두 축약형임을 주목하십시오. 그러나 그것은 눈덩이 교환이 축약어를 포함하지 않기 때문이 아닙니다. 그들 중 많은 사람들이 그녀가 이 목록에 있다는 것을 알아차렸고 그래서 그 목록에는 그가 있지만 그녀의 목록은 없다는 것을 의미합니다. 그래서 이것은 내가 언급한 편향의 한 예입니다. 방대한 텍스트 데이터 세트에서 생성된 어휘집 작성자는 큰 언어 모음에서 가장 자주 사용되는 단어를 살펴보고 잘라낸 다음 포함하거나 제외할 항목에 대한 몇 가지 결정을 내립니다. 따라서 많은 대규모 언어 데이터 세트에서 남성의 표현이 더 많기 때문에 불용어 목록에는 남성이 있지만 랑구로 모델링하거나 분석할 때 그녀는 결정을 내리지 못하는 상황이 발생합니다. 우리는 실무자로서 우리의 특정 도메인에 적합한 것을 결정해야 합니다. 이는 불용어 목록을 선택하는 경우에도 마찬가지입니다. 따라서 Tidymodels에서는 추가 단계를 레시피 그래서 먼저 사용할 변수를 지정한 다음 텍스트를 토큰화했습니다. 이제 기본 단계가 아닌 단계나 사용자 정의 목록을 사용할 수 있는 다른 인수를 전달하지 않기 때문에 기본 단계만 사용하여 여기서 불용어를 제거합니다. 그것이 우리 영역에 가장 적절한 경우 이 플롯은 동일한 대법원 의견 데이터 세트의 연도를 예측하기 위한 모델 성능을 길이가 다른 3개의 다른 불용어 사전과 비교하여 눈덩이 사전이 가장 적은 수의 단어를 포함하도록 합니다. 결과적으로 최고의 성능을 제공하므로 더 적은 수의 불용어를 제거하면 여기에서 최상의 성능을 얻을 수 있으므로 이 특정 결과를 모든 데이터 세트 및 컨텍스트에 일반화할 수 없습니다. 그러나 서로 다른 불용어 세트를 제거하면 모델에 눈에 띄게 다른 영향을 미칠 수 있고 이는 상당히 이전할 수 있으므로 가장 좋은 방법을 아는 유일한 방법은 여러 옵션을 시도하고 일반적으로 기계 학습을 확인하는 것입니다. 이것은 우리가 알지 못하는 것과 같은 경험적 필드입니다. 우리는 종종 가장 좋은 일이 무엇인지 알아야 할 선험적 이유가 없으므로 일반적으로 가장 좋은 일이 무엇인지 확인하기 위해 다른 옵션을 시도해야 합니다. 오른쪽. 그런 다음 텍스트에 대해 이야기하고 싶은 세 번째 전처리 단계는 형태소 분석입니다. 그래서 우리가 텍스트를 다룰 때 문서에는 종종 줄기라고 불리는 하나의 기본 단어의 다른 버전이 포함됩니다. 동물의 복수형과 동물의 단수형의 차이점에 관심이 있고 우리는 두 가지를 함께 취급하여 아이디어가 형태소 분석의 핵심이므로 텍스트를 줄기하는 올바른 방법이나 올바른 방법이 없으므로 이 플롯은 영어로 시작하는 형태소 분석에 대한 세 가지 접근 방식을 보여줍니다. 이봐, 복수형 처리에 대한 더 복잡한 규칙에 대한 마지막 s를 제거합시다. 그 중간에 있는 것은 s 형태소 분석기라고 하며, 그 집합은 일종의 규칙 집합과 같으며 마지막 것이 가장 잘 알려진 것일 수 있습니다. Porter 알고리즘이라고 하는 영어로 형태소 분석의 알려진 구현은 여기에서 Porter 형태소 분석이 내가 사용한 동물 설명 데이터 세트에서 상위 20개 단어의 다른 두 단어와 가장 다르다는 것을 알 수 있습니다. 노래 우리는 종이라는 단어가 어떻게 다르게 취급되었는지 봅니다. 동물 포식자 이런 종류의 단어 모음은 살아 있는 삶을 살고 있습니다. 그래서 실무자들은 일반적으로 텍스트 데이터의 형태소 분석에 관심이 있습니다. 우리는 언어의 인간 사용자로서 이와 같은 접근 방식을 사용할 수 있다는 점을 이해합니다. 이 접근 방식은 일반적으로 형태소 분석이라고 하는 단계별 규칙과 매우 유사합니다. 일반적으로 큰 단어 사전을 기반으로 하는 보조 정리를 사용할 수 있으며 어떤 단어가 함께 속하는지에 대한 언어적 이해를 통합하므로 한국어에서 이러한 종류의 작업에 대한 기존 접근 방식의 대부분은 이러한 사전을 기반으로 하고 다음을 사용하여 훈련된 제한된 보조 정리입니다. 언어의 큰 데이터 세트 그래서 이것은 당신이 이것에 대해 들었을 때 하는 것이 도움이 될 것 같습니다 오 예 텍스트 데이터의 경우 일반적으로 토큰 수가 많은 기능에 압도되기 때문에 소리가 좋고 똑똑하게 들립니다. 이는 일반적으로 텍스트 데이터를 처리할 때의 상황이므로 여기에 이러한 동물 설명 데이터가 있고 다음과 같이 매트릭스 표현을 만들었습니다. 우리는 일반적으로 일부 기계 학습 알고리즘에서 사용하고 기능이 몇 개 있는지 확인합니다. 16,000개 거의 17,000개에 달하는 기능이 모델에 들어갈 기능의 수입니다. 희소성을 살펴봅니다. 98% 희소가 높은 매우 희소 데이터이므로 이것이 단어를 어간하면 지도 머신 러닝 모델을 구축하기 위해 머신 러닝 알고리즘에 들어갈 데이터가 여기에서 파생어에 대한 접근 방식을 사용하면 단어 기능의 수를 수천 배로 줄입니다. 희소성은 불행히도 많이 변경되지 않았지만 우리는 우리의 형태소 분석 알고리즘이 함께 속하는 단어를 함께 버킷화하여 기능의 수를 많이 줄였습니다. 단어 기능의 수를 극적으로 늘리면 기계 학습 모델의 성능이 향상되지만 이는 형태소 분석을 통해 중요한 정보를 잃지 않았다고 가정하고 형태소 분석 또는 보조 정리가 종종 매우 도움이 될 수 있음이 밝혀졌습니다. 일부 상황에서는 이러한 알고리즘에 사용되는 일반적인 알고리즘이 다소 공격적이며 감도 또는 재현율 또는 참 양성률을 선호하도록 구축되었으며 감독되는 기계에서 특이성 또는 정밀도 또는 참 음성률을 희생합니다. 학습 컨텍스트 이것이 하는 일은 모델의 긍정적 예측 값에 영향을 미친다는 것입니다. 정밀도 또는 참 부정을 긍정적으로 잘못 레이블을 지정하지 않는 기능 우리가 모델링하는 것이지만 텍스트가 끝나면 특정 식단을 말하는 것과 관련된 동물 설명을 말하는 긍정적인 예 결과 모델이 부정적인 예에 레이블을 지정할 수 있는 능력을 잃는다는 이유로 우리가 찾고 있는 식단에 대한 설명이 아닌 설명이 있으며 이는 텍스트 데이터를 사용하여 모델을 훈련할 때 이러한 균형을 찾는 일종의 진정한 도전이 될 수 있습니다. 우리는 이러한 형태소 분석 알고리즘에서 이러한 형태소 분석에 대해 변경할 수 있는 다이얼이 없으므로 이 기능 엔지니어링 레시피에서 여기에서 보여주는 것과 같은 텍스트에 대한 아주 기본적인 사전 처리조차도 계산 비용이 많이 들고 실무자가 선택하는 불용어를 제거할지 텍스트를 줄기로 만들지 여부는 모든 종류의 기계 학습 모델이 수행하는 방식에 극적인 영향을 미칠 수 있습니다. 더 단순한 모델이든 더 전통적인 기계 학습 모델이든 딥 러닝 모델이든 이것이 의미하는 바는 우리가 실무자가 텍스트에 대한 피쳐 엔지니어링 단계에 대해 가르치고 쓰는 것과 같이 실제로 더 강력한 통계 p에 기여합니다. 텍스트 데이터의 희소성 이전에 언급한 우리 분야의 관행 언어가 작동하는 방식 때문에 텍스트 데이터가 실제로 정의하는 특성 중 하나이기 때문에 몇 가지 단어를 여러 번 사용하고 여러 번 사용합니다. 단어는 단 몇 번만 몇 번만 그리고 실제 자연어 세트를 사용하면 더 많은 문서와 고유성을 추가함에 따라 희소성이 어떻게 변하는 면에서 이러한 플롯과 같은 다음과 같은 관계로 끝납니다. 단어를 말뭉치에 추가하면 희소성이 정말 빠르게 올라가고 이 문서 집합을 처리하는 데 필요한 메모리가 매우 빠르게 증가하므로 희소 행렬과 같은 희소 데이터를 저장하기 위한 특수 데이터 구조를 사용하더라도 여전히 매우 비선형적인 방식으로 이러한 데이터 세트를 처리하는 데 필요한 메모리가 증가합니다. 여전히 매우 빠르게 증가하므로 모델을 훈련하는 데 매우 오랜 시간이 걸리거나 사용 가능한 메모리가 초과될 수도 있습니다. r 기계는 값비싼 대용량 메모리를 사용하기 위해 클라우드로 이동해야 하는 상황입니다. 이것은 실제 도전이 될 수 있으며 이 도전은 모델에 대한 벡터 언어의 동기 부여 뒤에 있는 것이므로 언어학자는 모델용 벡터 언어에 대해 오랫동안 작업해 왔습니다. 사람들이 언어를 사용하는 방식에 따라 텍스트 데이터를 나타내는 차원의 수를 줄일 수 있으므로 이 인용문은 1957년으로 거슬러 올라갑니다. 따라서 여기서 아이디어는 데이터가 매우 희박하지만 단어를 무작위로 사용하지 않는 것처럼 사용한다는 것입니다. 독립적이지 않은 단어는 서로 독립적으로 사용되지 않고 오히려 단어가 함께 사용되는 방식 사이에 존재하는 관계가 있으며 이러한 관계를 사용하여 희소한 고차원 공간을 특별한 조밀한 저차원 공간으로 변환하기 위해 생성할 수 있습니다. 100개의 차원이지만 수천 수백 수만 개의 공간보다 훨씬 낮기 때문에 여기서 우리는 통계 모델링을 사용합니다. 단어 수와 행렬 인수만 더하면 됩니다. 새로운 공간은 어떤 단어가 함께 사용되는지에 대한 정보를 통합하는 벡터를 기반으로 생성되기 때문에 특별한 새로운 저차원 저차원 공간을 생성하기 때문에 신경망을 포함하는 더 멋진 수학이 될 수 있습니다. 회사에서 제공하는 단어이므로 이러한 종류의 단어 벡터 또는 단어 임베딩을 생성하거나 배우려면 큰 데이터 세트의 텍스트가 필요합니다. 그래서 지금 제가 보여주는 이 테이블은 데이터를 사용하여 생성한 임베딩 세트에서 가져온 것입니다. 미국 소비자 금융 보호국에 불만을 제기하거나 불만을 제기하는 집단을 설정하여 사람들이 불만을 제기하고 신용 카드 모기지와 같은 금융 상품과 관련된 문제에 대해 말할 수 있는 미국 정부 기관입니다. 모기지 학생 금융 상품과 같은 관련 대출 내 신용 카드에 문제가 발생한 것과 같습니다. 대출에 문제가 발생했습니다. 회사는 공정하지 않습니다. 당신이 와서 당신이 그것에 대해 불평하기 때문에 나는 모든 불만을 가져 와서 그것을 우리의 고차원 공간과 저차원 공간을 구축하고 그 공간을보고이 공간에서 어떤 단어가 서로 관련되어 있는지 이해할 수 있습니다. 그래서 새로운 공간에서 임베딩에 의해 정의된 월이라는 단어는 연도와 같은 단어에 가장 가깝습니다. 복수 월별 할부 지불 따라서 이러한 임베딩에 의해 정의된 새로운 공간에서 신용 카드 또는 모기지와 같은 금융 상품의 맥락에서 의미가 있는 단어입니다. 오류라는 단어는 실수와 같은 단어에 가장 가깝습니다. 사무적인 오류 문제의 오류 또는 모기지 명세서에 오류가 있어서 이러한 종류의 또는 잘못된 의사소통 오해를 봅니다. 임베딩을 만들기 위해서는 많은 데이터가 필요하기 때문에 임베딩을 직접 만들어야 합니다. 그래서 미리 훈련된 단어 임베딩을 사용할 수 있습니다. e 그들이 액세스할 수 있는 방대한 양의 데이터와 당신은 아마 그렇지 않을 것이므로 해당 데이터 세트 중 하나를 살펴보겠습니다. 이 표에서 동일한 단어 오류에 대한 결과를 보여주지만 장갑 임베딩에 대한 결과를 보여줍니다. 모든 wikipedia와 같은 매우 큰 데이터 세트를 기반으로 생성된 사전 훈련된 임베딩 인터넷의 거대한 범위와 같은 모든 Google 뉴스 데이터 세트가 이러한 임베딩을 생성하기 위해 공급되었으므로 여기에서 가장 가까운 단어는 다음과 같습니다. 이전과 유사하지만 더 이상 사무적 불일치와 같은 해당 도메인 고유의 특성이 없으며 이제는 잘못된 의사 소통과 같은 문제가 있지만 이제 사람들이 금융 상품 불만 사항에 대해 이야기하지 않은 계산 및 확률이 있으므로 실제로 우리가 우리 자신을 만들고 이 컨텍스트에 특정한 관계를 배울 수 있기 전에 이것이 어떻게 작동하는지 강조하고 여기에서 배운 보다 일반적인 집합으로 이동합니다. d 다른 곳에서 임베딩이 대규모 텍스트 데이터 코퍼스에서 훈련되거나 학습되고 해당 코퍼스의 특성이 임베딩의 일부가 되므로 일반적으로 머신 러닝은 훈련 데이터에 있는 내용에 매우 민감합니다. 텍스트 데이터와 아마도 단어 임베딩을 다룰 때보다 더 분명한 것은 이것이 사실인 이러한 고전적인 예 중 하나와 같습니다. 이것은 말뭉치에 있는 인간의 편견이나 편견이 임베딩에 각인되는 방식에서 나타납니다. 편견이 있는 가장 일반적으로 사용 가능한 임베딩 중 일부를 볼 때 우리는 미국에서 아프리카계 미국인에게 더 흔한 아프리카계 미국인 이름이 유럽계 미국인 이름보다 더 불쾌한 감정과 관련이 있다는 것을 알 수 있습니다. 이러한 임베딩 공간에서 여성의 이름은 가족과 더 관련이 있고 남성의 이름은 경력 및 w와 관련된 용어와 더 관련이 있습니다. 징조는 예술과 더 관련이 있고 남성과 관련된 용어는 과학과 더 관련이 있으므로 실제로 편견이 단어 임베딩에 너무 깊이 뿌리박혀 있어 단어 임베딩 자체가 시간이 지남에 따라 사회적 태도의 변화를 정량화하는 데 사용될 수 있으므로 단어 임베딩이 아마도 과장되거나 극단적인 예이지만 텍스트 데이터와 관련하여 내리는 모든 기능 엔지니어링 결정은 우리가 보는 모델 성능과 얼마나 적절하거나 공정한지 모두에서 결과에 상당한 영향을 미친다는 것이 밝혀졌습니다. 우리 모델은 모든 것을 제공하므로 텍스트 데이터를 사전 처리하여 필요한 이러한 기능을 생성할 때 많은 옵션과 상당한 책임이 있으므로 제 조언은 항상 매우 깊이 이해할 수 있는 더 간단한 모델로 시작하는 것입니다. 다른 접근 방식을 시도할 때 모델 성능 향상에 속지 않도록 모델을 훈련하고 조정할 때 좋은 통계 방법을 채택해야 합니다. d 또한 모델 설명 도구와 프레임워크를 사용하여 덜 간단한 모델을 이해할 수 있도록 하기 위해 제 동료와 제가 이 모든 주제에 대해 썼고 Tidymodels와 함께 사용하는 방법을 썼습니다. 계속 그렇게 할 것입니다. 정말 감사하고 한국의 R 사용자 그룹 주최자에게 다시 한 번 감사드립니다. Rstudio의 Tidymodels 팀 동료와 동료에게 감사합니다. -저자 EMIL HVITFELDT."]},"columns":[{"accessor":"text","name":"text","type":"character","cell":["Hi my name is julia silge.  I'm a data scientist and software engineer at RStudio. and I'd like to thank the the organizers of the R user group in Korea so much for having me.  Today speak to you I am so happy to be speaking specifically today about creating features for machine learning from text data for a couple of reasons Having a better understanding of what we do to take text data and then to make it appropriate as an input for machine learning algorithms has many benefits both if you are directly getting ready to train a model or if you're at the beginning of some text analysis project or if you are trying to understand the behavior of a model that you're interacting with some way which is something that we do in our work as data scientists or in our in our daily lives more and more so when we build models for text either supervised or unsupervised we start with something like this this is some example text data that I'll use a couple of times during this talk that describes some animals I'm using some text data so you know to me as an english speaker looks familiar like I am as someone who uses a human language so I look at this and I can read it I could speak it aloud and I understand I can interpret it what it means so this kind of data this sort of natural language data is being generated all the time in all kinds of languages in all kinds of contexts so whether you work in healthcare in tech in finance basically any kind of organization this sort of text data is being generated by customers by clients by internal stakeholders inside of a business by people taking surveys via social media via business processes and in all this natural language there's information latent in that text data that can be used to make better decisions However, computers are not great at looking at this and doing math on language as it's represented like this and instead language has to be dramatically transformed to some kind of machine readable numeric representation that looks more like this what I'm showing here on the screen to be ready for almost any kind of model so I spent a fair amount of time working on software for people to be able to do exploratory data analysis, visualization, summarization tasks like that with text data in a tidy format where we have one observation per row and I love using tidy data principles for text analysis especially during those exploratory phases of an analysis when it comes time to build a model often what the underlying mathematical implementation really needs is typically something like this which is a way to this particular representation is called the document term matrix so the exact representation may differ from what I've shown here what I have here is we're weighting things by counts so each row in this matrix is a document each column is a is a word. A token and the numbers represent counts how many times does each document use each word you could weight it in a different way using say TF-IDF instead of counts or you might keep sequence information if you're interested in building a deep learning model but basically for all kinds of text modeling from simpler models like Naive Bayes models which work well for text to word embeddings to really the most state-of-the-art kind of work that's happening today like transformers for text data we have to heavily feature engineer and process language to get it to some kind of representation that's suitable for machine learning algorithms so I work on an open source framework in R for modeling and machine learning that's called Tidymodels and the examples that I'll be showing today use Tidymodels code some of the specific goals of the Tidymodels project are to provide a consistent flexible framework for real world modeling practitioners people who are you know doing that are dealing with real world data those who are just starting out to those who are very experienced in modeling and the goal is to harmonize the heterogeneous interfaces that exist within R and to encourage good statistical practice I'm glad to get to show you some of what I work on and build and how we apply it to text modeling but a lot of what I will talk about today isn't very specific to Tidymodels or even to R. I know this is an R user group but what we're going to talk about and focus on is a little more conceptual and basic how do we transform text into predictors for machine learning I am excited though to talk about Tidymodels and Tidymodels if you have not used it before is a meta package in a similar way that the Tidyverse is a meta package so if you've ever typed library Tidyverse and then you've used ggplot2 for visualization dplyr for data manipulation Tidymodels works in a similar way there are different packages inside of it that are used for different purposes so the pre-processing or the feature engineering is part of a broader model process you know it that process starts really with with exploratory data analysis that helps us decide what kind of model we will build and then it comes to completion I think I would argue with model evaluation when you measure how well your model performed Tidymodels as a piece of software is made up of our packages each of which has a specific focus like our sample is for re-sampling data to be able to create bootstrap resamples cross-validation resamples all different kinds of resamples you might want to use to train and evaluate models the tune package is for hyper parameter tuning as you might guess from the name one of these packages is for feature engineering for a data preprocessing feature engineering and it is the one that is called recipes so in Tidymodels we capture this idea of data pre-processing and feature engineering in the concept of a pre-processing recipe that has steps so you choose ingredients or variables that you're going to use then you define the steps that go into your recipe then you prepare them using training data and then you can apply that to any data set like testing data or new data at prediction time so the variables or ingredients that we use in modeling come in all kinds of shapes and sizes including text data so some of the techniques and approaches that we use for pre-processing text data are the same um as for any other kind of data that you might use like non-text data numeric data categorical data some for some of it is the same but some of what you need to know to be able to do a good job in this process for text is different and is specific to the nature of what language data is like and so I've written a book with my co-author Emile Hvitfeldt on supervised machine learning for text analysis and R and fully the first third of the book focuses on how we transform the natural language that we have in text data into features for modeling the middle section is about how we use these features in simpler or more traditional machine learning models like regularized regression or support vector machines and then the last third of the book talks about how we use deep learning models with text data so deep learning models still require these kinds of transformations from natural language into features as input for these kinds of models but deep learning models are often able to inherently learn structure of features from text in ways that those more traditional or simpler machine learning models are not so this book is now complete and available as of this month as of november folks are getting their first paper copies and also this book is available in its entirety at smalltar.com so if you're new to dealing with text data understanding these fundamental pre-processing approaches for text will set you up for being able to train effective models if you're really experienced with text data if you've dealt with it a lot already you've probably noticed like we have that the existing you know resources or literature whether that's books or tutorials or blog posts is quite sparse when it comes to detailed thoughtful explorations of how these pre-processing steps work and how choices made in these feature engineering steps impact our model output so let's walk through several of some of these like basic feature engineering approaches and how they work and what they do let's start out with tokenization so typically one of the first steps in transfer information from natural language to machine learning feature for really any kind of text analysis including exploratory data analysis or building a model. Anything is tokenization in tokenization we take an input some string some character vector and some kind of token type some meaningful unit of text we're interested in a word and we split the input pieces into tokens that correspond to the type we're interested in so most commonly the meaningful unit or type of token that we want to split text into units of is a word so this might seem straightforward or obvious but it turns out it's difficult to clearly define what a word is for many or even most languages so many languages do not use white space between words at all which presents a challenge for tokenization even languages that do use white space like english and korean often have particular examples that are ambiguous like contractions in english like didn't which should be you know maybe more accurately considered two words the way particles are used in Korean and how pronouns and negation words are written in romance languages like italian and french where they're stuck together and really maybe they should be considered two words once you have figured out what you're going to do and you make some choices and you tokenize your text then it's on its way to being able to be used in exploratory data analysis or unsupervised algorithms or as features for predictive modeling which is what we're talking about here and what these results show here so these results are from a regression model trained on descriptions of media from artwork in the Tate collection in the UK so what we're predicting in what we are predicting is when what year was a piece of art created based on the the medium that the artwork was created with and the medium is described with a little bit of text so we see here that artwork created using graphite watercolor and engraving was more likely to be created earlier that though that is more likely to come from older art and artwork that is created using photography screen point or sorry screen print screen printing and and dung and glitter are more likely to be created later. there this is more likely to come from contemporary art modern art so the the way that we tokenize this text you know we started with natural human generated texts of people writing out the descriptions of the the media that these art pieces of art were created with and the way we tokenized that natural human generated text that we started with has a big impact on what we learned from it if we tokenized in a different way we would have gotten different results in terms of performance like how accurately we were able to predict predict the year and also in terms of how we interpret the model like what is it that we're able to learn from it so this is one kind of tokenization to the single word but we also we all another way to tokenize instead of breaking up into single words or unigrams we can tokenize to n-grams so an n-gram is a continuous sequence of N items from a given sequence of texts so this shows that same piece of little bit of text i'm describing this animal divided up into bi-grams or n-grams of two tokens so notice how the words in the bi-grams overlap so the word collard appears in both of the first bigrams the collared collared peccary peccary also referred to so n-gram tokenization slides along the text to create overlapping sets of tokens this shows tri-grams for the same thing so using uni-grams one word is faster and more efficient but we don't capture information about word order I'm using a higher value two or three or even more keeps more complex information about word order and concepts that are described in multi-word phrases but the vector space of tokens increases dramatically that corresponds to a reduction in token counts we don't count each token as very many times and that means depending on your particular data set you might not be able to get good results so combining different degrees of n-grams can allow you to extract different levels of detail from text so uni-grams can tell you which individual words have been used a lot of times some of those words might be overlooked in bi-gram or tri-gram crowns if they don't co-appear with other words as often this plot compares model performance for a Lasso regression model predicting the year of supreme court opinions the United States supreme court opinions with three different degrees of n-grams what we're doing here is we are taking the text of the writings of the United States supreme court and we're predicting when did it when was that text written so can we predict how old a piece of text is from the contents of the text so holding the number of tokens constant at a thousand using uni-grams alone performs best for this corpus of opinions from the United States supreme court this is not always the case depending on the kind of model you use the data set itself we might see the best performance combining uni-grams and bi-grams or maybe some other option in this case if we wanted to incorporate some of that more complex information that we have in the bi-grams and the tri-grams we probably would need to increase the number of tokens in the model quite a bit so keep in mind when you look at results like these that identifying n-grams is computationally expensive this is especially compared to the amount of like a model the improvement in model performance that we often see like if we if we see some you know modest improvement by adding in bigrams it's important to keep in mind how much improvement we see relative to how long it takes to identify bi-grams and then train that model so for example for this data set of supreme court opinions where we held the number of tokens constant so the model training had the same number of tokens in it using bi-grams plus uni-grams takes twice as long to train to do the feature engineering and the training than only uni-grams and adding in tri-grams as well takes almost five times as long as training on uni-grams alone so this is a computationally expensive thing to do going in the other direction we can tokenize to units smaller than words so like these are what are called character shingles so we take words the collared peccary and we can instead of looking at words we can go down and look at sub word information there's multiple different ways to break words up into sub words that are appropriate for machine learning and often these kinds of approaches or algorithms have the benefit of being able to encode unknown or new words at prediction time so when it when it's time to make a prediction on new data it's not it's not uncommon for there to be new vocabulary words at that time and if we didn't see them in the training data you know what are we going to do about those new words when we train using subword information often we can handle those new words if we saw the subword in our training data set so using this kind of subword information is a way to incorporate morphological sequences into our models of you know various kinds of this is something that applies to various languages not just english so these results are for a classification model with a data set of very short texts it's just the names of post offices in the United States so super short and the goal of the model was to predict the post office located in hawaii in the middle of the pacific ocean or it located in the rest of the united states so I created features for the model that are subwords of these post office names and we end up learning that the names that start with h and p or contain that ale sub word are more likely to be in hawaii and the sub words a and d and ri and ing are are more likely to come from the post office that are outside of hawaii so this is an example of how we tokenized differently and we're able to learn something new we're able to learn something different so in Tidymodels we collect all these kinds of decisions about tokenization and code that looks like this so we start with a recipe that specifies what variables or ingredients that we'll use and then we define these preprocessing steps so even at this first and arguably you know simple and basic step the choices that we make affect our modeling results in a big way the next pre-processing um step that I want to talk about is stop words so once we have split text into tokens we often find that not all words carry the same amount of information if maybe any information at all actually for a machine learning task so common words that carry little or perhaps no meaningful information are called stopwords so this is one of the stopword lists that's available for Korean so it's common advice and practice to say hey just remove just remove remove these stopwords for a lot of natural language processing tasks what I'm showing here is the entirety of one of the shorter english stopword lists that's used really broadly so you know it's words like I me my pronouns conjunctions and of the and these are very common words that are not considered super important the decision though to just remove stopwords is often more involved and perhaps more fraught than what you'll than what you'll find reflected in a lot of resources that are out there so almost all the time real world NLP practitioners use pre-made stopword lists so this plot visualizes set intersections for three common stopword lists in english in what is called an upset plot so the three lists are called the snowball list smart and the iso list so you can see the the lengths of the list are represented by the length of the bars and then we see the intersections which words are in common on these lists by the by the vertical bars so the lengths of the list are quite different and also notice they don't all contain the same sets of words the important thing to remember about stopword lexicons is that they are not created in some neutral perfect setting but instead they are they are context specific they they can be biased both of these things are true because they are lists created from large data sets of language so they reflect the characteristics of the data used in their creation so this is the ten words that are in the english language smart lexicon but not in the English snowball lexicon so notice that they're all contractions but that's not because the snowball exchange doesn't include contractions it has a lot of them also notice that it has that she's is on this list and so that means that that list has he's but it does not have the list she's so this is an example of that The bias I mentioned that occurs because these lists are created from large data sets of text lexicon creators look at the most frequent words in some big corpus of language they make a cut off and then some decisions about what to include or exclude you know based on the list that they have and you end up here so because in many large data sets of language you have more representation of men you end up with a situation like this where a stopword list will have he's but not she's so many decisions when it comes to modeling or analysis with language we as practitioners have to decide what is appropriate for our particular domain it turns out this is even true when it comes to picking a stopword list so in Tidymodels we can implement a pre-processing step like removing stopwords by adding an additional step to our recipe so first we specified what variables we would use then we tokenized the text and now we are removing stopwords here using just the default step since we are not passing in any other arguments we could though use a non-default step or even a custom list if that was most appropriate to our domain this plot compares the model performance for predicting the year of that same data set of supreme court opinions with three different stopword lexicons of different lengths so the snowball lexicon contains the smallest number of words and in this case it results in the best performance so removing fewer stopwords results in the best performance here so this specific result is not generalizable to all data sets and contexts but the fact that removing different sets of stopwords can have noticeably different effects on your model that is quite transferable so the only way to know what is the best thing to do is to try several options and see so machine learning in general. this is an empirical field right like we don't know we don't often have reasons a priori to know what will be the best thing to do and so typically we have to try a different option to see what will be the best thing all right. then the the third pre-processing step that I want to talk about for text is stemming so when we deal with text often documents contain different versions of one base word often called a stem so what if say for an english example if we aren't interested in the difference between animals plural and animal singular and we want to treat them both together so that idea is at the heart of stemming so there's no one right way or correct way to stem text so this plot shows three approaches for stemming in English starting from hey let's just remove a final s to more complex rules about plural handling plural endings that middle one it is called the s stemmer it's a set of it's like a little set of rules and that last one is the best known one probably the best-known implementation of stemming in English called the Porter algorithm so you can see here that Porter stemming is the most different from the other two in the top 20 words here from the data set of animal descriptions that I've been using we see how the word species was treated differently animal predator this sort of collection of words live living life lives that was treated differently so practitioners are typically interested in stemming text data because it buckets tokens together that we believe belong together in in a way that we understand that as human users of language so we can use approaches like this which are pretty like step-by-step rules based this is typically called stemming or and it's fairly algorithmic in nature like first do this then do this then do this or you can use lemmatization which is usually based on large dictionaries of words and it incorporates like a linguistic understanding of what words belong together so most of the existing approaches for this kind of task in Korean are are limited lemmatizers based on these dictionaries and that are trained using large data sets of language so this seems like it's going to be a helpful thing to do when you hear about this you're like oh yeah sounds good,  sounds smart especially because with text data we are typically overwhelmed with features with numbers of tokens this is typically the situation when we're dealing with text data so here we have these animal description data and I made a matrix representation of it like we would typically use in some machine learning algorithm and look how many features there are 16,000 almost 17,000 features that's the number of features that would be going into the model look at the sparsity 98 percent sparse that's high very sparse data so this is the sparsity of the data that will go into the machine learning algorithm to build our supervised machine learning model if we stem the words if I use here an approach for stemming we reduce the number of word features by many thousands the sparsity unfortunately did not change as much but we reduced the number of features by a lot by bucketing those words together that our stemming algorithm belong together so you know common sense says reducing the number of words features so dramatically is going to perform improve the performance of our machine learning model but that is that does assume that we have not lost any important information by by stemming and it turns out that stemming or lemmatization can often be very helpful in some contexts but the typical algorithms used for these are somewhat aggressive and they have been built to favor sensitivity or recall or the true positive rate and this is at the expense of the specificity or the precision or the true negative rate so in a supervised machine learning context what this does is this affects a model's positive predictive value the precision or its ability to to not incorrectly label true negatives as positive I hope I got that right so you know to make this more concrete stemming can increase a model's ability to find the positive examples of say the animal descriptions that are associated with say a certain diet if that's what we're modeling however if text is over stemmed the resulting model loses its ability to label the negative examples say the descriptions that are not about that diet that's what we're looking for and this can be a real challenge when training models with text data kind of finding that that balance there because often we don't have a dial that we can change on these stemming on these stemming algorithms so even just very basic pre-processing for text like what I'm showing here in this feature engineering recipe can be computationally expensive and the choices that a practitioner makes like whether or not to remove stopwords or to stem text can have dramatic impact on how machine learning models of all kinds perform whether those are simpler models more traditional machine learning models or deep learning models what this means is that the price the prioritization that we as practitioners give to like learning teaching and writing about feature engineering steps for text really contributes to better more robust statistical practice in our field I mentioned before the sparsity of text data and I want to come back to that because it is one of text data's really defining characteristics because of just how language works we use a few words a lot of times and then a lot of words only just a couple of times only a few a few times and with a real set of natural language you end up with relationships that look like this that look like these plots in terms of how the sparsity changes as you add more documents and more unique words to a corpus so the sparsity goes up real fast as you add more unique words and the memory that is required to handle this set of documents goes up very fast so even if you use specialized data structures meant to store sparse data like sparse matrices you still end up growing the memory required to handle these data sets in a very non-linear way it still grows up very fast so this means it can take a very long time to train your model or even that you outgrow the memory available on your machine you have to go to the cloud to an expensive big memory situation this can be a real challenge and this challenge it is what has behind the motivating of vector languages for models so linguists have worked for a long time on vector languages for models that can reduce the number of dimensions representing text data based on how people use language so this quote here goes all the way back to 1957. so the idea here is that we use like the data is very sparse but we don't use words randomly it's not independent the words are not used independently of each other but rather there's relationships that exist between how words are used together and we can use those relationships to create to transform our sparse high dimensional space into a special dense low dimensional space lower we still has like 100 dimensions but much lower than the many thousands hundreds tens hundreds of thousands of space so the idea here we use statistical modeling maybe just word counts plus matrix factorization maybe fancier math that involves neural networks to take this really high dimensional space and we create a new lower dimensional lower dimensional space that is special because the new space is created based on vectors that incorporate information about which words are used together so you shall know a word by the company it keeps so you need a big data set of text to create or learn these kinds of word vectors or word embeddings so this table that I'm showing right now it's from a set of embeddings that I created using a data set or a corpus of complaints complaints to the United States consumer financial protection bureau so this is a government body in the United States where people can complain and say what is wrong with something to do with a financial product like a credit card a mortgage a student loan something to do with like a financial product they're like something went wrong with my credit card something went wrong with my mortgage that company is not being fair so you come and you complain to it so I took all those complaints and built it's our high dimensional space and build a low dimensional space and we can look in that space and understand what words are related to each other in this space so in the new space defined by the embeddings the word month is closest to words like year months plural monthly installments payment so these are words that are that makes sense in the context of financial products like credit cards or mortgages in the new space defined by these embeddings the word error is closest to the words like mistake clerical like a clerical mistake problem glitch or there was a glitch on my mortgage statement so we see these kinds of or miscommunication misunderstanding you know like these are these are words that are used in similar ways so you don't have to create embeddings yourself because it requires quite a lot of data to make them so you can use word embeddings that are pre-trained i.e created by someone else based on some huge corpus of data that they have access to and you probably don't so let's look at one of those data sets let's look at this table shows the results for the same word error but for the glove embeddings so the glove embeddings are a set of pre-trained embeddings that are created based on a very large data set that's like all of wikipedia all of the google news data set just like huge swaths of the internet have been fed in to create these embeddings so some of the closest words here are similar to those that are before but we no longer have some of that domain specific flavor like clerical discrepancy and now we have like miscommunication you know but and now we have calculation and probability which people were not talking about with their financial product complaints so this really highlights how these how these work here before we we created our own and we were able to learn relationships that were specific to this context and here we go to a more general set that that was learned somewhere else so embeddings are trained or learned from a large corpus of text data and the characteristics of that corpus become part of the embeddings so machine learning in general you know is exquisitely sensitive to whatever it is that's in your training data and this is never more obvious than when dealing with text data and perhaps with word embeddings is just like one of these classic examples where this is true it turns out that this shows up in how any human prejudice or bias in the corpus becomes imprinted into the embeddings so in fact when we look at some of these most commonly available embeddings that are out there bias is we we see that african-american first names that are more common for african americans in the United States they're associated with more unpleasant feelings than European American first names in these embedding spaces women's first names are more associated with family and men's first names are more associated with career and terms associated with women are more associated with the arts and terms associated with men are more associated with science so it turns out actually bias is so ingrained in word embeddings that the word embeddings themselves can be used to quantify change in social attitudes over time so word embeddings are maybe an exaggerated or extreme example but it turns out that all the feature engineering decisions that we make when it comes to text data have a significant effect on our results both in terms of the model performance that we see and also in terms of how appropriate or fair our models are so given all that when it comes to pre-processing your text data creating these features that you need you have a lot of options and quite a bit of responsibility so my advice is always start with simpler models that you can understand quite deeply be sure to adopt good statistical practices as you train and tune your models so you aren't fooled about model performance improvements when you try different approaches and also to use model explainability tools and frameworks so you can understand any less straightforward models that you try so my co-workers and I have written about all of these topics and how to use them with Tidymodels if that's what you like to use and we will continue to do so with that i will say thank you so very much and I want to be sure to again thank the organizers of the R user group in Korea I want to thank my teammates on the Tidymodels team at Rstudio as well as my co-author EMIL HVITFELDT."],"header":"text","minWidth":70,"align":"center","headerStyle":{"background":"#f7f7f8"}},{"accessor":"translatedText","name":"translatedText","type":"character","cell":["안녕하세요 제 이름은 줄리아 실지입니다. 저는 RStudio의 데이터 과학자이자 소프트웨어 엔지니어입니다. 그리고 저를 허락해주신 한국 R 유저 그룹의 주최측에 감사드립니다. 오늘은 몇 가지 이유로 텍스트 데이터에서 기계 학습을 위한 기능을 생성하는 방법에 대해 오늘 구체적으로 이야기하게 되어 매우 기쁩니다. 학습 알고리즘은 모델을 직접 훈련할 준비를 하거나 일부 텍스트 분석 프로젝트를 시작하는 경우 또는 어떤 방식으로든 상호 작용하는 모델의 동작을 이해하려는 경우 모두 많은 이점이 있습니다. 우리가 데이터 과학자로서 일하거나 일상 생활에서 점점 더 많이 하는 것입니다. 그래서 감독 또는 감독되지 않은 텍스트에 대한 모델을 구축할 때 다음과 같이 시작합니다. 다음은 제가 사용할 텍스트 데이터의 예입니다. 동물에 대해 설명하는 이 강연에서 몇 번이고 텍스트 데이터를 사용하고 있습니다. 그래서 영어 사용자인 저에게 친숙한 사람처럼 보이기 때문에 이것을 보고 읽을 수 있습니다. 큰 소리로 말하고 이해합니다. 의미를 해석할 수 있습니다. 그래서 이런 종류의 데이터 이런 종류의 자연어 데이터는 모든 종류의 컨텍스트에서 모든 종류의 언어로 항상 생성되고 있으므로 기술 분야의 의료 분야에서 일하든 금융 기본적으로 모든 종류의 조직 이러한 종류의 텍스트 데이터는 비즈니스 프로세스를 통해 소셜 미디어를 통해 설문 조사에 참여하는 사람들이 비즈니스 내부의 내부 이해 관계자에 의해 고객에 의해 고객에 의해 생성되며 이 모든 자연 언어에는 해당 텍스트 데이터에 잠재된 정보가 있습니다. 더 나은 결정을 내리는 데 사용되어야 합니다. 그러나 컴퓨터는 이것을 보고 언어에 대해 수학을 수행하는 데 능숙하지 않습니다. 이렇게 표현되기 때문에 대신 언어는 기계가 읽을 수 있는 일종의 숫자 표현으로 극적으로 변환되어야 합니다. m 거의 모든 종류의 모델에 사용할 수 있도록 화면에 표시되므로 사람들이 ex를 할 수 있도록 소프트웨어 작업에 상당한 시간을 할애했습니다. 행당 하나의 관찰이 있는 깔끔한 형식의 텍스트 데이터와 같은 탐색적 데이터 분석, 시각화, 요약 작업 특히 모델을 구축할 때 분석의 탐색 단계에서 텍스트 분석을 위해 깔끔한 데이터 원칙을 사용하는 것을 좋아합니다. 종종 기본 수학적 구현이 실제로 필요로 하는 것은 일반적으로 이와 같은 것입니다. 이 특정 표현에 대한 방법은 문서 용어 행렬이라고 하므로 정확한 표현은 여기에 표시된 것과 다를 수 있습니다. 여기에 있는 것은 가중치를 적용한다는 것입니다 이 행렬의 각 행은 문서이므로 각 열은 단어입니다. 토큰과 숫자는 각 문서가 각 단어를 몇 번 사용하는지를 나타냅니다. 카운트 대신 TF-IDF를 사용하여 다른 방식으로 가중치를 부여하거나 딥 러닝 모델을 구축하는 데 관심이 있는 경우 시퀀스 정보를 유지할 수 있습니다. 기본적으로 텍스트에서 단어로의 임베딩에 잘 작동하는 Naive Bayes 모델과 같은 단순한 모델에서 텍스트 데이터용 변환기와 같이 오늘날 일어나고 있는 가장 최첨단 작업에 이르기까지 모든 종류의 텍스트 모델링을 위해 많은 기능을 엔지니어링해야 합니다. 기계 학습 알고리즘에 적합한 일종의 표현을 얻기 위해 처리 언어를 사용합니다. 그래서 저는 Tidymodels라고 하는 모델링 및 기계 학습을 위한 R의 오픈 소스 프레임워크에서 작업하고 있으며 오늘 보여드릴 예제에서는 Tidymodels 코드 중 일부를 사용합니다. Tidymodels 프로젝트의 특정 목표는 실제 작업을 처리하는 실제 모델링 실무자에게 일관되고 유연한 프레임워크를 제공하는 것입니다. 세계 데이터 모델링을 시작한지 얼마 되지 않은 사람들, 그리고 목표는 R 내에 존재하는 이기종 인터페이스를 조화시키고 좋은 통계 관행을 장려하는 것입니다. 제가 작업하고 있는 몇 가지를 보여드리게 되어 기쁩니다. 빌드하고 텍스트 모델링에 적용하는 방법에 대해 설명하지만 오늘 이야기할 내용은 Tidymodels 또는 R에만 국한된 것이 아닙니다. 이것이 R 사용자 그룹이라는 것을 알고 있지만 우리가 이야기할 내용과 초점은 좀 더 개념적이고 기본적입니다. 기계 학습을 위해 텍스트를 예측자로 변환하는 방법 이전에 사용하지 않은 경우 Tidymodels 및 Tidymodels에 대해 이야기하게 되어 기쁩니다. Tidyverse가 메타 패키지인 것과 유사한 방식으로 메타 패키지입니다. 패키지이므로 Tidyverse 라이브러리를 입력한 다음 시각화를 위해 ggplot2를 사용한 경우 데이터 조작을 위한 dplyr Tidymodels는 유사한 방식으로 작동합니다. 피쳐 엔지니어링은 더 광범위한 모델 프로세스의 일부입니다. 프로세스는 실제로 어떤 종류의 모델을 구축할지 결정하는 데 도움이 되는 탐색적 데이터 분석으로 시작하여 완성됩니다. 잘 수행된 모델 소프트웨어의 한 조각으로서 Tidymodels는 우리의 패키지로 구성되어 있습니다. 각 패키지는 우리의 샘플과 같이 특정 초점을 가지고 있습니다. 부트스트랩 재샘플을 생성할 수 있도록 데이터를 재샘플링하는 것입니다. 모델을 훈련하고 평가하는 데 사용하고 싶은 튜닝 패키지는 이름에서 짐작할 수 있듯이 하이퍼 매개변수 튜닝을 위한 것입니다. 이 패키지 중 하나는 데이터 전처리 기능 엔지니어링을 위한 기능 엔지니어링을 위한 것이며 Tidymodels에서 우리는 레시피라고 하는 것입니다. 단계가 있는 사전 처리 레시피의 개념에서 데이터 사전 처리 및 기능 엔지니어링에 대한 아이디어를 캡처하여 원하는 재료 또는 변수를 선택합니다. 다시 사용할 것입니다. 그런 다음 레시피에 들어가는 단계를 정의한 다음 훈련 데이터를 사용하여 준비한 다음 예측 시간에 테스트 데이터 또는 새 데이터와 같은 모든 데이터 세트에 적용할 수 있으므로 모델링에 사용하는 변수 또는 성분 텍스트 데이터를 포함한 모든 종류의 모양과 크기로 제공되므로 텍스트 데이터를 사전 처리하는 데 사용하는 일부 기술과 접근 방식은 텍스트가 아닌 데이터와 같이 사용할 수 있는 다른 종류의 데이터와 동일합니다. 숫자 데이터 범주형 데이터 중 일부는 동일하지만 텍스트에 대한 이 프로세스에서 좋은 작업을 수행하기 위해 알아야 할 일부는 다르며 언어 데이터의 특성에 따라 다릅니다. 공동 저자인 Emile Hvitfeldt와 함께 텍스트 분석 및 R을 위한 감독된 기계 학습에 관한 책과 이 책의 1/3은 완전히 텍스트 데이터에 있는 자연어를 중간 섹션을 모델링하기 위한 기능으로 변환하는 방법에 초점을 맞추고 있습니다. 우리는 t를 사용합니다 정규화된 회귀 또는 지원 벡터 머신과 같은 더 단순하거나 더 전통적인 머신 러닝 모델의 이러한 기능과 책의 마지막 3분의 1은 텍스트 데이터와 함께 딥 러닝 모델을 사용하는 방법에 대해 설명하므로 딥 러닝 모델은 여전히 자연어에서 이러한 종류의 변환이 필요합니다. 기능은 이러한 종류의 모델에 대한 입력으로 사용되지만 딥 러닝 모델은 더 전통적이거나 더 간단한 기계 학습 모델이 아닌 방식으로 텍스트에서 기능 구조를 본질적으로 학습할 수 있으므로 이 책은 현재 이달 현재 완성되어 사용할 수 있습니다. 11월 사람들은 첫 종이 사본을 받고 있으며 이 책 전체는 smalltar.com에서 구할 수 있습니다. 따라서 텍스트 데이터를 처음 다루는 경우 텍스트에 대한 이러한 기본적인 사전 처리 접근 방식을 이해하면 이미 텍스트 데이터를 많이 다루었다면 텍스트 데이터에 대한 경험이 있는 경우 효과적인 모델이 될 것입니다. 기존 리소스나 문헌은 책이든 튜토리얼이든 블로그 게시물이든 이러한 사전 처리 단계가 작동하는 방식과 이러한 기능 엔지니어링 단계에서 선택한 사항이 모델 출력에 미치는 영향에 대한 자세한 사려 깊은 탐구와 관련하여 매우 희소합니다. 기본적인 기능 엔지니어링 접근 방식과 작동 방식 및 수행하는 작업과 같은 몇 가지를 통해 토큰화부터 시작하겠습니다. 따라서 일반적으로 탐색을 포함한 모든 종류의 텍스트 분석을 위해 자연어에서 기계 학습 기능으로 정보를 전송하는 첫 번째 단계 중 하나입니다. 데이터 분석 또는 모델 구축. 토큰화에서 모든 것은 토큰화입니다. 우리는 입력을 일부 문자열, 일부 문자 벡터 및 일종의 토큰 유형으로 우리가 관심 있는 단어에 관심이 있는 의미 있는 텍스트 단위를 취하고 입력 조각을 우리가 관심 있는 유형에 해당하는 토큰으로 분할합니다. 텍스트를 단위로 분할하려는 가장 일반적으로 의미 있는 단위 또는 토큰 유형은 단어입니다. 언어는 단어 사이에 공백을 전혀 사용하지 않아 토큰화에 어려움을 겪습니다. 심지어 영어와 한국어와 같이 공백을 사용하는 언어에도 종종 영어의 축약형처럼 모호한 특정 예가 있습니다. 한국어에서 입자가 사용되는 방식과 이탈리아어, 프랑스어와 같은 로맨스 언어에서 대명사와 부정 단어가 서로 붙어있는 방식으로 쓰여지는 두 단어로 간주됩니다. 그리고 실제로 무엇을 할 것인지 파악하고 몇 가지 선택을 하고 텍스트를 토큰화하면 두 단어로 간주되어야 합니다. 그러면 탐색적 데이터 분석이나 감독되지 않은 알고리즘 또는 예측 모델링을 위한 기능으로 여기에서 우리가 이야기하고 이러한 결과가 여기에 표시되는 것이므로 이러한 결과는 영국의 Tate 컬렉션에 있는 예술 작품의 미디어 설명에 대해 훈련된 회귀 모델에서 나온 것이므로 우리가 무엇에서 예측하고 있는지 작품을 만든 매체와 매체가 약간의 텍스트로 설명되어 있어 흑연 수채화와 판화를 사용하여 만든 작품이 몇 년도에 만들어졌는지 예측하고 있습니다. 사진 스크린 포인트 또는 유감 스크린 인쇄 스크린 인쇄를 사용하여 만든 오래된 예술 및 예술 작품에서 나올 가능성이 더 높지만 똥과 반짝이는 mor e 나중에 생성될 가능성이 있습니다. 거기에서 이것은 현대 미술 현대 미술에서 나올 가능성이 더 높기 때문에 이 텍스트를 토큰화하는 방법은 이 예술 작품이 만들어지는 미디어에 대한 설명을 작성하는 사람들의 자연스러운 인간 생성 텍스트로 시작했습니다. 우리가 시작한 자연적인 인간 생성 텍스트를 토큰화한 방식은 우리가 다른 방식으로 토큰화했다면 우리가 예측할 수 있었던 정확도와 같은 성능 측면에서 다른 결과를 얻었을 것입니다. 연도 및 또한 모델을 해석하는 방법의 측면에서 우리가 배울 수 있는 것은 무엇입니까? 이것은 단일 단어에 대한 일종의 토큰화이지만 우리는 또한 단일 단어로 분해하는 대신 토큰화하는 또 다른 방법입니다. 단어 또는 유니그램을 n-그램으로 토큰화할 수 있으므로 n-그램은 주어진 텍스트 시퀀스에서 N개 항목의 연속 시퀀스이므로 이 동물을 설명하는 동일한 텍스트 조각이 다음으로 나뉩니다. bi-grams 또는 n-grams of two tokens 그래서 bi-grams의 단어가 어떻게 겹치는지 확인하여 collard라는 단어가 첫 번째 bigrams 모두에 나타납니다. collared collared peccary라고도 하는 n-gram 토큰화가 텍스트를 따라 슬라이드 겹치는 토큰 세트 생성 이것은 동일한 것에 대한 트라이그램을 보여주므로 유니그램을 사용하면 한 단어가 더 빠르고 효율적이지만 단어 순서에 대한 정보는 캡처하지 않습니다. 더 높은 값을 사용하고 있습니다. 여러 단어로 된 구로 설명된 단어 순서 및 개념에 대한 더 복잡한 정보이지만 토큰의 벡터 공간은 토큰 수의 감소에 따라 극적으로 증가합니다. 이는 각 토큰을 여러 번 계산하지 않으며 이는 귀하의 특정 항목에 따라 달라짐을 의미합니다. 데이터 세트를 사용하면 좋은 결과를 얻지 못할 수 있으므로 서로 다른 정도의 n-gram을 결합하면 텍스트에서 다양한 수준의 세부 정보를 추출할 수 있으므로 uni-gram은 어떤 개별 단어가 많이 사용되었는지 알려줄 수 있습니다. 이러한 단어 중 일부는 다른 단어와 함께 나타나지 않는 경우 바이그램 또는 트라이그램 크라운에서 간과될 수 있습니다. 이 플롯은 대법원 판결의 연도를 예측하는 올가미 회귀 모델의 모델 성능을 비교합니다. 미국 대법원 3가지 다른 등급의 n-gram을 사용하는 법원 의견 우리가 여기서 하는 것은 우리가 미국 대법원의 문서를 가져오는 것입니다. 그리고 우리는 그 문서가 언제 작성되었는지 예측하고 있으므로 얼마나 오래되었는지 예측할 수 있습니다. 텍스트의 일부는 텍스트의 내용에서 가져온 것이므로 유니그램만 사용하여 토큰의 수를 1000으로 일정하게 유지하는 것이 미국 대법원의 의견 모음에 가장 적합합니다. 모델 자체를 사용하여 데이터 세트 자체를 사용하면 유니그램과 바이그램을 결합한 최상의 성능을 볼 수 있으며, 이 경우 바이그램 및 트라이그램의 경우 모델의 토큰 수를 상당히 늘려야 할 수도 있으므로 이러한 결과를 볼 때 n-그램을 식별하는 것은 계산적으로 비용이 많이 든다는 점을 염두에 두십시오. 이는 특히 모델과 같은 양과 비교됩니다. 우리가 흔히 볼 수 있는 모델 성능의 개선 빅그램을 추가하여 약간의 개선을 알면 바이그램을 식별한 다음 해당 모델을 훈련하는 데 걸리는 시간에 비해 얼마나 개선되었는지를 염두에 두는 것이 중요합니다. 예를 들어 이 데이터 세트의 경우 토큰 수를 일정하게 유지하여 모델 교육에 동일한 수의 토큰이 포함되어 바이그램과 유니그램을 사용하여 기능 엔지니어링을 수행하는 데 두 배의 시간이 걸립니다. 유니그램보다 훈련하고 트라이그램을 더하는 것 역시 유니그램만 훈련하는 것보다 거의 5배나 오래 걸리므로 토큰화할 수 있는 다른 방향으로 이동하는 데 계산적으로 많은 비용이 드는 일입니다. 단어보다 작은 단위로 이렇게 하는 것이 문자 대상 포진이라고 하는 것입니다. 그래서 우리는 칼라가 있는 페커리 단어를 사용합니다. 단어를 보는 대신 아래로 내려가서 하위 단어 정보를 볼 수 있습니다. 단어를 하위 단어로 나누는 여러 다른 방법이 있습니다. 머신 러닝에 적합하고 종종 이러한 종류의 접근 방식이나 알고리즘은 예측 시간에 알려지지 않은 단어나 새로운 단어를 인코딩할 수 있다는 이점이 있습니다. 그 당시에 새로운 단어가 있었고 훈련 데이터에서 그 단어를 보지 못했다면 하위 단어 정보를 자주 사용하여 훈련할 때 새로운 단어에 대해 무엇을 할 것인지 알 수 있습니다. 우리의 훈련 데이터 세트이므로 이러한 종류의 하위 단어 정보를 사용하는 것은 형태학적 시퀀스를 우리의 모델에 통합하는 방법입니다. 영어이므로 이 결과는 매우 짧은 텍스트의 데이터 세트가 있는 분류 모델에 대한 것입니다. 미국의 우체국 이름일 뿐이므로 매우 짧고 모델의 목표는 하와이 한가운데에 위치한 우체국을 예측하는 것이었습니다. 태평양 또는 미국의 나머지 지역에 위치하므로 이 우체국 이름의 하위 단어인 모델에 대한 기능을 만들었으며 결국 h와 p로 시작하거나 해당 ale 하위 단어를 포함하는 이름이 더 많다는 것을 알게 되었습니다. 하와이에 있을 가능성이 있고 하위 단어 a 및 d 및 ri 및 ing은 하와이 외부에 있는 우체국에서 올 가능성이 더 높으므로 이것은 우리가 어떻게 다르게 토큰화하고 새로운 것을 배울 수 있는지에 대한 예입니다 우리는 다른 것을 배울 수 있으므로 Tidymodels에서 토큰화 및 이와 같은 코드에 대한 모든 종류의 결정을 수집하므로 사용할 변수 또는 구성 요소를 지정하는 레시피로 시작한 다음 이러한 사전 절차를 정의합니다. 첫 번째 단계에서도 간단하고 기본적인 단계를 선택하면 모델링 결과에 큰 영향을 미칩니다. 제가 이야기하고 싶은 다음 사전 처리 단계는 단어를 중지하는 것이므로 텍스트를 분할하면 토큰으로 우리는 종종 모든 단어가 같은 양의 정보를 전달하는 것은 아니라는 사실을 발견합니다. 실제로 기계 학습 작업에 대한 정보가 전혀 없다면 의미 있는 정보가 거의 또는 전혀 전달되지 않는 일반적인 단어를 불용어라고 하므로 이것이 불용어 목록 중 하나입니다. 한국어로 사용할 수 있으므로 많은 자연어 처리 작업을 위해 이 불용어를 제거하십시오. I me와 같은 단어라는 것을 알고 있습니다. 내 대명사 접속사 및 of and 이것들은 매우 중요하지 않은 매우 일반적인 단어이지만 불용어를 제거하는 것이 더 자주 결정됩니다. 많은 리소스에 반영되어 있는 것보다 더 복잡하고 복잡할 수 있습니다. 그래서 거의 항상 실제 NLP 실무자들은 미리 만들어진 불용어 목록을 사용하므로 이 플롯은 세 가지 일반적인 영어로 된 불용어 목록은 업셋 플롯이라고 하므로 세 개의 목록을 눈덩이 목록 smart라고 하고 iso 목록이라고 하여 목록의 길이가 막대의 길이로 표시되는 것을 볼 수 있습니다. 그런 다음 교차점을 봅니다. 단어는 수직 막대로 이러한 목록에서 공통적이므로 목록의 길이가 상당히 다르며 모두 동일한 단어 집합을 포함하지 않는다는 점에 유의하십시오. 불용어 사전에 대해 기억해야 할 중요한 점은 해당 단어가 생성되지 않는다는 것입니다 일부 중립적인 완벽한 설정에서는 대신 문맥에 따라 다르며 편향될 수 있습니다. 이 두 가지 모두 사실은 언어의 큰 데이터 세트에서 생성된 목록이므로 문자를 반영하기 때문입니다. 생성에 사용된 데이터의 ristics 그래서 이것은 영어 스마트 어휘집에는 있지만 영어 눈덩이 어휘집에는 없는 10개의 단어입니다. 그래서 그것들이 모두 축약형임을 주목하십시오. 그러나 그것은 눈덩이 교환이 축약어를 포함하지 않기 때문이 아닙니다. 그들 중 많은 사람들이 그녀가 이 목록에 있다는 것을 알아차렸고 그래서 그 목록에는 그가 있지만 그녀의 목록은 없다는 것을 의미합니다. 그래서 이것은 내가 언급한 편향의 한 예입니다. 방대한 텍스트 데이터 세트에서 생성된 어휘집 작성자는 큰 언어 모음에서 가장 자주 사용되는 단어를 살펴보고 잘라낸 다음 포함하거나 제외할 항목에 대한 몇 가지 결정을 내립니다. 따라서 많은 대규모 언어 데이터 세트에서 남성의 표현이 더 많기 때문에 불용어 목록에는 남성이 있지만 랑구로 모델링하거나 분석할 때 그녀는 결정을 내리지 못하는 상황이 발생합니다. 우리는 실무자로서 우리의 특정 도메인에 적합한 것을 결정해야 합니다. 이는 불용어 목록을 선택하는 경우에도 마찬가지입니다. 따라서 Tidymodels에서는 추가 단계를 레시피 그래서 먼저 사용할 변수를 지정한 다음 텍스트를 토큰화했습니다. 이제 기본 단계가 아닌 단계나 사용자 정의 목록을 사용할 수 있는 다른 인수를 전달하지 않기 때문에 기본 단계만 사용하여 여기서 불용어를 제거합니다. 그것이 우리 영역에 가장 적절한 경우 이 플롯은 동일한 대법원 의견 데이터 세트의 연도를 예측하기 위한 모델 성능을 길이가 다른 3개의 다른 불용어 사전과 비교하여 눈덩이 사전이 가장 적은 수의 단어를 포함하도록 합니다. 결과적으로 최고의 성능을 제공하므로 더 적은 수의 불용어를 제거하면 여기에서 최상의 성능을 얻을 수 있으므로 이 특정 결과를 모든 데이터 세트 및 컨텍스트에 일반화할 수 없습니다. 그러나 서로 다른 불용어 세트를 제거하면 모델에 눈에 띄게 다른 영향을 미칠 수 있고 이는 상당히 이전할 수 있으므로 가장 좋은 방법을 아는 유일한 방법은 여러 옵션을 시도하고 일반적으로 기계 학습을 확인하는 것입니다. 이것은 우리가 알지 못하는 것과 같은 경험적 필드입니다. 우리는 종종 가장 좋은 일이 무엇인지 알아야 할 선험적 이유가 없으므로 일반적으로 가장 좋은 일이 무엇인지 확인하기 위해 다른 옵션을 시도해야 합니다. 오른쪽. 그런 다음 텍스트에 대해 이야기하고 싶은 세 번째 전처리 단계는 형태소 분석입니다. 그래서 우리가 텍스트를 다룰 때 문서에는 종종 줄기라고 불리는 하나의 기본 단어의 다른 버전이 포함됩니다. 동물의 복수형과 동물의 단수형의 차이점에 관심이 있고 우리는 두 가지를 함께 취급하여 아이디어가 형태소 분석의 핵심이므로 텍스트를 줄기하는 올바른 방법이나 올바른 방법이 없으므로 이 플롯은 영어로 시작하는 형태소 분석에 대한 세 가지 접근 방식을 보여줍니다. 이봐, 복수형 처리에 대한 더 복잡한 규칙에 대한 마지막 s를 제거합시다. 그 중간에 있는 것은 s 형태소 분석기라고 하며, 그 집합은 일종의 규칙 집합과 같으며 마지막 것이 가장 잘 알려진 것일 수 있습니다. Porter 알고리즘이라고 하는 영어로 형태소 분석의 알려진 구현은 여기에서 Porter 형태소 분석이 내가 사용한 동물 설명 데이터 세트에서 상위 20개 단어의 다른 두 단어와 가장 다르다는 것을 알 수 있습니다. 노래 우리는 종이라는 단어가 어떻게 다르게 취급되었는지 봅니다. 동물 포식자 이런 종류의 단어 모음은 살아 있는 삶을 살고 있습니다. 그래서 실무자들은 일반적으로 텍스트 데이터의 형태소 분석에 관심이 있습니다. 우리는 언어의 인간 사용자로서 이와 같은 접근 방식을 사용할 수 있다는 점을 이해합니다. 이 접근 방식은 일반적으로 형태소 분석이라고 하는 단계별 규칙과 매우 유사합니다. 일반적으로 큰 단어 사전을 기반으로 하는 보조 정리를 사용할 수 있으며 어떤 단어가 함께 속하는지에 대한 언어적 이해를 통합하므로 한국어에서 이러한 종류의 작업에 대한 기존 접근 방식의 대부분은 이러한 사전을 기반으로 하고 다음을 사용하여 훈련된 제한된 보조 정리입니다. 언어의 큰 데이터 세트 그래서 이것은 당신이 이것에 대해 들었을 때 하는 것이 도움이 될 것 같습니다 오 예 텍스트 데이터의 경우 일반적으로 토큰 수가 많은 기능에 압도되기 때문에 소리가 좋고 똑똑하게 들립니다. 이는 일반적으로 텍스트 데이터를 처리할 때의 상황이므로 여기에 이러한 동물 설명 데이터가 있고 다음과 같이 매트릭스 표현을 만들었습니다. 우리는 일반적으로 일부 기계 학습 알고리즘에서 사용하고 기능이 몇 개 있는지 확인합니다. 16,000개 거의 17,000개에 달하는 기능이 모델에 들어갈 기능의 수입니다. 희소성을 살펴봅니다. 98% 희소가 높은 매우 희소 데이터이므로 이것이 단어를 어간하면 지도 머신 러닝 모델을 구축하기 위해 머신 러닝 알고리즘에 들어갈 데이터가 여기에서 파생어에 대한 접근 방식을 사용하면 단어 기능의 수를 수천 배로 줄입니다. 희소성은 불행히도 많이 변경되지 않았지만 우리는 우리의 형태소 분석 알고리즘이 함께 속하는 단어를 함께 버킷화하여 기능의 수를 많이 줄였습니다. 단어 기능의 수를 극적으로 늘리면 기계 학습 모델의 성능이 향상되지만 이는 형태소 분석을 통해 중요한 정보를 잃지 않았다고 가정하고 형태소 분석 또는 보조 정리가 종종 매우 도움이 될 수 있음이 밝혀졌습니다. 일부 상황에서는 이러한 알고리즘에 사용되는 일반적인 알고리즘이 다소 공격적이며 감도 또는 재현율 또는 참 양성률을 선호하도록 구축되었으며 감독되는 기계에서 특이성 또는 정밀도 또는 참 음성률을 희생합니다. 학습 컨텍스트 이것이 하는 일은 모델의 긍정적 예측 값에 영향을 미친다는 것입니다. 정밀도 또는 참 부정을 긍정적으로 잘못 레이블을 지정하지 않는 기능 우리가 모델링하는 것이지만 텍스트가 끝나면 특정 식단을 말하는 것과 관련된 동물 설명을 말하는 긍정적인 예 결과 모델이 부정적인 예에 레이블을 지정할 수 있는 능력을 잃는다는 이유로 우리가 찾고 있는 식단에 대한 설명이 아닌 설명이 있으며 이는 텍스트 데이터를 사용하여 모델을 훈련할 때 이러한 균형을 찾는 일종의 진정한 도전이 될 수 있습니다. 우리는 이러한 형태소 분석 알고리즘에서 이러한 형태소 분석에 대해 변경할 수 있는 다이얼이 없으므로 이 기능 엔지니어링 레시피에서 여기에서 보여주는 것과 같은 텍스트에 대한 아주 기본적인 사전 처리조차도 계산 비용이 많이 들고 실무자가 선택하는 불용어를 제거할지 텍스트를 줄기로 만들지 여부는 모든 종류의 기계 학습 모델이 수행하는 방식에 극적인 영향을 미칠 수 있습니다. 더 단순한 모델이든 더 전통적인 기계 학습 모델이든 딥 러닝 모델이든 이것이 의미하는 바는 우리가 실무자가 텍스트에 대한 피쳐 엔지니어링 단계에 대해 가르치고 쓰는 것과 같이 실제로 더 강력한 통계 p에 기여합니다. 텍스트 데이터의 희소성 이전에 언급한 우리 분야의 관행 언어가 작동하는 방식 때문에 텍스트 데이터가 실제로 정의하는 특성 중 하나이기 때문에 몇 가지 단어를 여러 번 사용하고 여러 번 사용합니다. 단어는 단 몇 번만 몇 번만 그리고 실제 자연어 세트를 사용하면 더 많은 문서와 고유성을 추가함에 따라 희소성이 어떻게 변하는 면에서 이러한 플롯과 같은 다음과 같은 관계로 끝납니다. 단어를 말뭉치에 추가하면 희소성이 정말 빠르게 올라가고 이 문서 집합을 처리하는 데 필요한 메모리가 매우 빠르게 증가하므로 희소 행렬과 같은 희소 데이터를 저장하기 위한 특수 데이터 구조를 사용하더라도 여전히 매우 비선형적인 방식으로 이러한 데이터 세트를 처리하는 데 필요한 메모리가 증가합니다. 여전히 매우 빠르게 증가하므로 모델을 훈련하는 데 매우 오랜 시간이 걸리거나 사용 가능한 메모리가 초과될 수도 있습니다. r 기계는 값비싼 대용량 메모리를 사용하기 위해 클라우드로 이동해야 하는 상황입니다. 이것은 실제 도전이 될 수 있으며 이 도전은 모델에 대한 벡터 언어의 동기 부여 뒤에 있는 것이므로 언어학자는 모델용 벡터 언어에 대해 오랫동안 작업해 왔습니다. 사람들이 언어를 사용하는 방식에 따라 텍스트 데이터를 나타내는 차원의 수를 줄일 수 있으므로 이 인용문은 1957년으로 거슬러 올라갑니다. 따라서 여기서 아이디어는 데이터가 매우 희박하지만 단어를 무작위로 사용하지 않는 것처럼 사용한다는 것입니다. 독립적이지 않은 단어는 서로 독립적으로 사용되지 않고 오히려 단어가 함께 사용되는 방식 사이에 존재하는 관계가 있으며 이러한 관계를 사용하여 희소한 고차원 공간을 특별한 조밀한 저차원 공간으로 변환하기 위해 생성할 수 있습니다. 100개의 차원이지만 수천 수백 수만 개의 공간보다 훨씬 낮기 때문에 여기서 우리는 통계 모델링을 사용합니다. 단어 수와 행렬 인수만 더하면 됩니다. 새로운 공간은 어떤 단어가 함께 사용되는지에 대한 정보를 통합하는 벡터를 기반으로 생성되기 때문에 특별한 새로운 저차원 저차원 공간을 생성하기 때문에 신경망을 포함하는 더 멋진 수학이 될 수 있습니다. 회사에서 제공하는 단어이므로 이러한 종류의 단어 벡터 또는 단어 임베딩을 생성하거나 배우려면 큰 데이터 세트의 텍스트가 필요합니다. 그래서 지금 제가 보여주는 이 테이블은 데이터를 사용하여 생성한 임베딩 세트에서 가져온 것입니다. 미국 소비자 금융 보호국에 불만을 제기하거나 불만을 제기하는 집단을 설정하여 사람들이 불만을 제기하고 신용 카드 모기지와 같은 금융 상품과 관련된 문제에 대해 말할 수 있는 미국 정부 기관입니다. 모기지 학생 금융 상품과 같은 관련 대출 내 신용 카드에 문제가 발생한 것과 같습니다. 대출에 문제가 발생했습니다. 회사는 공정하지 않습니다. 당신이 와서 당신이 그것에 대해 불평하기 때문에 나는 모든 불만을 가져 와서 그것을 우리의 고차원 공간과 저차원 공간을 구축하고 그 공간을보고이 공간에서 어떤 단어가 서로 관련되어 있는지 이해할 수 있습니다. 그래서 새로운 공간에서 임베딩에 의해 정의된 월이라는 단어는 연도와 같은 단어에 가장 가깝습니다. 복수 월별 할부 지불 따라서 이러한 임베딩에 의해 정의된 새로운 공간에서 신용 카드 또는 모기지와 같은 금융 상품의 맥락에서 의미가 있는 단어입니다. 오류라는 단어는 실수와 같은 단어에 가장 가깝습니다. 사무적인 오류 문제의 오류 또는 모기지 명세서에 오류가 있어서 이러한 종류의 또는 잘못된 의사소통 오해를 봅니다. 임베딩을 만들기 위해서는 많은 데이터가 필요하기 때문에 임베딩을 직접 만들어야 합니다. 그래서 미리 훈련된 단어 임베딩을 사용할 수 있습니다. e 그들이 액세스할 수 있는 방대한 양의 데이터와 당신은 아마 그렇지 않을 것이므로 해당 데이터 세트 중 하나를 살펴보겠습니다. 이 표에서 동일한 단어 오류에 대한 결과를 보여주지만 장갑 임베딩에 대한 결과를 보여줍니다. 모든 wikipedia와 같은 매우 큰 데이터 세트를 기반으로 생성된 사전 훈련된 임베딩 인터넷의 거대한 범위와 같은 모든 Google 뉴스 데이터 세트가 이러한 임베딩을 생성하기 위해 공급되었으므로 여기에서 가장 가까운 단어는 다음과 같습니다. 이전과 유사하지만 더 이상 사무적 불일치와 같은 해당 도메인 고유의 특성이 없으며 이제는 잘못된 의사 소통과 같은 문제가 있지만 이제 사람들이 금융 상품 불만 사항에 대해 이야기하지 않은 계산 및 확률이 있으므로 실제로 우리가 우리 자신을 만들고 이 컨텍스트에 특정한 관계를 배울 수 있기 전에 이것이 어떻게 작동하는지 강조하고 여기에서 배운 보다 일반적인 집합으로 이동합니다. d 다른 곳에서 임베딩이 대규모 텍스트 데이터 코퍼스에서 훈련되거나 학습되고 해당 코퍼스의 특성이 임베딩의 일부가 되므로 일반적으로 머신 러닝은 훈련 데이터에 있는 내용에 매우 민감합니다. 텍스트 데이터와 아마도 단어 임베딩을 다룰 때보다 더 분명한 것은 이것이 사실인 이러한 고전적인 예 중 하나와 같습니다. 이것은 말뭉치에 있는 인간의 편견이나 편견이 임베딩에 각인되는 방식에서 나타납니다. 편견이 있는 가장 일반적으로 사용 가능한 임베딩 중 일부를 볼 때 우리는 미국에서 아프리카계 미국인에게 더 흔한 아프리카계 미국인 이름이 유럽계 미국인 이름보다 더 불쾌한 감정과 관련이 있다는 것을 알 수 있습니다. 이러한 임베딩 공간에서 여성의 이름은 가족과 더 관련이 있고 남성의 이름은 경력 및 w와 관련된 용어와 더 관련이 있습니다. 징조는 예술과 더 관련이 있고 남성과 관련된 용어는 과학과 더 관련이 있으므로 실제로 편견이 단어 임베딩에 너무 깊이 뿌리박혀 있어 단어 임베딩 자체가 시간이 지남에 따라 사회적 태도의 변화를 정량화하는 데 사용될 수 있으므로 단어 임베딩이 아마도 과장되거나 극단적인 예이지만 텍스트 데이터와 관련하여 내리는 모든 기능 엔지니어링 결정은 우리가 보는 모델 성능과 얼마나 적절하거나 공정한지 모두에서 결과에 상당한 영향을 미친다는 것이 밝혀졌습니다. 우리 모델은 모든 것을 제공하므로 텍스트 데이터를 사전 처리하여 필요한 이러한 기능을 생성할 때 많은 옵션과 상당한 책임이 있으므로 제 조언은 항상 매우 깊이 이해할 수 있는 더 간단한 모델로 시작하는 것입니다. 다른 접근 방식을 시도할 때 모델 성능 향상에 속지 않도록 모델을 훈련하고 조정할 때 좋은 통계 방법을 채택해야 합니다. d 또한 모델 설명 도구와 프레임워크를 사용하여 덜 간단한 모델을 이해할 수 있도록 하기 위해 제 동료와 제가 이 모든 주제에 대해 썼고 Tidymodels와 함께 사용하는 방법을 썼습니다. 계속 그렇게 할 것입니다. 정말 감사하고 한국의 R 사용자 그룹 주최자에게 다시 한 번 감사드립니다. Rstudio의 Tidymodels 팀 동료와 동료에게 감사합니다. -저자 EMIL HVITFELDT."],"header":"translatedText","minWidth":70,"align":"center","headerStyle":{"background":"#f7f7f8"}}],"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"highlight":true,"bordered":true,"dataKey":"4ff57ac89b86ddb1a3f5e4580cac146f","key":"4ff57ac89b86ddb1a3f5e4580cac146f"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
</div>
<div id="srt-translation" class="section level2" number="5.2">
<h2 number="5.2"><span class="header-section-number">5.2</span> <code>.srt</code> 자막 번역</h2>
<p>통번역이 아니라 <code>.srt</code> 파일에 나온 자막 시작시간과 종료시간을 고려하여 번역작업을 수행해보자.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true"></a>keynote_gl_tbl &lt;-<span class="st"> </span>keynote <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">translation =</span> googleLanguageR<span class="op">::</span><span class="kw">gl_translate</span>(subtitle, <span class="dt">target =</span> <span class="st">&quot;ko&quot;</span>, <span class="dt">source =</span> <span class="st">&quot;en&quot;</span>, <span class="dt">model =</span> <span class="st">&quot;nmt&quot;</span>))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true"></a>keynote_gl_tbl <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">write_rds</span>(<span class="st">&quot;data/julia_silge/keynote_gl_tbl.rds&quot;</span>)</span></code></pre></div>
<p><code>.srt</code> 파일로 번역한 것을 영어 원문과 비교해 보면 다음과 같다.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true"></a>keynote_gl_tbl &lt;-<span class="st">  </span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">read_rds</span>(<span class="st">&quot;data/julia_silge/keynote_gl_tbl.rds&quot;</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true"></a>translation_obj &lt;-<span class="st"> </span>keynote_gl_tbl <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">pull</span>(translation)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true"></a>translation_obj <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true"></a><span class="st">  </span><span class="kw">select</span>(translatedText) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true"></a><span class="st">  </span><span class="kw">bind_cols</span>(keynote_gl_tbl <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>translation)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true"></a><span class="st">  </span><span class="kw">select</span>(n, start, end, subtitle, translatedText) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">start =</span> scales<span class="op">::</span><span class="kw">comma</span>(start),</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true"></a>         <span class="dt">end =</span> scales<span class="op">::</span><span class="kw">comma</span>(end)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true"></a><span class="st">  </span>reactable<span class="op">::</span><span class="kw">reactable</span>(</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true"></a>    <span class="dt">defaultColDef =</span> <span class="kw">colDef</span>(</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true"></a>      <span class="dt">header =</span> <span class="cf">function</span>(value) <span class="kw">gsub</span>(<span class="st">&quot;.&quot;</span>, <span class="st">&quot; &quot;</span>, value, <span class="dt">fixed =</span> <span class="ot">TRUE</span>),</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true"></a>      <span class="dt">cell =</span> <span class="cf">function</span>(value) <span class="kw">format</span>(value, <span class="dt">nsmall =</span> <span class="dv">1</span>),</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true"></a>      <span class="dt">align =</span> <span class="st">&quot;center&quot;</span>,</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true"></a>      <span class="dt">minWidth =</span> <span class="dv">70</span>,</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true"></a>      <span class="dt">headerStyle =</span> <span class="kw">list</span>(<span class="dt">background =</span> <span class="st">&quot;#f7f7f8&quot;</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true"></a>  ),</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true"></a>  <span class="dt">columns =</span> <span class="kw">list</span>(</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true"></a>    <span class="dt">n =</span> <span class="kw">colDef</span>(<span class="dt">minWidth =</span> <span class="dv">10</span>),</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true"></a>    <span class="dt">start =</span> <span class="kw">colDef</span>(<span class="dt">minWidth =</span> <span class="dv">25</span>),</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true"></a>    <span class="dt">end =</span> <span class="kw">colDef</span>(<span class="dt">minWidth =</span> <span class="dv">25</span>),</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true"></a>    <span class="dt">subtitle =</span> <span class="kw">colDef</span>(<span class="dt">minWidth =</span> <span class="dv">100</span>),</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true"></a>    <span class="dt">translatedText =</span> <span class="kw">colDef</span>(<span class="dt">minWidth =</span> <span class="dv">100</span>)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true"></a>  ),</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true"></a>  <span class="dt">bordered =</span> <span class="ot">TRUE</span>,</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true"></a>  <span class="dt">highlight =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<div id="htmlwidget-164d43bc0d782215a53e" class="reactable html-widget" style="width:auto;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-164d43bc0d782215a53e">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"n":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885],"start":["1.1","6.1","11.2","15.5","21.1","27.2","31.7","34.5","39.7","42.9","48.4","52.8","57.9","63.4","66.6","71.6","75.8","77.6","81.0","84.6","86.7","91.0","94.4","96.7","101.3","103.9","105.8","110.0","113.6","116.9","122.6","125.0","129.4","135.2","140.0","144.3","146.7","150.9","153.5","161.3","165.4","167.4","170.6","173.7","175.4","177.3","179.8","181.8","185.0","191.8","195.1","199.0","204.5","209.7","212.6","218.0","223.0","229.8","232.3","233.8","235.8","242.6","244.7","247.4","250.1","252.3","256.7","259.4","263.1","265.2","267.9","270.6","273.2","275.7","279.5","281.1","284.7","287.3","291.0","294.6","298.9","302.3","307.4","310.6","316.2","319.2","321.4","324.5","328.9","335.4","338.9","341.4","343.8","348.2","350.9","354.7","357.4","364.6","368.8","371.3","373.4","376.3","379.7","383.2","385.7","390.6","393.0","397.3","399.8","405.3","408.0","412.0","414.1","416.2","419.3","423.1","426.5","429.3","431.9","437.2","440.8","443.4","445.9","449.5","451.9","454.8","458.6","460.8","463.5","465.7","469.4","472.7","475.3","477.2","483.4","487.6","491.6","493.7","496.1","498.3","503.0","505.8","508.3","512.0","514.6","517.9","521.5","524.5","526.6","528.5","531.0","535.1","538.1","540.8","543.7","545.3","548.5","551.4","554.7","558.2","560.8","563.6","565.9","568.8","570.8","573.4","577.4","580.2","582.8","586.0","588.6","591.3","598.9","603.0","605.9","607.9","610.2","612.8","615.3","618.5","620.6","623.6","628.6","630.7","633.5","635.8","638.7","641.8","643.8","645.2","649.0","654.5","658.0","662.9","665.3","667.6","671.1","675.6","677.5","681.4","683.4","687.0","688.8","691.9","693.8","698.3","700.5","704.5","709.0","712.3","715.4","717.1","719.4","722.1","725.6","727.7","729.5","732.5","735.1","737.8","739.0","741.4","744.5","749.5","754.0","755.8","758.2","761.0","765.5","769.8","773.1","775.0","776.8","779.2","781.5","784.1","786.2","788.0","789.9","792.4","793.8","795.5","797.8","800.5","803.8","806.9","808.4","810.3","812.7","815.2","818.0","823.3","825.2","828.9","833.1","836.3","838.6","840.1","844.1","847.4","849.3","851.0","855.4","860.6","864.7","867.8","869.9","873.4","879.4","881.5","883.8","887.5","891.0","893.1","895.4","897.8","900.7","901.8","905.8","907.8","910.3","912.2","915.3","917.3","919.8","921.8","924.0","927.4","931.3","935.2","937.9","941.7","944.9","947.4","951.5","956.0","958.5","960.8","963.9","967.0","971.1","974.5","977.7","980.6","983.4","988.2","990.3","993.1","997.7","1,001.4","1,003.8","1,007.2","1,010.5","1,012.9","1,015.1","1,018.2","1,020.4","1,023.5","1,027.8","1,029.7","1,031.4","1,035.8","1,038.3","1,043.5","1,046.7","1,049.0","1,052.1","1,054.6","1,057.8","1,059.7","1,062.9","1,067.1","1,069.8","1,071.8","1,075.4","1,078.8","1,084.5","1,088.1","1,091.3","1,093.6","1,096.2","1,098.8","1,101.7","1,103.7","1,105.8","1,108.7","1,110.5","1,114.0","1,116.8","1,120.2","1,123.0","1,126.0","1,129.0","1,131.8","1,134.6","1,138.7","1,140.6","1,143.6","1,145.8","1,148.6","1,151.0","1,153.2","1,155.8","1,158.5","1,160.8","1,165.9","1,173.3","1,175.2","1,178.2","1,182.0","1,184.3","1,186.8","1,191.8","1,193.8","1,196.6","1,199.0","1,203.3","1,205.4","1,208.5","1,210.9","1,213.1","1,216.0","1,218.5","1,220.9","1,223.4","1,229.0","1,231.4","1,233.8","1,236.4","1,239.2","1,241.2","1,242.6","1,245.6","1,249.4","1,252.9","1,255.8","1,258.4","1,261.4","1,264.7","1,268.1","1,271.9","1,273.4","1,276.2","1,280.0","1,284.2","1,287.1","1,292.9","1,295.3","1,298.2","1,302.2","1,306.5","1,309.2","1,314.2","1,317.4","1,324.6","1,326.5","1,329.4","1,331.4","1,334.5","1,338.6","1,341.8","1,344.3","1,346.9","1,350.8","1,353.8","1,355.7","1,358.6","1,361.6","1,364.2","1,367.1","1,370.4","1,373.6","1,376.2","1,380.3","1,383.4","1,386.7","1,389.5","1,391.8","1,393.9","1,396.1","1,398.9","1,400.4","1,405.0","1,408.0","1,411.9","1,415.0","1,416.6","1,419.6","1,421.9","1,427.2","1,432.1","1,434.4","1,437.7","1,440.4","1,444.4","1,447.3","1,449.0","1,452.0","1,455.8","1,460.0","1,462.4","1,466.4","1,468.7","1,471.2","1,473.0","1,477.4","1,480.2","1,481.8","1,483.7","1,486.4","1,488.8","1,493.0","1,495.8","1,498.3","1,500.7","1,504.2","1,506.9","1,512.8","1,517.0","1,520.5","1,523.6","1,527.0","1,530.6","1,534.0","1,537.4","1,540.6","1,543.0","1,545.6","1,547.5","1,550.0","1,555.4","1,559.0","1,561.4","1,563.4","1,565.6","1,567.7","1,571.4","1,576.4","1,579.8","1,582.5","1,586.1","1,588.4","1,589.9","1,593.8","1,596.8","1,602.7","1,605.7","1,607.4","1,611.8","1,614.8","1,617.8","1,621.1","1,625.3","1,627.7","1,630.7","1,633.0","1,637.1","1,639.2","1,641.8","1,644.7","1,648.0","1,650.7","1,652.6","1,655.4","1,657.6","1,662.9","1,666.8","1,670.2","1,672.4","1,675.0","1,679.3","1,682.1","1,684.5","1,687.0","1,690.3","1,692.8","1,696.4","1,699.4","1,702.1","1,703.9","1,706.8","1,709.9","1,713.2","1,715.4","1,719.3","1,721.4","1,725.7","1,727.4","1,729.9","1,732.2","1,738.8","1,742.1","1,745.3","1,747.8","1,750.6","1,755.4","1,758.6","1,760.0","1,764.5","1,767.9","1,771.3","1,773.2","1,777.3","1,780.3","1,782.5","1,786.2","1,789.0","1,791.8","1,794.5","1,798.6","1,801.4","1,802.6","1,805.0","1,807.3","1,809.9","1,812.6","1,814.9","1,816.8","1,819.8","1,824.2","1,828.2","1,831.4","1,836.2","1,839.0","1,843.7","1,847.0","1,851.6","1,856.8","1,861.5","1,864.2","1,867.0","1,869.6","1,872.1","1,875.5","1,879.4","1,883.5","1,888.6","1,891.1","1,895.9","1,898.6","1,902.5","1,905.7","1,909.0","1,910.6","1,913.4","1,916.9","1,922.3","1,924.1","1,925.8","1,932.1","1,936.1","1,938.2","1,940.2","1,945.2","1,949.7","1,951.6","1,953.5","1,956.6","1,959.9","1,963.4","1,965.4","1,967.8","1,971.6","1,973.8","1,976.5","1,980.2","1,984.6","1,986.5","1,989.5","1,992.7","1,995.2","1,996.9","1,999.5","2,002.7","2,006.6","2,009.0","2,012.6","2,017.1","2,021.4","2,025.5","2,028.3","2,032.4","2,036.9","2,039.3","2,041.8","2,044.6","2,047.2","2,051.4","2,056.4","2,059.2","2,062.8","2,065.8","2,069.0","2,071.4","2,073.8","2,076.3","2,078.4","2,080.7","2,083.2","2,085.5","2,087.2","2,089.8","2,092.2","2,095.0","2,097.2","2,099.4","2,101.3","2,105.1","2,108.4","2,110.8","2,113.2","2,115.3","2,117.4","2,120.1","2,124.4","2,127.6","2,130.3","2,132.3","2,134.0","2,139.0","2,142.6","2,145.5","2,147.9","2,150.1","2,153.0","2,156.3","2,158.6","2,161.4","2,164.1","2,170.4","2,174.3","2,176.6","2,179.6","2,182.6","2,185.1","2,187.5","2,190.6","2,192.4","2,195.7","2,198.3","2,203.0","2,208.5","2,211.9","2,216.3","2,220.1","2,223.3","2,225.9","2,229.9","2,233.1","2,237.0","2,239.8","2,241.8","2,243.8","2,247.7","2,251.2","2,257.2","2,260.2","2,263.5","2,266.5","2,271.7","2,275.3","2,283.0","2,287.1","2,289.3","2,292.8","2,295.4","2,297.4","2,299.6","2,303.4","2,307.6","2,311.0","2,314.6","2,317.4","2,320.2","2,324.0","2,326.2","2,328.2","2,331.8","2,335.6","2,338.2","2,340.5","2,343.7","2,345.8","2,348.5","2,352.0","2,355.0","2,358.6","2,364.1","2,366.5","2,369.4","2,372.0","2,375.1","2,377.0","2,384.6","2,389.2","2,391.7","2,393.8","2,397.9","2,400.7","2,404.7","2,407.6","2,409.8","2,411.1","2,413.1","2,415.4","2,420.0","2,425.4","2,427.8","2,430.2","2,433.9","2,437.7","2,440.2","2,444.1","2,449.7","2,454.6","2,457.8","2,463.4","2,466.7","2,469.7","2,475.0","2,479.0","2,483.4","2,486.2","2,488.3","2,491.6","2,494.8","2,497.2","2,499.5","2,502.2","2,504.9","2,507.5","2,511.4","2,514.1","2,520.2","2,522.8","2,524.6","2,527.4","2,531.0","2,535.3","2,541.4","2,544.8","2,549.5","2,551.7","2,554.8","2,558.6","2,560.8","2,563.3","2,566.7","2,569.0","2,571.6","2,574.3","2,578.2","2,580.8","2,582.6","2,586.8","2,590.4","2,593.1","2,596.7","2,599.6","2,602.1","2,604.2","2,607.4","2,610.2","2,612.2","2,615.8","2,618.2","2,620.3","2,622.4","2,626.1","2,631.0","2,634.4","2,639.0","2,641.4","2,644.7","2,652.2","2,654.6","2,657.5","2,659.7","2,663.3","2,667.7","2,670.2","2,676.1","2,678.6","2,680.6","2,682.6","2,688.3","2,691.4","2,694.2","2,698.5","2,701.4","2,705.0","2,708.2","2,711.1","2,713.6","2,716.7","2,718.5","2,722.6","2,726.0","2,729.5","2,731.6","2,734.6","2,737.0","2,739.7","2,743.5","2,746.6","2,750.6","2,753.5","2,756.7","2,761.8","2,764.7","2,767.1","2,770.4","2,772.8","2,776.1","2,778.0","2,780.2","2,782.6","2,784.7","2,788.7","2,790.3","2,793.0","2,796.6","2,799.0","2,802.6","2,805.1"],"end":["8.64","13.52","18.08","24.00","29.36","34.48","36.88","42.88","45.04","50.72","55.44","60.48","66.64","69.36","75.76","77.60","81.04","84.56","86.72","90.96","94.40","96.72","99.52","103.92","105.76","110.00","111.20","116.88","119.84","125.04","128.48","131.92","137.60","144.32","146.72","149.20","153.52","155.36","164.32","167.44","170.56","173.68","175.44","177.28","179.76","181.84","185.04","187.68","195.12","197.92","202.16","207.28","212.64","214.88","220.48","224.96","232.32","233.84","235.76","238.40","244.72","247.44","250.08","252.32","253.52","259.36","260.80","265.20","267.92","270.64","273.20","275.68","278.32","281.12","284.72","287.28","289.36","293.68","296.16","301.12","304.80","310.64","313.12","319.20","321.36","322.80","327.12","332.24","337.36","341.36","343.84","347.12","350.88","352.80","357.44","360.48","366.80","371.28","373.44","376.32","379.72","383.20","385.68","389.68","393.04","396.00","399.84","403.44","408.00","411.12","414.08","416.24","419.28","423.12","426.48","429.28","431.92","434.48","440.80","443.44","445.92","447.68","451.92","454.80","458.56","460.80","463.52","465.68","467.12","472.72","475.28","477.20","481.20","485.36","490.32","493.68","496.08","498.32","501.52","505.76","508.32","510.08","514.56","517.92","519.20","524.48","526.64","528.48","531.04","533.04","538.08","540.80","543.68","545.28","548.48","551.36","554.72","558.16","560.80","563.60","565.92","568.80","570.80","573.36","575.84","580.24","582.80","586.00","588.64","591.28","594.48","601.60","605.92","607.92","610.24","612.80","615.28","617.44","620.56","623.60","626.32","630.72","633.52","635.76","638.72","641.76","643.84","645.20","647.68","651.36","658.00","661.68","665.28","667.60","671.12","674.00","677.52","680.00","683.36","685.44","688.80","691.92","693.84","696.80","700.48","704.48","707.04","712.32","715.36","717.12","719.44","722.08","725.60","727.68","729.52","732.48","735.12","737.84","739.04","741.44","744.48","748.16","750.88","755.84","758.24","760.96","764.48","768.16","773.12","775.04","776.80","779.20","781.52","784.08","786.24","788.00","789.92","792.40","793.84","795.52","797.84","800.48","803.84","806.88","808.40","810.32","812.72","815.20","818.00","820.64","825.20","827.68","832.32","836.32","838.56","840.08","842.08","847.44","849.28","850.96","853.36","857.84","862.88","867.76","869.92","872.56","877.52","881.52","883.84","887.52","889.76","893.12","895.44","897.76","899.84","901.84","904.88","907.76","910.32","912.24","915.28","917.28","919.76","921.76","924.00","927.44","931.28","935.20","937.92","941.68","944.88","947.44","949.36","954.96","958.48","960.80","963.92","967.04","971.12","974.48","977.68","980.56","983.36","985.84","990.32","992.24","996.16","1,001.44","1,003.84","1,007.20","1,010.48","1,012.88","1,015.12","1,016.48","1,020.40","1,023.52","1,026.40","1,029.68","1,031.44","1,034.08","1,038.32","1,039.92","1,043.36","1,048.96","1,052.08","1,054.64","1,057.84","1,059.68","1,062.88","1,064.24","1,069.76","1,071.84","1,074.08","1,078.80","1,081.84","1,088.08","1,090.16","1,093.60","1,096.24","1,098.80","1,101.68","1,102.64","1,105.76","1,108.72","1,110.48","1,114.00","1,116.80","1,120.16","1,123.04","1,126.00","1,128.96","1,131.04","1,134.64","1,137.60","1,140.64","1,143.60","1,145.76","1,148.64","1,151.04","1,153.20","1,155.84","1,158.48","1,160.80","1,164.48","1,167.12","1,175.20","1,178.24","1,182.00","1,184.32","1,186.80","1,190.64","1,190.64","1,196.56","1,198.96","1,200.88","1,205.44","1,208.48","1,210.88","1,213.12","1,216.00","1,217.44","1,220.88","1,223.44","1,225.20","1,231.36","1,233.76","1,236.40","1,239.20","1,241.20","1,242.64","1,245.60","1,249.36","1,252.00","1,254.96","1,258.40","1,261.44","1,264.72","1,266.24","1,269.28","1,273.36","1,276.24","1,280.00","1,282.96","1,287.12","1,288.80","1,295.28","1,297.28","1,301.36","1,306.48","1,309.20","1,313.12","1,317.44","1,320.24","1,326.48","1,329.36","1,331.44","1,334.48","1,337.28","1,339.60","1,344.32","1,346.88","1,349.92","1,353.76","1,355.68","1,358.64","1,361.60","1,364.16","1,367.12","1,370.40","1,372.08","1,376.24","1,380.32","1,383.36","1,386.72","1,389.52","1,391.84","1,393.92","1,396.08","1,398.88","1,400.40","1,402.72","1,408.00","1,411.20","1,415.04","1,416.64","1,419.60","1,421.92","1,424.32","1,432.08","1,434.40","1,437.68","1,440.40","1,444.40","1,447.28","1,448.96","1,452.00","1,455.84","1,458.88","1,462.40","1,466.40","1,468.72","1,471.20","1,472.96","1,477.44","1,480.16","1,481.84","1,483.68","1,486.40","1,488.80","1,491.76","1,495.76","1,498.32","1,500.72","1,503.12","1,506.88","1,510.00","1,516.08","1,520.48","1,523.60","1,527.04","1,528.56","1,532.16","1,537.44","1,540.56","1,543.04","1,545.60","1,547.52","1,550.00","1,552.24","1,556.96","1,561.36","1,563.36","1,565.60","1,567.68","1,571.44","1,573.52","1,579.76","1,582.48","1,584.72","1,588.40","1,589.92","1,593.84","1,596.80","1,599.76","1,605.68","1,607.44","1,611.84","1,614.80","1,617.84","1,621.12","1,623.92","1,627.68","1,630.72","1,632.96","1,634.72","1,639.20","1,641.76","1,644.72","1,648.00","1,650.72","1,652.64","1,655.36","1,657.60","1,659.92","1,662.40","1,668.96","1,672.40","1,675.04","1,678.24","1,682.08","1,684.48","1,686.96","1,690.32","1,692.80","1,696.40","1,699.44","1,702.08","1,703.92","1,706.80","1,709.92","1,713.20","1,715.36","1,717.60","1,721.36","1,722.48","1,727.44","1,729.92","1,732.24","1,734.96","1,741.04","1,744.00","1,747.76","1,750.64","1,754.40","1,758.56","1,760.00","1,762.00","1,766.48","1,771.28","1,773.20","1,777.28","1,780.32","1,782.48","1,784.88","1,789.04","1,791.76","1,794.48","1,797.52","1,801.36","1,802.64","1,804.96","1,807.28","1,809.92","1,812.56","1,814.88","1,816.80","1,819.84","1,821.52","1,825.60","1,831.44","1,833.20","1,838.96","1,840.56","1,846.96","1,851.60","1,853.36","1,858.32","1,864.16","1,867.04","1,869.60","1,872.08","1,874.72","1,877.84","1,883.52","1,886.56","1,891.12","1,894.96","1,898.56","1,901.28","1,905.68","1,907.60","1,910.56","1,913.36","1,914.88","1,920.48","1,924.08","1,925.84","1,929.44","1,936.08","1,938.24","1,940.16","1,944.00","1,948.88","1,951.60","1,953.52","1,956.64","1,959.92","1,963.44","1,965.36","1,967.76","1,971.60","1,973.76","1,976.48","1,980.16","1,982.24","1,986.48","1,989.52","1,991.60","1,995.20","1,996.88","1,999.52","2,001.68","2,004.88","2,009.04","2,012.56","2,015.04","2,019.28","2,024.40","2,028.32","2,031.04","2,033.84","2,039.28","2,041.76","2,044.64","2,047.20","2,049.68","2,053.84","2,059.20","2,060.56","2,065.76","2,068.96","2,071.36","2,073.76","2,076.32","2,078.40","2,080.72","2,083.20","2,085.52","2,087.20","2,089.76","2,092.24","2,094.96","2,097.20","2,099.36","2,101.28","2,105.12","2,108.40","2,110.80","2,113.20","2,115.28","2,117.44","2,120.08","2,124.40","2,127.60","2,130.32","2,132.32","2,134.00","2,136.00","2,142.56","2,145.52","2,147.92","2,150.08","2,153.04","2,156.32","2,158.56","2,161.36","2,164.08","2,166.72","2,174.32","2,176.64","2,179.60","2,182.64","2,185.12","2,187.52","2,190.64","2,192.40","2,195.68","2,198.32","2,203.04","2,205.76","2,211.92","2,213.52","2,217.52","2,223.28","2,225.92","2,227.60","2,233.12","2,235.52","2,239.84","2,241.76","2,243.84","2,246.08","2,251.20","2,254.32","2,259.36","2,263.52","2,266.48","2,269.68","2,275.28","2,278.16","2,284.96","2,289.28","2,292.80","2,295.44","2,297.36","2,299.60","2,302.24","2,305.92","2,311.04","2,313.60","2,317.36","2,320.16","2,322.48","2,326.16","2,328.16","2,329.84","2,335.60","2,338.24","2,340.48","2,343.68","2,345.84","2,348.48","2,352.00","2,355.04","2,358.56","2,361.84","2,366.48","2,369.36","2,372.00","2,375.12","2,377.04","2,380.56","2,388.00","2,391.68","2,393.84","2,396.64","2,400.72","2,403.68","2,407.60","2,409.76","2,411.12","2,413.12","2,415.36","2,417.52","2,424.00","2,427.84","2,430.16","2,432.40","2,437.68","2,440.24","2,444.08","2,448.00","2,450.88","2,456.48","2,461.28","2,465.12","2,469.68","2,471.76","2,477.68","2,480.88","2,486.24","2,488.32","2,490.88","2,493.60","2,497.20","2,499.52","2,502.24","2,504.88","2,507.52","2,509.84","2,514.08","2,515.60","2,522.80","2,524.56","2,527.36","2,529.92","2,532.80","2,536.64","2,544.80","2,547.28","2,551.68","2,554.80","2,558.64","2,560.84","2,563.28","2,566.72","2,568.96","2,571.60","2,574.32","2,578.24","2,580.80","2,582.64","2,586.80","2,590.40","2,593.12","2,596.72","2,599.60","2,602.08","2,604.24","2,607.36","2,610.16","2,612.24","2,615.84","2,618.16","2,620.32","2,622.40","2,623.76","2,628.16","2,634.40","2,636.48","2,641.44","2,644.72","2,647.04","2,654.64","2,657.52","2,659.68","2,663.28","2,665.52","2,670.24","2,671.60","2,678.64","2,680.56","2,682.64","2,684.08","2,691.44","2,694.16","2,696.80","2,701.44","2,705.04","2,708.16","2,711.12","2,713.60","2,716.72","2,718.48","2,721.44","2,726.00","2,729.52","2,731.60","2,734.56","2,737.04","2,739.68","2,743.52","2,746.56","2,750.48","2,750.48","2,756.72","2,759.04","2,764.72","2,767.12","2,770.40","2,772.84","2,776.08","2,778.00","2,780.24","2,782.64","2,784.72","2,786.88","2,790.32","2,793.04","2,794.88","2,798.96","2,802.56","2,805.12","2,807.44","2,811.24"],"subtitle":["Hi my name is julia silge.  I'm a data scientist and software engineer at RStudio.","and I'd like to thank the the organizers of the R user group in Korea","so much for having me.  Today speak to you","I am so happy to be speaking specifically today about creating","features for machine learning from text data for a couple of reasons","Having a better understanding of what we do to take text data","and then to make it appropriate","as an input for machine learning algorithms has many benefits","both if you are directly getting ready","to train a model or if you're at the beginning of some text analysis project","or if you are trying to understand the behavior of a model that","you're interacting with some way which is something that we do in our work as data scientists","or in our in our daily lives more and more","so when we build models for text either supervised or unsupervised","we start with something","like this this is some example text data","that I'll use a couple of times during","this talk that describes some animals","I'm using some text data so","you know to me as an english","speaker looks familiar like","I am as someone who uses a human","language so I look at this and I can","read it I could speak it aloud and I understand","I can interpret it what it means","so this kind of data this sort of","natural language data is being generated","all the time in all kinds of languages","in all kinds of contexts so","whether you work in healthcare in tech in finance","basically any kind of organization this","sort of text data is being generated","by customers by clients by internal stakeholders","inside of a business by people taking surveys","via social media via business processes","and in all this natural language","there's information latent in","that text data that can be used to make better decisions","However, computers are not great at looking at this and doing","math on language as it's represented like this","and instead language has to be","dramatically transformed to some kind of","machine readable numeric representation","that looks more like this what I'm","showing here on the screen to be ready","for almost any kind of model","so I spent a fair amount of time working","on software for people to be able to do","exploratory data analysis, visualization, summarization","tasks like that with text data in a tidy","format where we have one observation per row","and I love using tidy data principles for text analysis","especially during those exploratory phases of an analysis","when it comes time to build a model","often what the underlying mathematical implementation really needs","is typically something like this which is a","way to this particular representation is called the document term matrix","so the exact representation may differ from","what I've shown here","what I have here is we're weighting","things by counts so each row in this matrix is a document","each column is a is a word.","A token and the numbers represent","counts how many times does each document","use each word you could weight it in a different way","using say TF-IDF instead of counts","or you might keep sequence information","if you're interested in building a deep learning model but basically","for all kinds of text modeling","from simpler models like Naive Bayes","models which work well for text","to word embeddings to really the most","state-of-the-art kind of work that's","happening today like transformers for text data","we have to heavily","feature engineer and process language to","get it to some kind of representation","that's suitable for machine learning algorithms","so I work on an open source framework in R","for modeling and machine learning that's called Tidymodels and the examples that","I'll be showing today use Tidymodels code","some of the specific goals of the Tidymodels project are to provide","a consistent flexible framework for real","world modeling practitioners people who are you know doing","that are dealing with real world data","those who are just starting out to","those who are very experienced in modeling and","the goal is to harmonize the heterogeneous","interfaces that exist within R and to encourage good statistical practice","I'm glad to get to show you some of what I work on","and build and how we apply it to text","modeling but a lot of what I will talk","about today isn't very specific to Tidymodels","or even to R. I know this is an R user","group but what we're going to talk about and focus on","is a little more conceptual and basic","how do we transform text into predictors for machine learning","I am excited though to talk about Tidymodels and Tidymodels if you have not","used it before is a meta package","in a similar way that the Tidyverse is","a meta package so if you've ever typed","library Tidyverse and then you've used","ggplot2 for visualization","dplyr for data manipulation","Tidymodels works in a similar way","there are different packages inside of it","that are used for different purposes","so the pre-processing or the feature","engineering is part of a broader model process","you know it that process starts really","with with exploratory data analysis","that helps us decide what kind of model","we will build and then it comes to","completion I think I would argue with","model evaluation when you","measure how well your model performed","Tidymodels as a piece of software is","made up of our packages each of which","has a specific focus like our sample","is for re-sampling data to be able to","create bootstrap resamples","cross-validation resamples all different","kinds of resamples you might want to use to","train and evaluate models","the tune package is for hyper parameter","tuning as you might guess from the name","one of these packages is for feature","engineering for a data preprocessing","feature engineering and it is the one","that is called recipes","so in Tidymodels we capture this idea","of data pre-processing and feature","engineering in the concept of a","pre-processing recipe that has steps so you choose","ingredients or variables","that you're going to use then you define the steps","that go into your recipe","then you prepare them using training","data and then you can apply that to any","data set like testing data or new data at prediction time","so the variables or ingredients that we","use in modeling come in all kinds of","shapes and sizes including text data","so some of the techniques and approaches","that we use for pre-processing text data","are the same um as for any other kind of data that","you might use like non-text data","numeric data categorical data some for","some of it is the same","but some of what you need to know to be","able to do a good job","in this process for text is different","and is specific to the nature of what","language data is like","and so I've written a book with my","co-author Emile Hvitfeldt on supervised","machine learning for text analysis and R","and fully the first third of the book","focuses on how we transform","the natural language that we have in","text data into features for modeling","the middle section is about how we use","these features in","simpler or more traditional machine","learning models like regularized","regression or support vector machines and","then the last third of the book","talks about how we use deep learning","models with text data so deep learning","models still require these kinds of","transformations from natural language","into features as input for these kinds of models but","deep learning models are often able to","inherently learn structure of features","from text in ways that those","more traditional or simpler machine","learning models are not","so this book is now complete and","available as of this month as of november","folks are getting their first paper","copies and also this book is available","in its entirety at smalltar.com","so if you're new to dealing with text","data understanding these","fundamental pre-processing approaches","for text will set you up for being able","to train effective models","if you're really experienced with text","data if you've dealt with it a lot","already you've probably noticed like we have","that the existing you know resources or literature whether","that's books or tutorials or blog posts","is quite sparse when it comes to","detailed thoughtful explorations of how","these pre-processing steps work","and how choices made in these feature","engineering steps impact our model output","so let's walk through","several of some of these like basic","feature engineering approaches and how","they work and what they do let's start out with","tokenization","so typically one of the first steps in","transfer information from natural","language to machine learning feature for","really any kind of text analysis","including exploratory data analysis","or building a model. Anything is tokenization","in tokenization we take an input some","string some character vector and some","kind of token type","some meaningful unit of text","we're interested in a word","and we split the input pieces into","tokens that correspond to the type","we're interested in","so most commonly the meaningful unit or","type of token that we want to split text","into units of is a word","so this might seem","straightforward or obvious but it turns","out it's difficult to clearly define","what a word is for many or even most languages","so many languages do not use white space","between words at all","which presents a challenge","for tokenization even languages that","do use white space like english and korean","often have particular examples that are ambiguous","like contractions in english like didn't","which should be","you know maybe more accurately","considered two words the way","particles are used in Korean","and how pronouns and negation words are","written in romance languages like","italian and french where they're stuck","together and really maybe they should be","considered two words","once you have figured out what you're","going to do and you make some choices","and you tokenize your text then it's on","its way to being able to be used","in exploratory data analysis or","unsupervised algorithms or as features","for predictive modeling which is what","we're talking about here and what these","results show here so these results are","from a regression model trained on","descriptions of media","from artwork in the Tate collection in the UK so","what we're predicting in what we are","predicting is when what year","was a piece of art created based on the","the medium that the artwork was created","with and the medium is described with a","little bit of text","so we see here that artwork created using graphite","watercolor and engraving was more likely","to be created earlier","that though that is more likely to come","from older art and artwork that is created using","photography screen point or sorry screen print","screen printing and and dung","and glitter are more likely to be","created later. there this is more likely","to come from contemporary art modern art","so the the way that we tokenize this text","you know we started with natural human","generated texts of people writing out","the descriptions of the the media","that these art pieces of art were created with","and the way we tokenized that natural","human generated text that we started","with has a big impact on what we learned","from it if we tokenized in a different way","we would have gotten different","results in terms of performance like how","accurately we were able to predict","predict the year and also in terms of","how we interpret the model like what is","it that we're able to learn from it","so this is one kind of tokenization to","the single word but we also","we all another way to tokenize instead","of breaking up into single words or","unigrams we can tokenize to n-grams","so an n-gram is a continuous sequence of","N items from a given sequence of texts","so this shows that same piece of little","bit of text i'm describing this animal","divided up into bi-grams or n-grams of","two tokens so notice how the words in","the bi-grams overlap so the word collard","appears in both of the first bigrams the collared","collared peccary peccary also","referred to so n-gram","tokenization slides along the text to","create overlapping sets of tokens","this shows tri-grams for the same thing","so using uni-grams one word is","faster and more efficient but we don't","capture information about word order","I'm using a higher value","two or three or even more keeps","more complex information about word","order and concepts","that are described in multi-word phrases","but the vector space of tokens","increases dramatically","that corresponds to a reduction in token","counts we don't count each token as very","many times and that means depending on","your particular data set","you might not be able to get good results","so combining different degrees of","n-grams can allow you to extract","different levels of detail from text so uni-grams","can tell you which individual words have","been used a lot of times","some of those words might be overlooked","in bi-gram or tri-gram crowns if they","don't co-appear with other words as often","this plot compares model performance for","a Lasso regression model predicting the","year of supreme court opinions the","United States supreme court opinions","with three different degrees of n-grams","what we're doing here is we are taking","the text of the writings of the United","States supreme court and we're predicting","when did it when was that text","written so can we predict how old a","piece of text is from the contents of the text","so holding the number of tokens","constant at a thousand using uni-grams alone","performs best for this corpus of","opinions from the United States supreme court","this is not always the case depending on","the kind of model you use the data set","itself we might see the best performance","combining uni-grams and bi-grams or maybe","some other option","in this case if we wanted to incorporate","some of that more complex information","that we have in the bi-grams and the","tri-grams we probably would need to","increase the number of","tokens in the model quite a bit","so keep in mind when you look at results","like these that identifying n-grams is","computationally expensive","this is especially compared to the","amount of like a model the improvement","in model performance that we often see like if we","if we see some you know modest","improvement by adding in bigrams it's","important to keep in mind how much","improvement we see relative to","how long it takes to","identify bi-grams and then train that","model so for example for this data set","of supreme court opinions where we held","the number of tokens constant so the","model training had the same number of tokens in it","using bi-grams plus uni-grams takes twice as long to train","to do the feature engineering and the","training than only uni-grams and adding","in tri-grams as well takes almost five","times as long as training on uni-grams","alone so this is a computationally","expensive thing to do","going in the other direction","we can tokenize to units smaller than","words so like these are what are called","character shingles so we take words","the collared peccary","and we can instead of looking at words","we can go down and look at sub word","information there's multiple different","ways to break words up into sub words","that are appropriate for machine learning","and often these kinds of approaches","or algorithms have the benefit of being","able to encode unknown or new words","at prediction time so when it when it's","time to make a prediction on new data","it's not it's not uncommon for there to","be new vocabulary words at that time and","if we didn't see them in the training","data you know what are we going to do","about those new words when we train","using subword information often we can","handle those new words if we saw the subword","in our training data set so","using this kind of subword information","is a way to incorporate morphological","sequences into our models of you know","various kinds of this is something that applies to","various languages not just english","so these results are for a","classification model with a data set of","very short texts it's just the names of","post offices in the United States so super short","and the goal of the model was to predict","the post office located in hawaii","in the middle of the pacific ocean or","it located in the rest of the united states","so I created features for the model that are","subwords of these post office names","and we end up learning that the names","that start with h and p or contain that ale","sub word are more likely to be in hawaii","and the sub words a and d and ri and ing are are","more likely to come from the post office","that are outside of hawaii","so this is an example of how we","tokenized differently and we're able to","learn something new we're able to learn something","different so in Tidymodels we collect all these","kinds of decisions about tokenization","and code that looks like this","so we start with a recipe that specifies what","variables or ingredients that we'll use","and then we define these preprocessing","steps so even at this first","and arguably you know simple and basic","step the choices that we make affect our","modeling results in a big way","the next pre-processing um step that","I want to talk about is stop words","so once we have split text","into tokens we often find that not","all words carry the same amount of","information if maybe any information at","all actually for a machine learning task","so common words that carry little or","perhaps no meaningful information are","called stopwords","so this is one of the stopword lists","that's available for","Korean so it's common advice and practice","to say hey just remove just remove","remove these stopwords for a lot of","natural language processing tasks","what I'm showing here","is the entirety of one of the shorter","english stopword lists that's used","really broadly so you know it's words like I","me my pronouns conjunctions and of the","and these are very common words","that are not considered super important","the decision though to just remove","stopwords is often more involved and","perhaps more fraught than what you'll","than what you'll find reflected in a lot","of resources that are out there","so almost all the time real world NLP","practitioners use pre-made stopword lists","so this plot visualizes","set intersections for three common","stopword lists in english","in what is called an upset plot","so the three lists are called the","snowball list smart and the iso list","so you can see the the lengths of the","list are represented by the length of","the bars and then we see the","intersections which words are in common","on these lists by the by the vertical","bars so the lengths of the list are quite different","and also notice they don't all contain","the same sets of words","the important thing to remember about","stopword lexicons is that","they are not created in some","neutral perfect setting but instead they are","they are context specific","they they can be biased both of these","things are true because they are lists","created from large data sets of language","so they reflect the characteristics of the data used in","their creation so this is","the ten words that are in the english","language smart lexicon but not in the","English snowball lexicon","so notice that they're all contractions","but that's not because the snowball","exchange doesn't include contractions","it has a lot of them also notice that it has","that she's is on this list and so that means that","that list has he's but it does not have","the list she's","so this is an example of that","The bias I mentioned that occurs because","these lists are created from large data","sets of text lexicon creators look at the most","frequent words in some big corpus of","language they make a cut off","and then some decisions about what to include or","exclude you know","based on the list that they","have and you end up here so because","in many large data sets of language","you have more representation of","men you end up with a","situation like this where a stopword","list will have he's but not she's","so many decisions when it comes to","modeling or analysis with language","we as practitioners have to decide","what is appropriate for our particular domain","it turns out this is even true when it","comes to picking a stopword list","so in Tidymodels we can implement a","pre-processing step like removing stopwords","by adding an additional step to our","recipe so first we specified what","variables we would use then we tokenized","the text and now we are removing","stopwords here using just the default","step since we are not passing in any","other arguments we could though","use a non-default step or even a custom","list if that was most appropriate to our domain","this plot compares the model performance","for predicting the year of","that same data set of","supreme court opinions with three","different stopword lexicons of different lengths","so the snowball lexicon contains the","smallest number of words and in this","case it results in the best performance","so removing fewer stopwords results in","the best performance here so this","specific result is not generalizable to","all data sets and contexts but the fact","that removing different sets of","stopwords can have noticeably different","effects on your model that is quite","transferable so the only way to know","what is the best thing to do is to try","several options and see so machine","learning in general.","this is an empirical field right like we","don't know we don't often have reasons a priori to","know what will be the best thing to do","and so typically we have to","try a different option to see what will","be the best thing all right. then the the third","pre-processing step that I want to talk about","for text is stemming","so when we deal with text often","documents contain different versions of","one base word often called a stem so what","if say for an english example","if we aren't interested in the difference","between animals plural and animal","singular and we want to treat them both together","so that idea is at the heart of stemming","so there's no one","right way or correct way to stem text so","this plot shows three approaches for","stemming in English","starting from hey let's just remove a final s","to more complex rules about plural","handling plural endings that middle","one it is called the s stemmer","it's a set of it's like a little set of rules","and that last one is the best known","one probably the best-known","implementation of stemming in English","called the Porter algorithm","so you can see here that Porter stemming","is the most different from the other two","in the top 20 words here from the data","set of animal descriptions that I've","been using we see how the word species","was treated differently animal predator","this sort of collection of words","live living life lives that was treated","differently so practitioners are typically","interested in stemming text data because","it buckets tokens together that we believe","belong together in in a way that","we understand that as human users","of language so we can use approaches like this","which are pretty like step-by-step","rules based this is typically called","stemming or and it's fairly","algorithmic in nature like","first do this then do this then do this","or you can use lemmatization","which is usually based on large dictionaries","of words and it incorporates like a","linguistic understanding of what words belong together","so most of the existing approaches for","this kind of task in Korean are","are limited lemmatizers based on these","dictionaries and that are trained","using large data sets of language","so this seems like it's going to be a helpful thing to do","when you hear about this you're like","oh yeah sounds good,  sounds smart","especially because with text data we are typically","overwhelmed with features with numbers of tokens","this is typically the situation","when we're dealing with text data","so here we have these animal description data","and I made a matrix representation of it","like we would typically use in some","machine learning algorithm","and look how many features there are","16,000 almost 17,000 features","that's the number of features that","would be going into the model","look at the sparsity","98 percent sparse that's high very","sparse data so this is the sparsity of","the data that will go into the machine","learning algorithm to build our","supervised machine learning model","if we stem the words","if I use here an approach for stemming","we reduce the number of word features by","many thousands the sparsity unfortunately did not","change as much but we reduced the number","of features by a lot by bucketing those","words together that our stemming algorithm","belong together so you know","common sense says","reducing the number of words features","so dramatically is going to perform","improve the performance of our machine learning model","but that is that does assume that we","have not lost any important information","by by stemming and it turns out that stemming or","lemmatization can often be very helpful in some","contexts but the typical algorithms used for these","are somewhat aggressive","and they have been built to favor sensitivity","or recall or the true positive rate and","this is at the expense of the","specificity or the precision or the true","negative rate so in a supervised machine","learning context what this does is this","affects a model's positive predictive","value the precision or its ability to","to not incorrectly label true negatives","as positive I hope I got that right","so you know to make this more concrete","stemming can increase a model's ability","to find the positive examples","of say the animal descriptions that are","associated with say a certain diet if","that's what we're modeling however if","text is over stemmed","the resulting model loses its ability to","label the negative examples","say the descriptions that are not about","that diet that's what we're looking for","and this can be a real challenge when","training models with text data kind of","finding that that balance there because","often we don't have a","dial that we can change on these","stemming on these stemming algorithms","so even just very basic pre-processing","for text like what I'm showing here in","this feature engineering recipe can be","computationally expensive","and the choices that a practitioner","makes like whether or not to remove","stopwords or to stem text can have dramatic","impact on how machine learning models","of all kinds perform whether those are","simpler models","more traditional machine learning models","or deep learning models what this means is that","the price the prioritization that we","as practitioners give to like learning","teaching and writing about feature","engineering steps for text really","contributes to better more robust","statistical practice in our field","I mentioned before the sparsity of text","data and I want to come back to that","because it is one of text data's really","defining characteristics because of just how language works","we use a few words a lot of times","and then a lot of words only just a","couple of times only a few a few times","and with a real set of natural language","you end up with relationships that look","like this that look like these plots in","terms of how the sparsity changes as you","add more documents","and more unique words to a corpus","so the sparsity goes up real fast as you","add more unique words and the memory","that is required to handle","this set of documents goes up very fast","so even if you use specialized data","structures meant to store sparse data like sparse","matrices you still end up growing the","memory required to handle these data","sets in a very non-linear way it still grows up very","fast so this means it can take a very","long time to train your model or even that","you outgrow the memory","available on your machine you have to go","to the cloud to an expensive","big memory situation this can be a real challenge","and this challenge","it is what has behind the motivating of vector","languages for models so","linguists have worked for a long time","on vector languages for models that can","reduce the number of dimensions representing text data","based on how people use language","so this quote here goes all the way back to 1957.","so the idea here is that we use","like the data is very sparse","but we don't use words","randomly it's not independent the words","are not used independently of each other","but rather there's relationships that","exist between how words are used together","and we can use those relationships to create","to transform our sparse high dimensional","space into a special dense","low dimensional space lower","we still has like 100","dimensions but much lower than the many thousands","hundreds tens hundreds of thousands","of space so the idea here we use","statistical modeling maybe just","word counts plus matrix factorization","maybe fancier math that involves neural","networks to take this really high","dimensional space and we create a new","lower dimensional lower dimensional","space that is special because the new","space is created based on vectors that","incorporate information","about which words are used together so","you shall know a word by the company it keeps","so you need a big data set of text to","create or learn these kinds of word","vectors or word embeddings","so this table that I'm showing right now","it's from a set of embeddings that","I created using a data set or a corpus of complaints","complaints to the United States consumer","financial protection bureau","so this is a government body in the","United States where people can complain and say","what is wrong with something to do with","a financial product like a credit card","a mortgage a student loan","something to do with like a financial","product they're like something went","wrong with my credit card something went","wrong with my mortgage that company is","not being fair so you come and you complain to it","so I took all those complaints and built","it's our high dimensional space and","build a low dimensional space","and we can look in that space and understand","what words are related to each other","in this space so in the new space","defined by the embeddings the word month","is closest to words like year months plural","monthly installments payment so these are words","that are that makes sense in the context of","financial products like credit cards or mortgages","in the new space defined by these embeddings","the word error is closest to the words","like mistake clerical like a clerical mistake","problem glitch or there was a glitch on my","mortgage statement so we see these kinds of","or miscommunication misunderstanding you","know like these are these are words that","are used in similar ways so","you don't have to create embeddings yourself","because it requires quite a lot of data","to make them so you can use word","embeddings that are pre-trained","i.e created by someone else","based on some huge corpus of data that","they have access to and you probably don't","so let's look at one of those data sets","let's look at this table shows the results for the same word error","but for the glove embeddings so the","glove embeddings are a set of","pre-trained embeddings that are created","based on a very large data set that's like","all of wikipedia all of the google news data set","just like huge swaths of the internet have been","fed in to create these embeddings","so some of the closest words here are similar","to those that are before but we no","longer have some of that domain specific","flavor like clerical discrepancy","and now we have like","miscommunication you know but and now we","have calculation and probability","which people were not talking about with","their financial product complaints","so this really highlights","how these how these work here before","we we created our own and we were able","to learn relationships that were","specific to this context and here we go","to a more general set that that","was learned somewhere else","so embeddings are trained or learned","from a large corpus of text data and the","characteristics of that corpus become","part of the embeddings","so machine learning in general you know","is exquisitely sensitive to whatever it","is that's in your training data and this","is never more obvious than when","dealing with text data","and perhaps with word embeddings is","just like one of these classic examples","where this is true it turns out that","this shows up in how any human","prejudice or bias in the corpus","becomes imprinted into the embeddings","so in fact when we look at some of these","most commonly available embeddings that","are out there bias is we we see that","african-american first names that are","more common for african americans in the","United States they're associated with","more unpleasant feelings than European","American first names in these embedding spaces","women's first names are more associated","with family and men's first names are more associated with career","and terms associated with women are more","associated with the arts and terms","associated with men are more associated","with science so it turns out actually bias is so","ingrained in word embeddings that the","word embeddings themselves can be used","to quantify change","in social attitudes over time","so word embeddings are","maybe an exaggerated or extreme example","but it turns out that all the feature","engineering decisions that we make when","it comes to text data have a significant","effect on our results","both in terms of the model performance that we see","and also in terms of how appropriate or","fair our models are","so given all that when it comes to","pre-processing your text data","creating these features that you need","you have a lot of options and quite a","bit of responsibility so my advice is","always start with simpler models that","you can understand quite deeply","be sure to adopt good statistical","practices as you train and tune your","models so you aren't fooled about model","performance improvements","when you try different approaches","and also to use model explainability","tools and frameworks so you can","understand any less straightforward","models that you try","so my co-workers and I have written","about all of these topics and how to","use them with Tidymodels if that's what","you like to use and we will continue to do so","with that i will say","thank you so very much and I want to be","sure to again","thank the organizers of the R user group","in Korea I want to thank my teammates on","the Tidymodels team at Rstudio as","well as my co-author EMIL HVITFELDT."],"translatedText":["안녕하세요 제 이름은 줄리아 실지입니다. 저는 RStudio의 데이터 과학자이자 소프트웨어 엔지니어입니다.","그리고 한국의 R 사용자 그룹의 주최자에게 감사의 말을 전하고 싶습니다.","나를 가진 것에 대해 너무 많이. 오늘은 너에게 말을 걸","오늘 특별히 제작에 대해 이야기하게 되어 매우 기쁩니다.","몇 가지 이유로 텍스트 데이터에서 기계 학습을 위한 기능","우리가 텍스트 데이터를 취하기 위해 무엇을 하는지 더 잘 이해하기","그런 다음 적절하게 만들기 위해","기계 학습 알고리즘에 대한 입력으로 많은 이점이 있습니다.","둘 다 직접 준비하는 경우","모델을 훈련시키거나 일부 텍스트 분석 프로젝트를 시작하는 경우","또는 모델의 동작을 이해하려는 경우","데이터 과학자로서 우리가 하는 일과 같은 방식으로 상호 작용하고 있습니다.","또는 우리의 일상 생활에서 점점 더","따라서 감독 또는 감독되지 않은 텍스트에 대한 모델을 구축할 때","우리는 무언가로 시작합니다","이것은 몇 가지 예제 텍스트 데이터입니다.","내가 몇 번 사용하는 동안","일부 동물을 설명하는 이 이야기","일부 텍스트 데이터를 사용하고 있으므로","당신은 나에게 영어로 알고","스피커는 다음과 같이 친숙해 보입니다.","나는 인간을 사용하는 사람으로서","언어를 사용하여 이것을 보고 할 수 있습니다.","그것을 읽으십시오 나는 큰 소리로 말할 수 있고 나는 이해할 수 있습니다","나는 그것이 무엇을 의미하는지 해석할 수 있다","그래서 이런 종류의 데이터는 이런 종류의","자연어 데이터가 생성되고 있습니다.","모든 종류의 언어로 항상","모든 종류의 상황에서","금융 기술 분야 의료 분야에서 일하는지 여부","기본적으로 어떤 종류의 조직이든","일종의 텍스트 데이터가 생성되고 있습니다.","고객에 의한 고객에 의한 내부 이해관계자에 의한","설문 조사에 참여하는 사람들에 의해 기업 내부","비즈니스 프로세스를 통해 소셜 미디어를 통해","그리고 이 모든 자연어에서","정보가 숨어있다","더 나은 결정을 내리는 데 사용할 수 있는 텍스트 데이터","그러나 컴퓨터는 이것을 보고 수행하는 데 능숙하지 않습니다.","다음과 같이 표현되는 언어에 대한 수학","대신 언어는","극적으로 어떤 종류의","기계 판독 가능한 숫자 표현","그것이 내가 무엇인지 더 많이 보인다","준비를 위해 여기 화면에 표시","거의 모든 종류의 모델에 대해","그래서 나는 상당한 시간을 일했다.","사람들이 할 수 있는 소프트웨어","탐색적 데이터 분석, 시각화, 요약","깔끔한 텍스트 데이터와 같은 작업","행당 하나의 관찰이 있는 형식","그리고 나는 텍스트 분석을 위해 깔끔한 데이터 원칙을 사용하는 것을 좋아합니다.","특히 분석의 탐색 단계에서","모델을 만들 때가 되면","종종 기본 수학적 구현이 실제로 필요로 하는 것","일반적으로 다음과 같은 것입니다.","이 특정 표현에 대한 방법을 문서 용어 행렬이라고 합니다.","따라서 정확한 표현은 다음과 다를 수 있습니다.","내가 여기서 보여준 것","내가 여기에 있는 것은 우리가 무게를 다는 것입니다","이 행렬의 각 행이 문서이기 때문에","각 열은 단어입니다.","토큰과 숫자는 다음을 나타냅니다.","각 문서가 몇 번인지 계산합니다.","각 단어를 사용하여 다른 방식으로 가중치를 부여할 수 있습니다.","카운트 대신 TF-IDF 사용","또는 시퀀스 정보를 유지할 수 있습니다.","딥 러닝 모델 구축에 관심이 있지만 기본적으로","모든 종류의 텍스트 모델링","Naive Bayes와 같은 단순한 모델에서","텍스트에 잘 맞는 모델","워드 임베딩까지 정말","최첨단 작업이다.","오늘날 텍스트 데이터의 변환기처럼 발생","우리는 무겁게해야합니다","기능 엔지니어 및 프로세스 언어","어떤 종류의 표현에 그것을 얻을","머신 러닝 알고리즘에 적합","그래서 저는 R의 오픈 소스 프레임워크에서 작업합니다.","Tidymodels라고 하는 모델링 및 머신 러닝과","나는 오늘 Tidymodels 코드를 사용하는 것을 보여줄 것이다.","Tidymodels 프로젝트의 특정 목표 중 일부는","현실을 위한 일관되고 유연한 프레임워크","당신이 알고있는 세계 모델링 실무자 사람들","실제 데이터를 다루는","이제 막 시작하는 사람들","모델링 경험이 풍부한 사람과","목표는 이기종을 조화시키는 것입니다.","R 내에 존재하고 우수한 통계 관행을 장려하기 위한 인터페이스","제가 작업하는 것을 보여드릴 수 있어서 기쁩니다.","빌드 및 텍스트에 적용하는 방법","모델링하지만 많은 이야기를 할 것입니다.","오늘은 Tidymodels에만 국한되지 않습니다.","또는 R에게도. 나는 이것이 R 사용자라는 것을 알고 있습니다.","그룹이지만 우리가 이야기하고 집중할 것은","조금 더 개념적이고 기본적인","기계 학습을 위해 텍스트를 예측자로 변환하는 방법","아직 Tidymodels 및 Tidymodels에 대해 이야기하고 싶지 않다면 흥분됩니다.","이전에 사용한 메타 패키지입니다.","Tidyverse와 유사한 방식으로","메타 패키지이므로 입력한 적이 있다면","라이브러리 Tidyverse를 사용한 다음","시각화를 위한 ggplot2","데이터 조작을 위한 dplyr","Tidymodels도 비슷한 방식으로 작동합니다.","그 안에 다른 패키지가 있습니다","다양한 용도로 사용되는","따라서 전처리 또는 기능","엔지니어링은 보다 광범위한 모델 프로세스의 일부입니다.","프로세스가 실제로 시작된다는 것을 알고 있습니다.","탐색적 데이터 분석으로","어떤 종류의 모델을 결정하는 데 도움이","우리는 구축 할 것입니다 그리고 그것은 온다","내가 생각하는 완성","당신이 할 때 모델 평가","모델이 얼마나 잘 수행되었는지 측정","소프트웨어의 한 조각으로서의 Tidymodels는","각각의 패키지로 구성","우리의 샘플과 같은 특정 초점이 있습니다","데이터를 다시 샘플링하기 위한 것입니다.","부트스트랩 재샘플 생성","교차 검증은 모두 다른 리샘플링","사용할 수 있는 리샘플의 종류","모델 훈련 및 평가","tune 패키지는 하이퍼 매개변수용입니다.","이름에서 짐작할 수 있듯이 튜닝","이 패키지 중 하나는 기능용입니다.","데이터 전처리를 위한 엔지니어링","피쳐 엔지니어링은","레시피라고 하는","그래서 Tidymodels에서 우리는 이 아이디어를","데이터 전처리 및 기능","개념의 공학","선택할 수 있는 단계가 있는 전처리 레시피","성분 또는 변수","사용할 단계를 정의합니다.","그것은 당신의 조리법에 들어갑니다","그런 다음 훈련을 사용하여 준비합니다.","모든 데이터에 적용할 수 있습니다.","테스트 데이터 또는 예측 시점의 새 데이터와 같은 데이터 세트","그래서 우리가","모든 종류의 모델링에 사용","텍스트 데이터를 포함한 모양 및 크기","따라서 일부 기술과 접근 방식은","텍스트 데이터를 사전 처리하는 데 사용하는","다른 종류의 데이터와 동일합니다.","텍스트가 아닌 데이터처럼 사용할 수 있습니다.","숫자 데이터 범주형 데이터 일부","일부는 동일","그러나 당신이 알아야 할 몇 가지","좋은 일을 할 수 있는","이 과정에서 텍스트가 다릅니다.","그리고 무엇의 성격에 따라","언어 데이터는 다음과 같습니다.","그래서 저는 제 책을 썼습니다.","공동 저자 Emile Hvitfeldt 감독","텍스트 분석 및 R을 위한 머신 러닝","그리고 책의 첫 번째 1/3을 완전히","우리가 어떻게 변화하는지에 중점을 둡니다.","우리가 가지고 있는 자연어","모델링을 위한 피쳐로 텍스트 데이터","중간 섹션은 우리가 사용하는 방법에 관한 것입니다","이러한 기능은","더 간단하거나 더 전통적인 기계","정규화와 같은 학습 모델","회귀 또는 지원 벡터 기계 및","그런 다음 책의 마지막 3분의 1","우리가 딥 러닝을 사용하는 방법에 대해 이야기합니다.","텍스트 데이터가 있는 모델로 딥 러닝","모델은 여전히 이러한 종류의","자연어에서 변형","이러한 종류의 모델에 대한 입력으로 기능에","딥 러닝 모델은 종종","본질적으로 기능의 구조를 학습","텍스트에서","더 전통적이거나 더 단순한 기계","학습 모델은","그래서 이 책은 이제 완성되었고","이달부터 11월까지 가능","사람들은 첫 번째 종이를 얻습니다.","사본 및 또한 이 책은 유효합니다","smalltar.com에서 전체","따라서 텍스트를 처음 다루는 경우","이를 이해하는 데이터","기본적인 전처리 접근법","텍스트가 가능하도록 설정합니다.","효과적인 모델을 훈련하기 위해","당신이 정말로 텍스트에 경험이 있다면","데이터를 많이 다루었다면","이미 당신은 아마 우리처럼","기존 리소스 또는 문헌을 알고 있는지 여부","그것은 책이나 튜토리얼 또는 블로그 게시물입니다.","에 관해서는 매우 희박하다.","방법에 대한 자세한 사려 깊은 탐구","이러한 전처리 단계가 작동합니다.","이 기능에서 선택한 방법","엔지니어링 단계는 모델 출력에 영향을 미칩니다.","그래서 통과하자","이 중 몇 가지는 기본과 같은","피쳐 엔지니어링 접근 방식 및 방법","그들은 일하고 그들이하는 일부터 시작하겠습니다.","토큰화","따라서 일반적으로","자연에서 정보를 전송","언어 대 기계 학습 기능","정말 모든 종류의 텍스트 분석","탐색적 데이터 분석 포함","또는 모델을 구축합니다. 뭐든지 토큰화","토큰화에서 우리는 입력을 받습니다.","문자열 일부 문자 벡터 및 일부","토큰 유형의 종류","의미 있는 텍스트 단위","우리는 단어에 관심이 있습니다","그리고 우리는 입력 조각을","유형에 해당하는 토큰","우리는 관심이있다","가장 일반적으로 의미 있는 단위 또는","텍스트를 분할하려는 토큰 유형","의 단위로 단어는","그래서 이것은 보일 수 있습니다","직설적이거나 명백하지만","명확하게 정의하기 어렵다","많은 또는 대부분의 언어에 대한 단어는 무엇입니까","많은 언어가 공백을 사용하지 않습니다","단어 사이에 전혀","도전을 제시하는","토큰화를 위해","영어와 한국어와 같은 공백을 사용하십시오","종종 모호한 특정 예가 있습니다.","영어로 된 수축처럼 like don't","어느 것이어야","당신은 아마 더 정확하게 알고","두 단어를 방법으로 간주","입자는 한국어로 사용됩니다.","대명사와 부정 단어가 어떻게","같은 로맨스 언어로 작성","그들이 갇혀있는 이탈리아어와 프랑스어","함께 그리고 정말로 아마도 그들은 그래야만 할 것입니다","두 단어로 간주","당신이 무엇인지 파악한 후에","할 것이고 당신은 몇 가지 선택을 할 것입니다","텍스트를 토큰화하면 켜집니다.","사용할 수 있는 방법","탐색적 데이터 분석에서 또는","감독되지 않은 알고리즘 또는 기능","예측 모델링을 위해","우리는 여기서 그리고 이것들이 무엇인지에 대해 이야기하고 있습니다","결과가 여기에 표시되므로 이러한 결과는","에 대해 학습된 회귀 모델에서","미디어에 대한 설명","영국 테이트 컬렉션의 아트웍에서","우리가 무엇을 예측하고 있는지","예측은 몇 년","를 기반으로 만들어진 예술 작품이었습니다.","작품이 만들어진 매체","와 매체는 다음과 같이 설명됩니다.","약간의 텍스트","그래파이트를 사용하여 만든 작품을 여기서 볼 수 있습니다.","수채화와 조각이 더 가능성이 높았습니다.","더 일찍 생성될","그것이 올 가능성이 더 높더라도","사용하여 만든 오래된 예술 및 예술 작품에서","사진 화면 포인트 또는 미안 화면 인쇄","스크린 인쇄와 똥","반짝이는 가능성이 더 높습니다.","나중에 생성됨. 거기에 이것은 더 가능성이 있습니다","현대 미술에서 오는 현대 미술","이 텍스트를 토큰화하는 방법","당신은 우리가 자연적인 인간으로 시작했다는 것을 알고 있습니다","작성하는 사람들의 생성된 텍스트","언론에 대한 설명","이 예술 작품은","그리고 우리가 그 자연스러운 것을 토큰화한 방법","우리가 시작한 인간 생성 텍스트","우리가 배운 것에 큰 영향을 미칩니다","다른 방식으로 토큰화하면","우리는 달라졌을 것이다","결과는 다음과 같은 성능 측면에서","정확하게 예측할 수 있었습니다","연도를 예측하고","모델을 어떻게 해석하는지","우리가 그것으로부터 배울 수 있다는 것","이것은 토큰화의 한 종류입니다.","한 단어지만 우리도","대신 토큰화하는 다른 방법","한 단어로 분해하거나","n-gram으로 토큰화할 수 있는 유니그램","따라서 n-gram은 다음의 연속 시퀀스입니다.","주어진 텍스트 시퀀스의 N개 항목","그래서 이것은 같은 작은 조각을 보여줍니다","이 동물을 설명하는 텍스트의 비트","bi-gram 또는 n-gram으로 나눈다.","두 개의 토큰이 있으므로 단어가 어떻게","bi-gram이 겹쳐서 단어 collard","첫 번째 bigrams 모두에 나타납니다.","칼라 페 케리 페 케리도","그래서 n-그램으로 참조","토큰화는 텍스트를 따라 슬라이드하여","겹치는 토큰 세트 생성","이것은 같은 것에 대한 트라이 그램을 보여줍니다.","따라서 유니그램을 사용하는 한 단어는","더 빠르고 효율적이지만","단어 순서에 대한 정보 캡처","더 높은 값을 사용하고 있습니다.","둘, 셋 또는 그 이상 유지","단어에 대한 더 복잡한 정보","순서와 개념","여러 단어로 된 구문으로 설명된","그러나 토큰의 벡터 공간","극적으로 증가","이는 토큰 감소에 해당합니다.","우리는 각 토큰을 그다지 세지 않습니다","여러 번 그리고 그 의미에 따라","귀하의 특정 데이터 세트","좋은 결과를 얻지 못할 수도 있습니다","서로 다른 정도를 결합하여","n-grams를 사용하면 다음을 추출할 수 있습니다.","텍스트와 다른 수준의 세부 정보를 제공하므로 유니그램","어떤 개별 단어가 있는지 말할 수 있습니다","많이 사용되었다","그 단어 중 일부는 간과 될 수 있습니다","바이그램 또는 트라이그램 크라운의 경우","자주 다른 단어와 함께 나타나지 마십시오","이 플롯은 에 대한 모델 성능을 비교합니다.","예측하는 올가미 회귀 모델","대법원 판결의 해","미국 대법원의 의견","세 가지 다른 정도의 n-gram","우리가 여기서 하는 것은 우리가 취하고 있는 것입니다","United의 글의 텍스트","주 대법원과 우리는 예측하고 있습니다","언제 했어 그 문자가 언제였더라","우리가 몇 살인지 예측할 수 있도록 작성되었습니다","텍스트의 일부는 텍스트의 내용에서 가져온 것입니다.","그래서 토큰의 수를 유지","유니그램만 사용하여 1000에서 상수","이 코퍼스에 대해 가장 잘 수행됩니다.","미국 대법원의 의견","에 따라 항상 그런 것은 아닙니다.","데이터 세트를 사용하는 모델의 종류","그 자체로 우리는 최고의 성능을 볼 수 있습니다","유니 그램과 바이 그램을 결합하거나 아마도","다른 옵션","이 경우 통합하려는 경우","좀 더 복잡한 정보","우리는 바이그램과","아마도 우리가 필요로 할 트라이그램","수를 늘리다","모델의 토큰","따라서 결과를 볼 때 염두에 두십시오.","이와 같이 n-gram을 식별하는 것은","계산적으로 비싸다","이것은 특히 비교된다","모델과 같은 양의 개선","우리가 자주 보는 모델 성능에서","우리가 겸손한 것을 알고 있다면","빅그램을 추가하여 개선합니다.","얼마나 많은지 기억하는 것이 중요합니다.","우리가 비교하는 개선 사항","얼마나 걸립니까","바이그램을 식별한 다음 훈련","예를 들어 이 데이터 세트에 대한 모델","우리가 개최한 대법원의 의견","토큰의 수는 일정하므로","모델 훈련에는 동일한 수의 토큰이 있습니다.","바이그램과 유니그램을 함께 사용하면 훈련 시간이 두 배 더 걸립니다.","기능 엔지니어링을 수행하고","유니그램 뿐만 아니라 훈련 및 추가","트라이 그램에서도 거의 5가 걸립니다.","유니그램 교육만큼","혼자이므로 이것은 계산적으로","값비싼 일","다른 방향으로 가고","보다 작은 단위로 토큰화할 수 있습니다.","이와 같은 말을 라고 한다.","캐릭터 대상 포진 그래서 우리는 단어를","칼라 페커리","단어를 보는 대신","우리는 내려가서 하위 단어를 볼 수 있습니다","정보는 여러 가지가 있습니다","단어를 하위 단어로 나누는 방법","머신러닝에 적합한","그리고 종종 이러한 종류의 접근 방식","또는 알고리즘은 다음과 같은 이점이 있습니다.","알려지지 않은 단어나 새로운 단어를 인코딩할 수 있음","예측 시간에 그래서 언제 그것이 될 때","새로운 데이터에 대한 예측 시간","드문 일이 아닙니다.","그 당시의 새로운 어휘가 되고","훈련에서 그들을 보지 못했다면","우리가 무엇을 할 것인지 알고 있는 데이터","우리가 훈련할 때 그 새로운 단어에 대해","하위 단어 정보를 자주 사용하여","하위 단어를 본 경우 새 단어를 처리하십시오.","훈련 데이터 세트에서","이러한 종류의 하위 단어 정보를 사용하여","형태소를 통합하는 방법이다","당신이 알고 있는 우리의 모델에 시퀀스","이것의 다양한 종류에 적용되는 것입니다","영어뿐만 아니라 다양한 언어","따라서 이 결과는","데이터 세트가 있는 분류 모델","매우 짧은 텍스트는 이름일 뿐입니다.","미국의 우체국은 너무 짧습니다.","모델의 목표는","하와이에 위치한 우체국","태평양 한가운데 또는","그것은 미국의 나머지 부분에 위치","그래서 모델에 대한 기능을 만들었습니다.","이 우체국 이름의 하위 단어","그리고 우리는 이름이","h와 p로 시작하거나 에일을 포함하는","하위 단어는 하와이에 있을 가능성이 더 높습니다.","하위 단어 a 및 d 및 ri 및 ing은 다음과 같습니다.","우체국에서 올 확률이 더 높습니다.","하와이 밖에 있는","이것은 우리가","다르게 토큰화하고 우리는","새로운 것을 배우다 우리는 무언가를 배울 수 있다","Tidymodels에서 우리는 이 모든 것을 수집합니다.","토큰화에 대한 결정의 종류","그리고 다음과 같은 코드","그래서 우리는 무엇을 지정하는 조리법으로 시작합니다","우리가 사용할 변수 또는 성분","그런 다음 이러한 전처리를 정의합니다.","처음에는 이렇게 단계를 밟아도","그리고 틀림없이 당신은 간단하고 기본적인 것을 알고 있습니다","우리가 내리는 선택이 우리의","모델링 결과를 크게","다음 전처리 um 단계","내가 말하고 싶은 것은 스톱 워드","그래서 일단 텍스트를 분할하면","토큰으로 우리는 종종 그렇지 않다는 것을 발견합니다","모든 단어는 같은 양의","정보","모두 실제로 기계 학습 작업을 위해","거의 또는","의미 있는 정보가 없을 수도 있습니다","불용어","그래서 이것은 불용어 목록 중 하나입니다","사용할 수 있는","한국어이므로 일반적인 조언과 관행입니다.","그냥 제거하기만 하면 제거","많은 경우 이러한 불용어를 제거하십시오.","자연어 처리 작업","내가 여기서 보여주는 것","더 짧은 것 중 하나의 전체입니다.","사용되는 영어 불용어 목록","정말 광범위하게 나와 같은 단어라는 것을 알 수 있습니다.","me 내 대명사 접속사 및 of","그리고 이것은 매우 일반적인 단어입니다","매우 중요하다고 여겨지지 않는","그냥 제거하기로 결정","불용어는 종종 더 많이 관련되고","아마도 당신이 생각하는 것보다","많이 반영된 것보다","거기에 있는 자원의","그래서 거의 항상 현실 세계 NLP","실무자는 미리 만들어진 불용어 목록을 사용합니다.","그래서 이 플롯은","세 가지 공통에 대한 교차점 설정","영어로 된 불용어 목록","화난 음모라고 불리는 것에서","따라서 세 개의 목록을","눈덩이 목록 스마트 및 ISO 목록","그래서 당신은 길이를 볼 수 있습니다","목록은 길이로 표시됩니다.","막대 그리고 우리는 본다","단어가 공통되는 교차점","이 목록에서 수직으로","막대가 있으므로 목록의 길이가 상당히 다릅니다.","또한 모든 항목이","같은 단어 집합","기억해야 할 중요한 것","불용어 사전은","그들은 일부에서 생성되지 않습니다","중립적인 완벽한 설정이지만 대신","그들은 상황에 따라 다릅니다","그들은 이 두 가지 모두에 편향될 수 있습니다.","그것들은 목록이기 때문에 사실입니다.","대규모 언어 데이터 세트에서 생성","사용된 데이터의 특성을 반영합니다.","그들의 창조는 이것이다.","영어로 된 10개의 단어","언어 스마트 사전에는 없지만","영어 눈덩이 사전","그래서 그것들이 모두 수축이라는 것을 알아차리세요.","하지만 그건 눈덩이 때문이 아니야","교환은 수축을 포함하지 않습니다","많은 사람들이","그녀가 이 목록에 있다는 것은","그 목록에는 그가 있지만 그것은 없다","그녀의 목록","그래서 이것은 그 예입니다","내가 언급한 편향은 다음과 같은 이유로 발생합니다.","이 목록은 대용량 데이터에서 생성됩니다.","텍스트 사전 제작자가 가장 많이 보는 세트","어떤 큰 말뭉치에서 자주 사용되는 단어","그들이 끊는 언어","그런 다음 무엇을 포함할지 또는","당신이 알고 제외","그들이 작성한 목록을 기반으로","가지고 있고 당신은 여기에서 끝납니다. 왜냐하면","많은 대규모 언어 데이터 세트에서","당신은 더 많은 표현을 가지고 있습니다","당신이 끝내는 남자들","이 같은 상황에서 stopword","목록에는 그가 있지만 그녀는 없습니다","많은 결정을 내릴 때","언어로 모델링 또는 분석","실무자로서 우리는 결정해야합니다","특정 도메인에 적합한 것","이것이 사실일 때도 사실로 밝혀졌습니다","불용어 목록을 선택하기 위해 온다","그래서 Tidymodels에서 우리는","불용어 제거와 같은 전처리 단계","추가 단계를 추가하여","조리법 그래서 먼저 우리는 무엇을 지정","우리가 사용할 변수를 토큰화했습니다.","텍스트 및 이제 우리는 제거하고 있습니다","기본값만 사용하는 불용어","우리는 어떤 단계도 통과하지 않기 때문에","우리가 할 수 있는 다른 주장","기본이 아닌 단계 또는 사용자 정의를 사용하십시오.","그것이 우리 도메인에 가장 적절한지 나열하십시오.","이 플롯은 모델 성능을 비교합니다.","년을 예측하기 위해","동일한 데이터 세트","3명의 대법원 의견","다른 길이의 다른 불용어 사전","따라서 눈덩이 사전에는","가장 적은 수의 단어와 이것에서","최상의 성능을 내는 경우","따라서 더 적은 수의 불용어를 제거하면","여기 최고의 성능 그래서 이것은","특정 결과를 일반화할 수 없음","사실을 제외한 모든 데이터 세트와 컨텍스트","다른 세트를 제거하는","불용어는 눈에 띄게 다를 수 있습니다","모델에 미치는 영향은 상당히","양도 가능하므로 알 수 있는 유일한 방법","가장 좋은 것은 시도하는 것입니다","몇 가지 옵션과 기계 참조","일반적으로 학습.","이것은 우리와 같은 경험적 분야입니다.","우리는 종종 사전에 이유가 없다는 것을 모릅니다","가장 좋은 일이 무엇인지 알고","그래서 일반적으로 우리는","다른 옵션을 시도하여","최고가 되십시오. 그 다음 세 번째","내가 말하고 싶은 전처리 단계","텍스트가 형태소 분석 중이기 때문에","그래서 우리가 텍스트를 자주 다룰 때","문서에는 다른 버전의","하나의 기본 단어는 종종 어간이라고 합니다.","영어로 예를 들면","차이에 관심이 없다면","복수와 동물 사이","특이하고 우리는 둘 다 함께 치료하고 싶습니다","그 아이디어는 스테밍의 핵심입니다","그래서 아무도 없다","텍스트를 줄기로 만드는 올바른 방법 또는 올바른 방법","이 플롯은 세 가지 접근 방식을 보여줍니다.","영어로 어간","헤이부터 시작하여 마지막 s를 제거합시다.","복수에 대한 더 복잡한 규칙","중간에 복수 엔딩 처리","하나는 s 스테머라고합니다","그것은 일종의 규칙의 집합과 같습니다.","그리고 마지막은 가장 잘 알려진","아마도 가장 잘 알려진","영어로 형태소 분석 구현","포터 알고리즘이라고 함","여기에서 Porter가","다른 둘과 가장 다른","여기 데이터에서 상위 20개 단어에","내가 가지고 있는 동물 설명 세트","우리는 종이라는 단어를 사용하여","다른 동물 포식자 취급","이런 종류의 단어 모음","삶을 살아라 치료받은 삶을 살아라","따라서 실무자들은 일반적으로","텍스트 데이터의 형태소 분석에 관심이 있기 때문에","우리가 믿는 토큰을 함께 버킷","하는 방식으로 함께 속해있다.","우리는 인간 사용자로서","우리가 이와 같은 접근 방식을 사용할 수 있도록","단계별로 꽤","이를 기반으로 하는 규칙을 일반적으로","형태소 분석 또는 그것은 상당히","본질적으로 알고리즘 같은","먼저 이렇게 하고 다음에는 이렇게 하세요","또는 표제어를 사용할 수 있습니다","일반적으로 큰 사전을 기반으로 합니다.","단어 및 그것은 다음과 같이 통합됩니다.","어떤 단어가 함께 속해 있는지에 대한 언어적 이해","따라서 대부분의 기존 접근 방식은","이런 종류의 한국어 작업은","다음을 기반으로 하는 제한된 표제어입니다.","사전 및 훈련된 사전","대규모 언어 데이터 세트 사용","도움이 될 것 같습니다.","당신이 이것에 대해 들었을 때 당신은","오 예 좋은 소리, 똑똑한 소리","특히 텍스트 데이터의 경우 일반적으로","많은 토큰이 있는 기능에 압도됨","이것은 일반적으로 상황","텍스트 데이터를 다룰 때","여기에 동물 설명 데이터가 있습니다.","그리고 나는 그것의 행렬 표현을 만들었습니다.","우리가 일반적으로 일부에서 사용하는 것처럼","기계 학습 알고리즘","그리고 얼마나 많은 기능이 있는지 보세요","16,000개 거의 17,000개 기능","그것은 기능의 수입니다","모델에 들어갈 것입니다","희소성을 봐","98% 희소, 매우 높음","희소 데이터이므로 이것은 희소성입니다.","기계에 들어갈 데이터","우리를 구축하기 위한 학습 알고리즘","지도 머신 러닝 모델","우리가 단어를 중단하면","여기에서 형태소 분석에 대한 접근 방식을 사용하면","우리는 다음과 같이 단어 기능의 수를 줄입니다.","수천 개의 희소성이 불행히도","많이 변경하지만 우리는 숫자를 줄였습니다","기능을 버킷팅하여","우리의 형태소 분석 알고리즘이","당신이 알 수 있도록 함께 속해","상식이 말한다","단어 수 줄이기 기능","너무 극적으로 수행 할 것입니다","기계 학습 모델의 성능 향상","그러나 그것은 우리가","중요한 정보를 잃지 않았습니다","형태소 분석을 통해 형태소 분석 또는","표제어는 종종 일부에서 매우 도움이 될 수 있습니다.","컨텍스트가 있지만 이러한 작업에 사용되는 일반적인 알고리즘은","다소 공격적이다","그리고 그들은 감도를 선호하도록 만들어졌습니다.","또는 회상 또는 참 양성률 및","이것은 비용으로","특이성 또는 정밀도 또는 진실","감독되는 기계에서 음수 비율","학습 컨텍스트 이것이 하는 일","모델의 긍정적인 예측에 영향을 줍니다.","정밀도 또는 그 능력에 가치를 둡니다.","참음성 라벨을 잘못 붙이지 않기 위해","내가 옳았으면 좋겠어","그래서 당신은 이것을 더 구체적으로 만들기 위해 알고 있습니다.","형태소 분석은 모델의 능력을 향상시킬 수 있습니다.","긍정적인 사례를 찾기 위해","동물에 대한 설명은","특정 식이요법과 관련된 경우","그것이 우리가 모델링하는 것입니다.","텍스트가 생략되었습니다.","결과 모델은 기능을 잃습니다.","부정적인 예에 레이블을 지정","에 관한 것이 아닌 설명을 말하다","우리가 찾던 바로 그 다이어트","이것은 다음과 같은 경우에 진정한 도전이 될 수 있습니다.","텍스트 데이터 종류의 훈련 모델","그 균형이 거기에 있다는 것을 발견하기 때문에","종종 우리는 가지고 있지 않습니다","우리가 이것을 변경할 수 있는 다이얼","이러한 형태소 분석 알고리즘에 대한 형태소 분석","아주 기본적인 전처리만 해도","내가 여기에 표시하는 것과 같은 텍스트의 경우","이 기능 엔지니어링 레시피는","계산적으로 비싸다","그리고 실무자가 선택하는","제거할지 여부를 결정합니다.","불용어 또는 줄기 텍스트는","머신 러닝 모델에 미치는 영향","모든 종류의","더 간단한 모델","보다 전통적인 기계 학습 모델","또는 딥 러닝 모델이 의미하는 바는","가격 우선 순위 우리가","실무자가 학습을 좋아하는 것처럼","기능에 대해 가르치고 쓰기","텍스트를 위한 엔지니어링 단계","더 나은 견고성에 기여합니다.","우리 분야의 통계 실습","나는 텍스트의 희소성 앞에서 언급했다.","데이터로 돌아가고 싶습니다.","텍스트 데이터 중 하나이기 때문에","언어가 작동하는 방식 때문에 특성 정의","우리는 몇 가지 단어를 많이 사용합니다","그리고 나서 많은 단어들은 단지","몇 번 몇 번만","실제 자연어 세트로","당신은 보이는 관계로 끝납니다","이 플롯처럼 보이는 이와 같이","희소성이 어떻게 변하는지에 대한 용어","더 많은 문서 추가","그리고 말뭉치에 대한 더 독특한 단어","그래서 희소성은 당신만큼 정말 빨리 올라갑니다.","더 독특한 단어와 메모리를 추가","처리하는 데 필요한","이 문서 세트는 매우 빠르게 올라갑니다.","따라서 전문 데이터를 사용하더라도","희소 데이터와 같은 희소 데이터를 저장하기 위한 구조","당신은 여전히 성장하는 매트릭스","이러한 데이터를 처리하는 데 필요한 메모리","매우 비선형적인 방식으로 설정되며 여전히 매우 자랍니다.","매우 빠르므로 매우 오래 걸릴 수 있습니다.","모델을 훈련하는 데 오랜 시간이 걸리거나","당신은 기억을 초과","당신이 가야 할 당신의 기계에서 사용 가능","값비싼 클라우드로","큰 메모리 상황은 이것이 진짜 도전이 될 수 있습니다.","그리고 이 도전","벡터의 동기 부여 뒤에 있는 것입니다.","모델을 위한 언어","언어학자들은 오랫동안 일해 왔다.","할 수 있는 모델을 위한 벡터 언어","텍스트 데이터를 나타내는 차원 수 줄이기","사람들이 언어를 사용하는 방식에 따라","그래서 이 인용문은 1957년으로 거슬러 올라갑니다.","그래서 여기에서 우리가 사용하는 아이디어는","데이터가 매우 희박한 것처럼","하지만 우리는 단어를 사용하지 않습니다","무작위로 독립적이지 않은 단어","서로 독립적으로 사용되지 않음","그러나 오히려 관계가 있습니다.","단어가 함께 사용되는 방식 사이에 존재","이러한 관계를 사용하여","희소한 고차원을 변환하기 위해","특별한 밀도로 공간","낮은 차원 공간","우리는 여전히 100과 같습니다","치수이지만 수천 개보다 훨씬 낮습니다.","수십만 수십만","여기에서 우리가 사용하는 아이디어는","통계 모델링은 아마도","단어 수와 행렬 분해","아마도 신경과 관련된 더 멋진 수학","네트워크를 통해","차원 공간과 우리는 새로운","저차원 저차원","새로워진 특별한 공간","공간은 벡터를 기반으로 생성됩니다.","정보를 통합하다","어떤 단어가 함께 사용되는지에 대해","당신은 그것이 유지하는 회사에 의해 단어를 알게 될 것입니다","따라서 큰 데이터 세트의 텍스트가 필요합니다.","이런 종류의 단어를 만들거나 배우다","벡터 또는 단어 임베딩","그래서 지금 보여드리는 이 테이블은","임베딩 세트에서 가져온 것입니다.","데이터 세트 또는 불만 모음을 사용하여 만들었습니다.","미국 소비자에 대한 불만","금융 보호국","그래서 이것은 정부 기관입니다.","불평하고 말할 수 있는 미국","뭔 상관이야","신용카드와 같은 금융상품","모기지 학자금 대출","금융과 같은 일","그들은 뭔가 갔다와 같은 제품","내 신용 카드에 문제가 생겼습니다.","회사가 가지고 있는 내 모기지와 관련하여 잘못되었습니다.","불공평해서 와서 불평한다","그래서 나는 모든 불만을 받아들이고","그것은 우리의 고차원 공간이고","저차원 공간을 구축","우리는 그 공간을 보고 이해할 수 있습니다.","어떤 단어가 서로 관련이 있는지","이 공간에서 그래서 새로운 공간에서","월이라는 단어 임베딩으로 정의","년 월 복수와 같은 단어에 가장 가깝습니다.","월 할부금 그래서 이것들은 단어입니다","의 맥락에서 의미가 있습니다.","신용 카드 또는 모기지와 같은 금융 상품","이러한 임베딩에 의해 정의된 새로운 공간에서","단어 오류는 단어에 가장 가깝습니다.","사무적 실수처럼 사무적 실수처럼","문제 결함 또는 결함이 내","모기지 명세서","또는 당신을 오해하는 잘못된 의사 소통","이것들은 다음과 같은 단어라는 것을 알고 있습니다.","비슷한 방식으로 사용되므로","임베딩을 직접 만들 필요가 없습니다.","많은 데이터가 필요하기 때문에","단어를 사용할 수 있도록","사전 훈련된 임베딩","즉, 다른 사람이 만든","엄청난 양의 데이터를 기반으로","그들은 접근할 수 있고 당신은 아마 그렇지 않을 것입니다","데이터 세트 중 하나를 살펴보겠습니다.","동일한 단어 오류에 대한 결과를 보여 주는 이 표를 살펴보겠습니다.","하지만 장갑 임베딩의 경우","장갑 임베딩은","생성된 사전 훈련된 임베딩","다음과 같은 매우 큰 데이터 세트를 기반으로","모든 wikipedia 모든 Google 뉴스 데이터 세트","인터넷의 거대한 범위가 그랬던 것처럼","이러한 임베딩을 생성하기 위해","그래서 여기에서 가장 가까운 단어 중 일부는 비슷합니다.","이전에 있지만 우리는 그렇지 않은 사람들에게","더 이상 해당 도메인의 일부가 특정","성직자의 불일치와 같은 맛","이제 우리는 좋아합니다","당신은 알고 있지만 지금 우리는 잘못된 의사 소통","계산과 확률이 있다","사람들이 이야기하지 않은 것","그들의 금융 상품 불만","그래서 이것은 정말 하이라이트","이전에 여기에서 어떻게 작동하는지","우리는 우리 자신의 것을 만들었고 우리는 할 수 있었습니다","있었던 관계를 배우기 위해","이 맥락에 따라 여기에 우리가 간다","더 일반적인 집합으로","다른 곳에서 배웠다","그래서 임베딩은 훈련되거나 학습됩니다.","방대한 양의 텍스트 데이터와","그 말뭉치의 특징은","임베딩의 일부","따라서 일반적으로 기계 학습은","무엇이든 간에 절묘하게 민감하다.","그것은 당신의 훈련 데이터에 있고 이것은","언제보다 더 명확하지 않습니다","텍스트 데이터 다루기","아마도 단어 임베딩은","이 고전적인 예 중 하나처럼","이것이 사실인 경우","이것은 어떤 인간이 어떻게","말뭉치의 편견 또는 편견","임베딩에 각인됩니다.","그래서 사실 우리가 이것들 중 일부를 볼 때","가장 일반적으로 사용 가능한 임베딩","거기에 편견이 있다는 것은 우리가 그것을 볼 수 있다는 것입니다","아프리카계 미국인 이름","아프리카계 미국인에게 더 흔합니다.","그들이 연결된 미국","유럽보다 더 불쾌한 감정","이러한 임베딩 공간의 미국 이름","여성의 이름이 더 관련이 있습니다.","가족 및 남성의 이름은 경력과 더 관련이 있습니다.","여성과 관련된 용어가 더 많습니다.","예술 및 용어와 관련된","남성과 관련된 것이 더 관련이 있습니다.","과학을 사용하면 실제로 편향이 있다는 것이 밝혀졌습니다.","워드 임베딩에 내재된","단어 임베딩 자체를 사용할 수 있습니다.","변화를 정량화하기 위해","시간이 지남에 따라 사회적 태도","그래서 워드 임베딩은","과장되거나 극단적인 예일 수 있습니다.","하지만 모든 기능이","우리가 내리는 엔지니어링 결정","그것은 텍스트 데이터에 오면 중요한","결과에 미치는 영향","우리가 보는 모델 성능 측면에서","또한 얼마나 적절하거나","공정한 우리 모델은","그래서 그것이 올 때 모든 것을 주었다.","텍스트 데이터 전처리","필요한 기능 생성","당신은 많은 옵션과 꽤","약간의 책임이 있으므로 내 조언은","항상 더 간단한 모델로 시작하여","당신은 꽤 깊이 이해할 수 있습니다","좋은 통계를 채택하십시오","훈련하고 조정할 때의 연습","모델에 속지 않도록 모델","성능 향상","다른 접근을 시도할 때","또한 모델 설명 가능성을 사용하기 위해","도구와 프레임워크를 통해","덜 간단하게 이해","당신이 시도하는 모델","그래서 내 동료와 나는 썼다.","이 모든 주제와 방법에 대해","그렇다면 Tidymodels와 함께 사용하십시오.","당신은 사용하기를 좋아하고 우리는 계속 그렇게 할 것입니다","그것으로 나는 말할 것이다","너무너무 감사하고 하고싶습니다","꼭 다시","R 사용자 그룹의 주최자에게 감사","한국에서는 팀원들에게 감사 인사를 전하고 싶습니다.","Rstudio의 Tidymodels 팀은 다음과 같이","제 공동 저자인 EMIL HVITFELDT도 마찬가지입니다."]},"columns":[{"accessor":"n","name":"n","type":"numeric","cell":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526","527","528","529","530","531","532","533","534","535","536","537","538","539","540","541","542","543","544","545","546","547","548","549","550","551","552","553","554","555","556","557","558","559","560","561","562","563","564","565","566","567","568","569","570","571","572","573","574","575","576","577","578","579","580","581","582","583","584","585","586","587","588","589","590","591","592","593","594","595","596","597","598","599","600","601","602","603","604","605","606","607","608","609","610","611","612","613","614","615","616","617","618","619","620","621","622","623","624","625","626","627","628","629","630","631","632","633","634","635","636","637","638","639","640","641","642","643","644","645","646","647","648","649","650","651","652","653","654","655","656","657","658","659","660","661","662","663","664","665","666","667","668","669","670","671","672","673","674","675","676","677","678","679","680","681","682","683","684","685","686","687","688","689","690","691","692","693","694","695","696","697","698","699","700","701","702","703","704","705","706","707","708","709","710","711","712","713","714","715","716","717","718","719","720","721","722","723","724","725","726","727","728","729","730","731","732","733","734","735","736","737","738","739","740","741","742","743","744","745","746","747","748","749","750","751","752","753","754","755","756","757","758","759","760","761","762","763","764","765","766","767","768","769","770","771","772","773","774","775","776","777","778","779","780","781","782","783","784","785","786","787","788","789","790","791","792","793","794","795","796","797","798","799","800","801","802","803","804","805","806","807","808","809","810","811","812","813","814","815","816","817","818","819","820","821","822","823","824","825","826","827","828","829","830","831","832","833","834","835","836","837","838","839","840","841","842","843","844","845","846","847","848","849","850","851","852","853","854","855","856","857","858","859","860","861","862","863","864","865","866","867","868","869","870","871","872","873","874","875","876","877","878","879","880","881","882","883","884","885"],"header":"n","minWidth":10,"align":"center","headerStyle":{"background":"#f7f7f8"}},{"accessor":"start","name":"start","type":"character","cell":["1.1","6.1","11.2","15.5","21.1","27.2","31.7","34.5","39.7","42.9","48.4","52.8","57.9","63.4","66.6","71.6","75.8","77.6","81.0","84.6","86.7","91.0","94.4","96.7","101.3","103.9","105.8","110.0","113.6","116.9","122.6","125.0","129.4","135.2","140.0","144.3","146.7","150.9","153.5","161.3","165.4","167.4","170.6","173.7","175.4","177.3","179.8","181.8","185.0","191.8","195.1","199.0","204.5","209.7","212.6","218.0","223.0","229.8","232.3","233.8","235.8","242.6","244.7","247.4","250.1","252.3","256.7","259.4","263.1","265.2","267.9","270.6","273.2","275.7","279.5","281.1","284.7","287.3","291.0","294.6","298.9","302.3","307.4","310.6","316.2","319.2","321.4","324.5","328.9","335.4","338.9","341.4","343.8","348.2","350.9","354.7","357.4","364.6","368.8","371.3","373.4","376.3","379.7","383.2","385.7","390.6","393.0","397.3","399.8","405.3","408.0","412.0","414.1","416.2","419.3","423.1","426.5","429.3","431.9","437.2","440.8","443.4","445.9","449.5","451.9","454.8","458.6","460.8","463.5","465.7","469.4","472.7","475.3","477.2","483.4","487.6","491.6","493.7","496.1","498.3","503.0","505.8","508.3","512.0","514.6","517.9","521.5","524.5","526.6","528.5","531.0","535.1","538.1","540.8","543.7","545.3","548.5","551.4","554.7","558.2","560.8","563.6","565.9","568.8","570.8","573.4","577.4","580.2","582.8","586.0","588.6","591.3","598.9","603.0","605.9","607.9","610.2","612.8","615.3","618.5","620.6","623.6","628.6","630.7","633.5","635.8","638.7","641.8","643.8","645.2","649.0","654.5","658.0","662.9","665.3","667.6","671.1","675.6","677.5","681.4","683.4","687.0","688.8","691.9","693.8","698.3","700.5","704.5","709.0","712.3","715.4","717.1","719.4","722.1","725.6","727.7","729.5","732.5","735.1","737.8","739.0","741.4","744.5","749.5","754.0","755.8","758.2","761.0","765.5","769.8","773.1","775.0","776.8","779.2","781.5","784.1","786.2","788.0","789.9","792.4","793.8","795.5","797.8","800.5","803.8","806.9","808.4","810.3","812.7","815.2","818.0","823.3","825.2","828.9","833.1","836.3","838.6","840.1","844.1","847.4","849.3","851.0","855.4","860.6","864.7","867.8","869.9","873.4","879.4","881.5","883.8","887.5","891.0","893.1","895.4","897.8","900.7","901.8","905.8","907.8","910.3","912.2","915.3","917.3","919.8","921.8","924.0","927.4","931.3","935.2","937.9","941.7","944.9","947.4","951.5","956.0","958.5","960.8","963.9","967.0","971.1","974.5","977.7","980.6","983.4","988.2","990.3","993.1","997.7","1,001.4","1,003.8","1,007.2","1,010.5","1,012.9","1,015.1","1,018.2","1,020.4","1,023.5","1,027.8","1,029.7","1,031.4","1,035.8","1,038.3","1,043.5","1,046.7","1,049.0","1,052.1","1,054.6","1,057.8","1,059.7","1,062.9","1,067.1","1,069.8","1,071.8","1,075.4","1,078.8","1,084.5","1,088.1","1,091.3","1,093.6","1,096.2","1,098.8","1,101.7","1,103.7","1,105.8","1,108.7","1,110.5","1,114.0","1,116.8","1,120.2","1,123.0","1,126.0","1,129.0","1,131.8","1,134.6","1,138.7","1,140.6","1,143.6","1,145.8","1,148.6","1,151.0","1,153.2","1,155.8","1,158.5","1,160.8","1,165.9","1,173.3","1,175.2","1,178.2","1,182.0","1,184.3","1,186.8","1,191.8","1,193.8","1,196.6","1,199.0","1,203.3","1,205.4","1,208.5","1,210.9","1,213.1","1,216.0","1,218.5","1,220.9","1,223.4","1,229.0","1,231.4","1,233.8","1,236.4","1,239.2","1,241.2","1,242.6","1,245.6","1,249.4","1,252.9","1,255.8","1,258.4","1,261.4","1,264.7","1,268.1","1,271.9","1,273.4","1,276.2","1,280.0","1,284.2","1,287.1","1,292.9","1,295.3","1,298.2","1,302.2","1,306.5","1,309.2","1,314.2","1,317.4","1,324.6","1,326.5","1,329.4","1,331.4","1,334.5","1,338.6","1,341.8","1,344.3","1,346.9","1,350.8","1,353.8","1,355.7","1,358.6","1,361.6","1,364.2","1,367.1","1,370.4","1,373.6","1,376.2","1,380.3","1,383.4","1,386.7","1,389.5","1,391.8","1,393.9","1,396.1","1,398.9","1,400.4","1,405.0","1,408.0","1,411.9","1,415.0","1,416.6","1,419.6","1,421.9","1,427.2","1,432.1","1,434.4","1,437.7","1,440.4","1,444.4","1,447.3","1,449.0","1,452.0","1,455.8","1,460.0","1,462.4","1,466.4","1,468.7","1,471.2","1,473.0","1,477.4","1,480.2","1,481.8","1,483.7","1,486.4","1,488.8","1,493.0","1,495.8","1,498.3","1,500.7","1,504.2","1,506.9","1,512.8","1,517.0","1,520.5","1,523.6","1,527.0","1,530.6","1,534.0","1,537.4","1,540.6","1,543.0","1,545.6","1,547.5","1,550.0","1,555.4","1,559.0","1,561.4","1,563.4","1,565.6","1,567.7","1,571.4","1,576.4","1,579.8","1,582.5","1,586.1","1,588.4","1,589.9","1,593.8","1,596.8","1,602.7","1,605.7","1,607.4","1,611.8","1,614.8","1,617.8","1,621.1","1,625.3","1,627.7","1,630.7","1,633.0","1,637.1","1,639.2","1,641.8","1,644.7","1,648.0","1,650.7","1,652.6","1,655.4","1,657.6","1,662.9","1,666.8","1,670.2","1,672.4","1,675.0","1,679.3","1,682.1","1,684.5","1,687.0","1,690.3","1,692.8","1,696.4","1,699.4","1,702.1","1,703.9","1,706.8","1,709.9","1,713.2","1,715.4","1,719.3","1,721.4","1,725.7","1,727.4","1,729.9","1,732.2","1,738.8","1,742.1","1,745.3","1,747.8","1,750.6","1,755.4","1,758.6","1,760.0","1,764.5","1,767.9","1,771.3","1,773.2","1,777.3","1,780.3","1,782.5","1,786.2","1,789.0","1,791.8","1,794.5","1,798.6","1,801.4","1,802.6","1,805.0","1,807.3","1,809.9","1,812.6","1,814.9","1,816.8","1,819.8","1,824.2","1,828.2","1,831.4","1,836.2","1,839.0","1,843.7","1,847.0","1,851.6","1,856.8","1,861.5","1,864.2","1,867.0","1,869.6","1,872.1","1,875.5","1,879.4","1,883.5","1,888.6","1,891.1","1,895.9","1,898.6","1,902.5","1,905.7","1,909.0","1,910.6","1,913.4","1,916.9","1,922.3","1,924.1","1,925.8","1,932.1","1,936.1","1,938.2","1,940.2","1,945.2","1,949.7","1,951.6","1,953.5","1,956.6","1,959.9","1,963.4","1,965.4","1,967.8","1,971.6","1,973.8","1,976.5","1,980.2","1,984.6","1,986.5","1,989.5","1,992.7","1,995.2","1,996.9","1,999.5","2,002.7","2,006.6","2,009.0","2,012.6","2,017.1","2,021.4","2,025.5","2,028.3","2,032.4","2,036.9","2,039.3","2,041.8","2,044.6","2,047.2","2,051.4","2,056.4","2,059.2","2,062.8","2,065.8","2,069.0","2,071.4","2,073.8","2,076.3","2,078.4","2,080.7","2,083.2","2,085.5","2,087.2","2,089.8","2,092.2","2,095.0","2,097.2","2,099.4","2,101.3","2,105.1","2,108.4","2,110.8","2,113.2","2,115.3","2,117.4","2,120.1","2,124.4","2,127.6","2,130.3","2,132.3","2,134.0","2,139.0","2,142.6","2,145.5","2,147.9","2,150.1","2,153.0","2,156.3","2,158.6","2,161.4","2,164.1","2,170.4","2,174.3","2,176.6","2,179.6","2,182.6","2,185.1","2,187.5","2,190.6","2,192.4","2,195.7","2,198.3","2,203.0","2,208.5","2,211.9","2,216.3","2,220.1","2,223.3","2,225.9","2,229.9","2,233.1","2,237.0","2,239.8","2,241.8","2,243.8","2,247.7","2,251.2","2,257.2","2,260.2","2,263.5","2,266.5","2,271.7","2,275.3","2,283.0","2,287.1","2,289.3","2,292.8","2,295.4","2,297.4","2,299.6","2,303.4","2,307.6","2,311.0","2,314.6","2,317.4","2,320.2","2,324.0","2,326.2","2,328.2","2,331.8","2,335.6","2,338.2","2,340.5","2,343.7","2,345.8","2,348.5","2,352.0","2,355.0","2,358.6","2,364.1","2,366.5","2,369.4","2,372.0","2,375.1","2,377.0","2,384.6","2,389.2","2,391.7","2,393.8","2,397.9","2,400.7","2,404.7","2,407.6","2,409.8","2,411.1","2,413.1","2,415.4","2,420.0","2,425.4","2,427.8","2,430.2","2,433.9","2,437.7","2,440.2","2,444.1","2,449.7","2,454.6","2,457.8","2,463.4","2,466.7","2,469.7","2,475.0","2,479.0","2,483.4","2,486.2","2,488.3","2,491.6","2,494.8","2,497.2","2,499.5","2,502.2","2,504.9","2,507.5","2,511.4","2,514.1","2,520.2","2,522.8","2,524.6","2,527.4","2,531.0","2,535.3","2,541.4","2,544.8","2,549.5","2,551.7","2,554.8","2,558.6","2,560.8","2,563.3","2,566.7","2,569.0","2,571.6","2,574.3","2,578.2","2,580.8","2,582.6","2,586.8","2,590.4","2,593.1","2,596.7","2,599.6","2,602.1","2,604.2","2,607.4","2,610.2","2,612.2","2,615.8","2,618.2","2,620.3","2,622.4","2,626.1","2,631.0","2,634.4","2,639.0","2,641.4","2,644.7","2,652.2","2,654.6","2,657.5","2,659.7","2,663.3","2,667.7","2,670.2","2,676.1","2,678.6","2,680.6","2,682.6","2,688.3","2,691.4","2,694.2","2,698.5","2,701.4","2,705.0","2,708.2","2,711.1","2,713.6","2,716.7","2,718.5","2,722.6","2,726.0","2,729.5","2,731.6","2,734.6","2,737.0","2,739.7","2,743.5","2,746.6","2,750.6","2,753.5","2,756.7","2,761.8","2,764.7","2,767.1","2,770.4","2,772.8","2,776.1","2,778.0","2,780.2","2,782.6","2,784.7","2,788.7","2,790.3","2,793.0","2,796.6","2,799.0","2,802.6","2,805.1"],"header":"start","minWidth":25,"align":"center","headerStyle":{"background":"#f7f7f8"}},{"accessor":"end","name":"end","type":"character","cell":["8.64","13.52","18.08","24.00","29.36","34.48","36.88","42.88","45.04","50.72","55.44","60.48","66.64","69.36","75.76","77.60","81.04","84.56","86.72","90.96","94.40","96.72","99.52","103.92","105.76","110.00","111.20","116.88","119.84","125.04","128.48","131.92","137.60","144.32","146.72","149.20","153.52","155.36","164.32","167.44","170.56","173.68","175.44","177.28","179.76","181.84","185.04","187.68","195.12","197.92","202.16","207.28","212.64","214.88","220.48","224.96","232.32","233.84","235.76","238.40","244.72","247.44","250.08","252.32","253.52","259.36","260.80","265.20","267.92","270.64","273.20","275.68","278.32","281.12","284.72","287.28","289.36","293.68","296.16","301.12","304.80","310.64","313.12","319.20","321.36","322.80","327.12","332.24","337.36","341.36","343.84","347.12","350.88","352.80","357.44","360.48","366.80","371.28","373.44","376.32","379.72","383.20","385.68","389.68","393.04","396.00","399.84","403.44","408.00","411.12","414.08","416.24","419.28","423.12","426.48","429.28","431.92","434.48","440.80","443.44","445.92","447.68","451.92","454.80","458.56","460.80","463.52","465.68","467.12","472.72","475.28","477.20","481.20","485.36","490.32","493.68","496.08","498.32","501.52","505.76","508.32","510.08","514.56","517.92","519.20","524.48","526.64","528.48","531.04","533.04","538.08","540.80","543.68","545.28","548.48","551.36","554.72","558.16","560.80","563.60","565.92","568.80","570.80","573.36","575.84","580.24","582.80","586.00","588.64","591.28","594.48","601.60","605.92","607.92","610.24","612.80","615.28","617.44","620.56","623.60","626.32","630.72","633.52","635.76","638.72","641.76","643.84","645.20","647.68","651.36","658.00","661.68","665.28","667.60","671.12","674.00","677.52","680.00","683.36","685.44","688.80","691.92","693.84","696.80","700.48","704.48","707.04","712.32","715.36","717.12","719.44","722.08","725.60","727.68","729.52","732.48","735.12","737.84","739.04","741.44","744.48","748.16","750.88","755.84","758.24","760.96","764.48","768.16","773.12","775.04","776.80","779.20","781.52","784.08","786.24","788.00","789.92","792.40","793.84","795.52","797.84","800.48","803.84","806.88","808.40","810.32","812.72","815.20","818.00","820.64","825.20","827.68","832.32","836.32","838.56","840.08","842.08","847.44","849.28","850.96","853.36","857.84","862.88","867.76","869.92","872.56","877.52","881.52","883.84","887.52","889.76","893.12","895.44","897.76","899.84","901.84","904.88","907.76","910.32","912.24","915.28","917.28","919.76","921.76","924.00","927.44","931.28","935.20","937.92","941.68","944.88","947.44","949.36","954.96","958.48","960.80","963.92","967.04","971.12","974.48","977.68","980.56","983.36","985.84","990.32","992.24","996.16","1,001.44","1,003.84","1,007.20","1,010.48","1,012.88","1,015.12","1,016.48","1,020.40","1,023.52","1,026.40","1,029.68","1,031.44","1,034.08","1,038.32","1,039.92","1,043.36","1,048.96","1,052.08","1,054.64","1,057.84","1,059.68","1,062.88","1,064.24","1,069.76","1,071.84","1,074.08","1,078.80","1,081.84","1,088.08","1,090.16","1,093.60","1,096.24","1,098.80","1,101.68","1,102.64","1,105.76","1,108.72","1,110.48","1,114.00","1,116.80","1,120.16","1,123.04","1,126.00","1,128.96","1,131.04","1,134.64","1,137.60","1,140.64","1,143.60","1,145.76","1,148.64","1,151.04","1,153.20","1,155.84","1,158.48","1,160.80","1,164.48","1,167.12","1,175.20","1,178.24","1,182.00","1,184.32","1,186.80","1,190.64","1,190.64","1,196.56","1,198.96","1,200.88","1,205.44","1,208.48","1,210.88","1,213.12","1,216.00","1,217.44","1,220.88","1,223.44","1,225.20","1,231.36","1,233.76","1,236.40","1,239.20","1,241.20","1,242.64","1,245.60","1,249.36","1,252.00","1,254.96","1,258.40","1,261.44","1,264.72","1,266.24","1,269.28","1,273.36","1,276.24","1,280.00","1,282.96","1,287.12","1,288.80","1,295.28","1,297.28","1,301.36","1,306.48","1,309.20","1,313.12","1,317.44","1,320.24","1,326.48","1,329.36","1,331.44","1,334.48","1,337.28","1,339.60","1,344.32","1,346.88","1,349.92","1,353.76","1,355.68","1,358.64","1,361.60","1,364.16","1,367.12","1,370.40","1,372.08","1,376.24","1,380.32","1,383.36","1,386.72","1,389.52","1,391.84","1,393.92","1,396.08","1,398.88","1,400.40","1,402.72","1,408.00","1,411.20","1,415.04","1,416.64","1,419.60","1,421.92","1,424.32","1,432.08","1,434.40","1,437.68","1,440.40","1,444.40","1,447.28","1,448.96","1,452.00","1,455.84","1,458.88","1,462.40","1,466.40","1,468.72","1,471.20","1,472.96","1,477.44","1,480.16","1,481.84","1,483.68","1,486.40","1,488.80","1,491.76","1,495.76","1,498.32","1,500.72","1,503.12","1,506.88","1,510.00","1,516.08","1,520.48","1,523.60","1,527.04","1,528.56","1,532.16","1,537.44","1,540.56","1,543.04","1,545.60","1,547.52","1,550.00","1,552.24","1,556.96","1,561.36","1,563.36","1,565.60","1,567.68","1,571.44","1,573.52","1,579.76","1,582.48","1,584.72","1,588.40","1,589.92","1,593.84","1,596.80","1,599.76","1,605.68","1,607.44","1,611.84","1,614.80","1,617.84","1,621.12","1,623.92","1,627.68","1,630.72","1,632.96","1,634.72","1,639.20","1,641.76","1,644.72","1,648.00","1,650.72","1,652.64","1,655.36","1,657.60","1,659.92","1,662.40","1,668.96","1,672.40","1,675.04","1,678.24","1,682.08","1,684.48","1,686.96","1,690.32","1,692.80","1,696.40","1,699.44","1,702.08","1,703.92","1,706.80","1,709.92","1,713.20","1,715.36","1,717.60","1,721.36","1,722.48","1,727.44","1,729.92","1,732.24","1,734.96","1,741.04","1,744.00","1,747.76","1,750.64","1,754.40","1,758.56","1,760.00","1,762.00","1,766.48","1,771.28","1,773.20","1,777.28","1,780.32","1,782.48","1,784.88","1,789.04","1,791.76","1,794.48","1,797.52","1,801.36","1,802.64","1,804.96","1,807.28","1,809.92","1,812.56","1,814.88","1,816.80","1,819.84","1,821.52","1,825.60","1,831.44","1,833.20","1,838.96","1,840.56","1,846.96","1,851.60","1,853.36","1,858.32","1,864.16","1,867.04","1,869.60","1,872.08","1,874.72","1,877.84","1,883.52","1,886.56","1,891.12","1,894.96","1,898.56","1,901.28","1,905.68","1,907.60","1,910.56","1,913.36","1,914.88","1,920.48","1,924.08","1,925.84","1,929.44","1,936.08","1,938.24","1,940.16","1,944.00","1,948.88","1,951.60","1,953.52","1,956.64","1,959.92","1,963.44","1,965.36","1,967.76","1,971.60","1,973.76","1,976.48","1,980.16","1,982.24","1,986.48","1,989.52","1,991.60","1,995.20","1,996.88","1,999.52","2,001.68","2,004.88","2,009.04","2,012.56","2,015.04","2,019.28","2,024.40","2,028.32","2,031.04","2,033.84","2,039.28","2,041.76","2,044.64","2,047.20","2,049.68","2,053.84","2,059.20","2,060.56","2,065.76","2,068.96","2,071.36","2,073.76","2,076.32","2,078.40","2,080.72","2,083.20","2,085.52","2,087.20","2,089.76","2,092.24","2,094.96","2,097.20","2,099.36","2,101.28","2,105.12","2,108.40","2,110.80","2,113.20","2,115.28","2,117.44","2,120.08","2,124.40","2,127.60","2,130.32","2,132.32","2,134.00","2,136.00","2,142.56","2,145.52","2,147.92","2,150.08","2,153.04","2,156.32","2,158.56","2,161.36","2,164.08","2,166.72","2,174.32","2,176.64","2,179.60","2,182.64","2,185.12","2,187.52","2,190.64","2,192.40","2,195.68","2,198.32","2,203.04","2,205.76","2,211.92","2,213.52","2,217.52","2,223.28","2,225.92","2,227.60","2,233.12","2,235.52","2,239.84","2,241.76","2,243.84","2,246.08","2,251.20","2,254.32","2,259.36","2,263.52","2,266.48","2,269.68","2,275.28","2,278.16","2,284.96","2,289.28","2,292.80","2,295.44","2,297.36","2,299.60","2,302.24","2,305.92","2,311.04","2,313.60","2,317.36","2,320.16","2,322.48","2,326.16","2,328.16","2,329.84","2,335.60","2,338.24","2,340.48","2,343.68","2,345.84","2,348.48","2,352.00","2,355.04","2,358.56","2,361.84","2,366.48","2,369.36","2,372.00","2,375.12","2,377.04","2,380.56","2,388.00","2,391.68","2,393.84","2,396.64","2,400.72","2,403.68","2,407.60","2,409.76","2,411.12","2,413.12","2,415.36","2,417.52","2,424.00","2,427.84","2,430.16","2,432.40","2,437.68","2,440.24","2,444.08","2,448.00","2,450.88","2,456.48","2,461.28","2,465.12","2,469.68","2,471.76","2,477.68","2,480.88","2,486.24","2,488.32","2,490.88","2,493.60","2,497.20","2,499.52","2,502.24","2,504.88","2,507.52","2,509.84","2,514.08","2,515.60","2,522.80","2,524.56","2,527.36","2,529.92","2,532.80","2,536.64","2,544.80","2,547.28","2,551.68","2,554.80","2,558.64","2,560.84","2,563.28","2,566.72","2,568.96","2,571.60","2,574.32","2,578.24","2,580.80","2,582.64","2,586.80","2,590.40","2,593.12","2,596.72","2,599.60","2,602.08","2,604.24","2,607.36","2,610.16","2,612.24","2,615.84","2,618.16","2,620.32","2,622.40","2,623.76","2,628.16","2,634.40","2,636.48","2,641.44","2,644.72","2,647.04","2,654.64","2,657.52","2,659.68","2,663.28","2,665.52","2,670.24","2,671.60","2,678.64","2,680.56","2,682.64","2,684.08","2,691.44","2,694.16","2,696.80","2,701.44","2,705.04","2,708.16","2,711.12","2,713.60","2,716.72","2,718.48","2,721.44","2,726.00","2,729.52","2,731.60","2,734.56","2,737.04","2,739.68","2,743.52","2,746.56","2,750.48","2,750.48","2,756.72","2,759.04","2,764.72","2,767.12","2,770.40","2,772.84","2,776.08","2,778.00","2,780.24","2,782.64","2,784.72","2,786.88","2,790.32","2,793.04","2,794.88","2,798.96","2,802.56","2,805.12","2,807.44","2,811.24"],"header":"end","minWidth":25,"align":"center","headerStyle":{"background":"#f7f7f8"}},{"accessor":"subtitle","name":"subtitle","type":"character","cell":["Hi my name is julia silge.  I'm a data scientist and software engineer at RStudio.","and I'd like to thank the the organizers of the R user group in Korea","so much for having me.  Today speak to you","I am so happy to be speaking specifically today about creating","features for machine learning from text data for a couple of reasons","Having a better understanding of what we do to take text data","and then to make it appropriate","as an input for machine learning algorithms has many benefits","both if you are directly getting ready","to train a model or if you're at the beginning of some text analysis project","or if you are trying to understand the behavior of a model that","you're interacting with some way which is something that we do in our work as data scientists","or in our in our daily lives more and more","so when we build models for text either supervised or unsupervised","we start with something","like this this is some example text data","that I'll use a couple of times during","this talk that describes some animals","I'm using some text data so","you know to me as an english","speaker looks familiar like","I am as someone who uses a human","language so I look at this and I can","read it I could speak it aloud and I understand","I can interpret it what it means","so this kind of data this sort of","natural language data is being generated","all the time in all kinds of languages","in all kinds of contexts so","whether you work in healthcare in tech in finance","basically any kind of organization this","sort of text data is being generated","by customers by clients by internal stakeholders","inside of a business by people taking surveys","via social media via business processes","and in all this natural language","there's information latent in","that text data that can be used to make better decisions","However, computers are not great at looking at this and doing","math on language as it's represented like this","and instead language has to be","dramatically transformed to some kind of","machine readable numeric representation","that looks more like this what I'm","showing here on the screen to be ready","for almost any kind of model","so I spent a fair amount of time working","on software for people to be able to do","exploratory data analysis, visualization, summarization","tasks like that with text data in a tidy","format where we have one observation per row","and I love using tidy data principles for text analysis","especially during those exploratory phases of an analysis","when it comes time to build a model","often what the underlying mathematical implementation really needs","is typically something like this which is a","way to this particular representation is called the document term matrix","so the exact representation may differ from","what I've shown here","what I have here is we're weighting","things by counts so each row in this matrix is a document","each column is a is a word.","A token and the numbers represent","counts how many times does each document","use each word you could weight it in a different way","using say TF-IDF instead of counts","or you might keep sequence information","if you're interested in building a deep learning model but basically","for all kinds of text modeling","from simpler models like Naive Bayes","models which work well for text","to word embeddings to really the most","state-of-the-art kind of work that's","happening today like transformers for text data","we have to heavily","feature engineer and process language to","get it to some kind of representation","that's suitable for machine learning algorithms","so I work on an open source framework in R","for modeling and machine learning that's called Tidymodels and the examples that","I'll be showing today use Tidymodels code","some of the specific goals of the Tidymodels project are to provide","a consistent flexible framework for real","world modeling practitioners people who are you know doing","that are dealing with real world data","those who are just starting out to","those who are very experienced in modeling and","the goal is to harmonize the heterogeneous","interfaces that exist within R and to encourage good statistical practice","I'm glad to get to show you some of what I work on","and build and how we apply it to text","modeling but a lot of what I will talk","about today isn't very specific to Tidymodels","or even to R. I know this is an R user","group but what we're going to talk about and focus on","is a little more conceptual and basic","how do we transform text into predictors for machine learning","I am excited though to talk about Tidymodels and Tidymodels if you have not","used it before is a meta package","in a similar way that the Tidyverse is","a meta package so if you've ever typed","library Tidyverse and then you've used","ggplot2 for visualization","dplyr for data manipulation","Tidymodels works in a similar way","there are different packages inside of it","that are used for different purposes","so the pre-processing or the feature","engineering is part of a broader model process","you know it that process starts really","with with exploratory data analysis","that helps us decide what kind of model","we will build and then it comes to","completion I think I would argue with","model evaluation when you","measure how well your model performed","Tidymodels as a piece of software is","made up of our packages each of which","has a specific focus like our sample","is for re-sampling data to be able to","create bootstrap resamples","cross-validation resamples all different","kinds of resamples you might want to use to","train and evaluate models","the tune package is for hyper parameter","tuning as you might guess from the name","one of these packages is for feature","engineering for a data preprocessing","feature engineering and it is the one","that is called recipes","so in Tidymodels we capture this idea","of data pre-processing and feature","engineering in the concept of a","pre-processing recipe that has steps so you choose","ingredients or variables","that you're going to use then you define the steps","that go into your recipe","then you prepare them using training","data and then you can apply that to any","data set like testing data or new data at prediction time","so the variables or ingredients that we","use in modeling come in all kinds of","shapes and sizes including text data","so some of the techniques and approaches","that we use for pre-processing text data","are the same um as for any other kind of data that","you might use like non-text data","numeric data categorical data some for","some of it is the same","but some of what you need to know to be","able to do a good job","in this process for text is different","and is specific to the nature of what","language data is like","and so I've written a book with my","co-author Emile Hvitfeldt on supervised","machine learning for text analysis and R","and fully the first third of the book","focuses on how we transform","the natural language that we have in","text data into features for modeling","the middle section is about how we use","these features in","simpler or more traditional machine","learning models like regularized","regression or support vector machines and","then the last third of the book","talks about how we use deep learning","models with text data so deep learning","models still require these kinds of","transformations from natural language","into features as input for these kinds of models but","deep learning models are often able to","inherently learn structure of features","from text in ways that those","more traditional or simpler machine","learning models are not","so this book is now complete and","available as of this month as of november","folks are getting their first paper","copies and also this book is available","in its entirety at smalltar.com","so if you're new to dealing with text","data understanding these","fundamental pre-processing approaches","for text will set you up for being able","to train effective models","if you're really experienced with text","data if you've dealt with it a lot","already you've probably noticed like we have","that the existing you know resources or literature whether","that's books or tutorials or blog posts","is quite sparse when it comes to","detailed thoughtful explorations of how","these pre-processing steps work","and how choices made in these feature","engineering steps impact our model output","so let's walk through","several of some of these like basic","feature engineering approaches and how","they work and what they do let's start out with","tokenization","so typically one of the first steps in","transfer information from natural","language to machine learning feature for","really any kind of text analysis","including exploratory data analysis","or building a model. Anything is tokenization","in tokenization we take an input some","string some character vector and some","kind of token type","some meaningful unit of text","we're interested in a word","and we split the input pieces into","tokens that correspond to the type","we're interested in","so most commonly the meaningful unit or","type of token that we want to split text","into units of is a word","so this might seem","straightforward or obvious but it turns","out it's difficult to clearly define","what a word is for many or even most languages","so many languages do not use white space","between words at all","which presents a challenge","for tokenization even languages that","do use white space like english and korean","often have particular examples that are ambiguous","like contractions in english like didn't","which should be","you know maybe more accurately","considered two words the way","particles are used in Korean","and how pronouns and negation words are","written in romance languages like","italian and french where they're stuck","together and really maybe they should be","considered two words","once you have figured out what you're","going to do and you make some choices","and you tokenize your text then it's on","its way to being able to be used","in exploratory data analysis or","unsupervised algorithms or as features","for predictive modeling which is what","we're talking about here and what these","results show here so these results are","from a regression model trained on","descriptions of media","from artwork in the Tate collection in the UK so","what we're predicting in what we are","predicting is when what year","was a piece of art created based on the","the medium that the artwork was created","with and the medium is described with a","little bit of text","so we see here that artwork created using graphite","watercolor and engraving was more likely","to be created earlier","that though that is more likely to come","from older art and artwork that is created using","photography screen point or sorry screen print","screen printing and and dung","and glitter are more likely to be","created later. there this is more likely","to come from contemporary art modern art","so the the way that we tokenize this text","you know we started with natural human","generated texts of people writing out","the descriptions of the the media","that these art pieces of art were created with","and the way we tokenized that natural","human generated text that we started","with has a big impact on what we learned","from it if we tokenized in a different way","we would have gotten different","results in terms of performance like how","accurately we were able to predict","predict the year and also in terms of","how we interpret the model like what is","it that we're able to learn from it","so this is one kind of tokenization to","the single word but we also","we all another way to tokenize instead","of breaking up into single words or","unigrams we can tokenize to n-grams","so an n-gram is a continuous sequence of","N items from a given sequence of texts","so this shows that same piece of little","bit of text i'm describing this animal","divided up into bi-grams or n-grams of","two tokens so notice how the words in","the bi-grams overlap so the word collard","appears in both of the first bigrams the collared","collared peccary peccary also","referred to so n-gram","tokenization slides along the text to","create overlapping sets of tokens","this shows tri-grams for the same thing","so using uni-grams one word is","faster and more efficient but we don't","capture information about word order","I'm using a higher value","two or three or even more keeps","more complex information about word","order and concepts","that are described in multi-word phrases","but the vector space of tokens","increases dramatically","that corresponds to a reduction in token","counts we don't count each token as very","many times and that means depending on","your particular data set","you might not be able to get good results","so combining different degrees of","n-grams can allow you to extract","different levels of detail from text so uni-grams","can tell you which individual words have","been used a lot of times","some of those words might be overlooked","in bi-gram or tri-gram crowns if they","don't co-appear with other words as often","this plot compares model performance for","a Lasso regression model predicting the","year of supreme court opinions the","United States supreme court opinions","with three different degrees of n-grams","what we're doing here is we are taking","the text of the writings of the United","States supreme court and we're predicting","when did it when was that text","written so can we predict how old a","piece of text is from the contents of the text","so holding the number of tokens","constant at a thousand using uni-grams alone","performs best for this corpus of","opinions from the United States supreme court","this is not always the case depending on","the kind of model you use the data set","itself we might see the best performance","combining uni-grams and bi-grams or maybe","some other option","in this case if we wanted to incorporate","some of that more complex information","that we have in the bi-grams and the","tri-grams we probably would need to","increase the number of","tokens in the model quite a bit","so keep in mind when you look at results","like these that identifying n-grams is","computationally expensive","this is especially compared to the","amount of like a model the improvement","in model performance that we often see like if we","if we see some you know modest","improvement by adding in bigrams it's","important to keep in mind how much","improvement we see relative to","how long it takes to","identify bi-grams and then train that","model so for example for this data set","of supreme court opinions where we held","the number of tokens constant so the","model training had the same number of tokens in it","using bi-grams plus uni-grams takes twice as long to train","to do the feature engineering and the","training than only uni-grams and adding","in tri-grams as well takes almost five","times as long as training on uni-grams","alone so this is a computationally","expensive thing to do","going in the other direction","we can tokenize to units smaller than","words so like these are what are called","character shingles so we take words","the collared peccary","and we can instead of looking at words","we can go down and look at sub word","information there's multiple different","ways to break words up into sub words","that are appropriate for machine learning","and often these kinds of approaches","or algorithms have the benefit of being","able to encode unknown or new words","at prediction time so when it when it's","time to make a prediction on new data","it's not it's not uncommon for there to","be new vocabulary words at that time and","if we didn't see them in the training","data you know what are we going to do","about those new words when we train","using subword information often we can","handle those new words if we saw the subword","in our training data set so","using this kind of subword information","is a way to incorporate morphological","sequences into our models of you know","various kinds of this is something that applies to","various languages not just english","so these results are for a","classification model with a data set of","very short texts it's just the names of","post offices in the United States so super short","and the goal of the model was to predict","the post office located in hawaii","in the middle of the pacific ocean or","it located in the rest of the united states","so I created features for the model that are","subwords of these post office names","and we end up learning that the names","that start with h and p or contain that ale","sub word are more likely to be in hawaii","and the sub words a and d and ri and ing are are","more likely to come from the post office","that are outside of hawaii","so this is an example of how we","tokenized differently and we're able to","learn something new we're able to learn something","different so in Tidymodels we collect all these","kinds of decisions about tokenization","and code that looks like this","so we start with a recipe that specifies what","variables or ingredients that we'll use","and then we define these preprocessing","steps so even at this first","and arguably you know simple and basic","step the choices that we make affect our","modeling results in a big way","the next pre-processing um step that","I want to talk about is stop words","so once we have split text","into tokens we often find that not","all words carry the same amount of","information if maybe any information at","all actually for a machine learning task","so common words that carry little or","perhaps no meaningful information are","called stopwords","so this is one of the stopword lists","that's available for","Korean so it's common advice and practice","to say hey just remove just remove","remove these stopwords for a lot of","natural language processing tasks","what I'm showing here","is the entirety of one of the shorter","english stopword lists that's used","really broadly so you know it's words like I","me my pronouns conjunctions and of the","and these are very common words","that are not considered super important","the decision though to just remove","stopwords is often more involved and","perhaps more fraught than what you'll","than what you'll find reflected in a lot","of resources that are out there","so almost all the time real world NLP","practitioners use pre-made stopword lists","so this plot visualizes","set intersections for three common","stopword lists in english","in what is called an upset plot","so the three lists are called the","snowball list smart and the iso list","so you can see the the lengths of the","list are represented by the length of","the bars and then we see the","intersections which words are in common","on these lists by the by the vertical","bars so the lengths of the list are quite different","and also notice they don't all contain","the same sets of words","the important thing to remember about","stopword lexicons is that","they are not created in some","neutral perfect setting but instead they are","they are context specific","they they can be biased both of these","things are true because they are lists","created from large data sets of language","so they reflect the characteristics of the data used in","their creation so this is","the ten words that are in the english","language smart lexicon but not in the","English snowball lexicon","so notice that they're all contractions","but that's not because the snowball","exchange doesn't include contractions","it has a lot of them also notice that it has","that she's is on this list and so that means that","that list has he's but it does not have","the list she's","so this is an example of that","The bias I mentioned that occurs because","these lists are created from large data","sets of text lexicon creators look at the most","frequent words in some big corpus of","language they make a cut off","and then some decisions about what to include or","exclude you know","based on the list that they","have and you end up here so because","in many large data sets of language","you have more representation of","men you end up with a","situation like this where a stopword","list will have he's but not she's","so many decisions when it comes to","modeling or analysis with language","we as practitioners have to decide","what is appropriate for our particular domain","it turns out this is even true when it","comes to picking a stopword list","so in Tidymodels we can implement a","pre-processing step like removing stopwords","by adding an additional step to our","recipe so first we specified what","variables we would use then we tokenized","the text and now we are removing","stopwords here using just the default","step since we are not passing in any","other arguments we could though","use a non-default step or even a custom","list if that was most appropriate to our domain","this plot compares the model performance","for predicting the year of","that same data set of","supreme court opinions with three","different stopword lexicons of different lengths","so the snowball lexicon contains the","smallest number of words and in this","case it results in the best performance","so removing fewer stopwords results in","the best performance here so this","specific result is not generalizable to","all data sets and contexts but the fact","that removing different sets of","stopwords can have noticeably different","effects on your model that is quite","transferable so the only way to know","what is the best thing to do is to try","several options and see so machine","learning in general.","this is an empirical field right like we","don't know we don't often have reasons a priori to","know what will be the best thing to do","and so typically we have to","try a different option to see what will","be the best thing all right. then the the third","pre-processing step that I want to talk about","for text is stemming","so when we deal with text often","documents contain different versions of","one base word often called a stem so what","if say for an english example","if we aren't interested in the difference","between animals plural and animal","singular and we want to treat them both together","so that idea is at the heart of stemming","so there's no one","right way or correct way to stem text so","this plot shows three approaches for","stemming in English","starting from hey let's just remove a final s","to more complex rules about plural","handling plural endings that middle","one it is called the s stemmer","it's a set of it's like a little set of rules","and that last one is the best known","one probably the best-known","implementation of stemming in English","called the Porter algorithm","so you can see here that Porter stemming","is the most different from the other two","in the top 20 words here from the data","set of animal descriptions that I've","been using we see how the word species","was treated differently animal predator","this sort of collection of words","live living life lives that was treated","differently so practitioners are typically","interested in stemming text data because","it buckets tokens together that we believe","belong together in in a way that","we understand that as human users","of language so we can use approaches like this","which are pretty like step-by-step","rules based this is typically called","stemming or and it's fairly","algorithmic in nature like","first do this then do this then do this","or you can use lemmatization","which is usually based on large dictionaries","of words and it incorporates like a","linguistic understanding of what words belong together","so most of the existing approaches for","this kind of task in Korean are","are limited lemmatizers based on these","dictionaries and that are trained","using large data sets of language","so this seems like it's going to be a helpful thing to do","when you hear about this you're like","oh yeah sounds good,  sounds smart","especially because with text data we are typically","overwhelmed with features with numbers of tokens","this is typically the situation","when we're dealing with text data","so here we have these animal description data","and I made a matrix representation of it","like we would typically use in some","machine learning algorithm","and look how many features there are","16,000 almost 17,000 features","that's the number of features that","would be going into the model","look at the sparsity","98 percent sparse that's high very","sparse data so this is the sparsity of","the data that will go into the machine","learning algorithm to build our","supervised machine learning model","if we stem the words","if I use here an approach for stemming","we reduce the number of word features by","many thousands the sparsity unfortunately did not","change as much but we reduced the number","of features by a lot by bucketing those","words together that our stemming algorithm","belong together so you know","common sense says","reducing the number of words features","so dramatically is going to perform","improve the performance of our machine learning model","but that is that does assume that we","have not lost any important information","by by stemming and it turns out that stemming or","lemmatization can often be very helpful in some","contexts but the typical algorithms used for these","are somewhat aggressive","and they have been built to favor sensitivity","or recall or the true positive rate and","this is at the expense of the","specificity or the precision or the true","negative rate so in a supervised machine","learning context what this does is this","affects a model's positive predictive","value the precision or its ability to","to not incorrectly label true negatives","as positive I hope I got that right","so you know to make this more concrete","stemming can increase a model's ability","to find the positive examples","of say the animal descriptions that are","associated with say a certain diet if","that's what we're modeling however if","text is over stemmed","the resulting model loses its ability to","label the negative examples","say the descriptions that are not about","that diet that's what we're looking for","and this can be a real challenge when","training models with text data kind of","finding that that balance there because","often we don't have a","dial that we can change on these","stemming on these stemming algorithms","so even just very basic pre-processing","for text like what I'm showing here in","this feature engineering recipe can be","computationally expensive","and the choices that a practitioner","makes like whether or not to remove","stopwords or to stem text can have dramatic","impact on how machine learning models","of all kinds perform whether those are","simpler models","more traditional machine learning models","or deep learning models what this means is that","the price the prioritization that we","as practitioners give to like learning","teaching and writing about feature","engineering steps for text really","contributes to better more robust","statistical practice in our field","I mentioned before the sparsity of text","data and I want to come back to that","because it is one of text data's really","defining characteristics because of just how language works","we use a few words a lot of times","and then a lot of words only just a","couple of times only a few a few times","and with a real set of natural language","you end up with relationships that look","like this that look like these plots in","terms of how the sparsity changes as you","add more documents","and more unique words to a corpus","so the sparsity goes up real fast as you","add more unique words and the memory","that is required to handle","this set of documents goes up very fast","so even if you use specialized data","structures meant to store sparse data like sparse","matrices you still end up growing the","memory required to handle these data","sets in a very non-linear way it still grows up very","fast so this means it can take a very","long time to train your model or even that","you outgrow the memory","available on your machine you have to go","to the cloud to an expensive","big memory situation this can be a real challenge","and this challenge","it is what has behind the motivating of vector","languages for models so","linguists have worked for a long time","on vector languages for models that can","reduce the number of dimensions representing text data","based on how people use language","so this quote here goes all the way back to 1957.","so the idea here is that we use","like the data is very sparse","but we don't use words","randomly it's not independent the words","are not used independently of each other","but rather there's relationships that","exist between how words are used together","and we can use those relationships to create","to transform our sparse high dimensional","space into a special dense","low dimensional space lower","we still has like 100","dimensions but much lower than the many thousands","hundreds tens hundreds of thousands","of space so the idea here we use","statistical modeling maybe just","word counts plus matrix factorization","maybe fancier math that involves neural","networks to take this really high","dimensional space and we create a new","lower dimensional lower dimensional","space that is special because the new","space is created based on vectors that","incorporate information","about which words are used together so","you shall know a word by the company it keeps","so you need a big data set of text to","create or learn these kinds of word","vectors or word embeddings","so this table that I'm showing right now","it's from a set of embeddings that","I created using a data set or a corpus of complaints","complaints to the United States consumer","financial protection bureau","so this is a government body in the","United States where people can complain and say","what is wrong with something to do with","a financial product like a credit card","a mortgage a student loan","something to do with like a financial","product they're like something went","wrong with my credit card something went","wrong with my mortgage that company is","not being fair so you come and you complain to it","so I took all those complaints and built","it's our high dimensional space and","build a low dimensional space","and we can look in that space and understand","what words are related to each other","in this space so in the new space","defined by the embeddings the word month","is closest to words like year months plural","monthly installments payment so these are words","that are that makes sense in the context of","financial products like credit cards or mortgages","in the new space defined by these embeddings","the word error is closest to the words","like mistake clerical like a clerical mistake","problem glitch or there was a glitch on my","mortgage statement so we see these kinds of","or miscommunication misunderstanding you","know like these are these are words that","are used in similar ways so","you don't have to create embeddings yourself","because it requires quite a lot of data","to make them so you can use word","embeddings that are pre-trained","i.e created by someone else","based on some huge corpus of data that","they have access to and you probably don't","so let's look at one of those data sets","let's look at this table shows the results for the same word error","but for the glove embeddings so the","glove embeddings are a set of","pre-trained embeddings that are created","based on a very large data set that's like","all of wikipedia all of the google news data set","just like huge swaths of the internet have been","fed in to create these embeddings","so some of the closest words here are similar","to those that are before but we no","longer have some of that domain specific","flavor like clerical discrepancy","and now we have like","miscommunication you know but and now we","have calculation and probability","which people were not talking about with","their financial product complaints","so this really highlights","how these how these work here before","we we created our own and we were able","to learn relationships that were","specific to this context and here we go","to a more general set that that","was learned somewhere else","so embeddings are trained or learned","from a large corpus of text data and the","characteristics of that corpus become","part of the embeddings","so machine learning in general you know","is exquisitely sensitive to whatever it","is that's in your training data and this","is never more obvious than when","dealing with text data","and perhaps with word embeddings is","just like one of these classic examples","where this is true it turns out that","this shows up in how any human","prejudice or bias in the corpus","becomes imprinted into the embeddings","so in fact when we look at some of these","most commonly available embeddings that","are out there bias is we we see that","african-american first names that are","more common for african americans in the","United States they're associated with","more unpleasant feelings than European","American first names in these embedding spaces","women's first names are more associated","with family and men's first names are more associated with career","and terms associated with women are more","associated with the arts and terms","associated with men are more associated","with science so it turns out actually bias is so","ingrained in word embeddings that the","word embeddings themselves can be used","to quantify change","in social attitudes over time","so word embeddings are","maybe an exaggerated or extreme example","but it turns out that all the feature","engineering decisions that we make when","it comes to text data have a significant","effect on our results","both in terms of the model performance that we see","and also in terms of how appropriate or","fair our models are","so given all that when it comes to","pre-processing your text data","creating these features that you need","you have a lot of options and quite a","bit of responsibility so my advice is","always start with simpler models that","you can understand quite deeply","be sure to adopt good statistical","practices as you train and tune your","models so you aren't fooled about model","performance improvements","when you try different approaches","and also to use model explainability","tools and frameworks so you can","understand any less straightforward","models that you try","so my co-workers and I have written","about all of these topics and how to","use them with Tidymodels if that's what","you like to use and we will continue to do so","with that i will say","thank you so very much and I want to be","sure to again","thank the organizers of the R user group","in Korea I want to thank my teammates on","the Tidymodels team at Rstudio as","well as my co-author EMIL HVITFELDT."],"header":"subtitle","minWidth":100,"align":"center","headerStyle":{"background":"#f7f7f8"}},{"accessor":"translatedText","name":"translatedText","type":"character","cell":["안녕하세요 제 이름은 줄리아 실지입니다. 저는 RStudio의 데이터 과학자이자 소프트웨어 엔지니어입니다.","그리고 한국의 R 사용자 그룹의 주최자에게 감사의 말을 전하고 싶습니다.","나를 가진 것에 대해 너무 많이. 오늘은 너에게 말을 걸","오늘 특별히 제작에 대해 이야기하게 되어 매우 기쁩니다.","몇 가지 이유로 텍스트 데이터에서 기계 학습을 위한 기능","우리가 텍스트 데이터를 취하기 위해 무엇을 하는지 더 잘 이해하기","그런 다음 적절하게 만들기 위해","기계 학습 알고리즘에 대한 입력으로 많은 이점이 있습니다.","둘 다 직접 준비하는 경우","모델을 훈련시키거나 일부 텍스트 분석 프로젝트를 시작하는 경우","또는 모델의 동작을 이해하려는 경우","데이터 과학자로서 우리가 하는 일과 같은 방식으로 상호 작용하고 있습니다.","또는 우리의 일상 생활에서 점점 더","따라서 감독 또는 감독되지 않은 텍스트에 대한 모델을 구축할 때","우리는 무언가로 시작합니다","이것은 몇 가지 예제 텍스트 데이터입니다.","내가 몇 번 사용하는 동안","일부 동물을 설명하는 이 이야기","일부 텍스트 데이터를 사용하고 있으므로","당신은 나에게 영어로 알고","스피커는 다음과 같이 친숙해 보입니다.","나는 인간을 사용하는 사람으로서","언어를 사용하여 이것을 보고 할 수 있습니다.","그것을 읽으십시오 나는 큰 소리로 말할 수 있고 나는 이해할 수 있습니다","나는 그것이 무엇을 의미하는지 해석할 수 있다","그래서 이런 종류의 데이터는 이런 종류의","자연어 데이터가 생성되고 있습니다.","모든 종류의 언어로 항상","모든 종류의 상황에서","금융 기술 분야 의료 분야에서 일하는지 여부","기본적으로 어떤 종류의 조직이든","일종의 텍스트 데이터가 생성되고 있습니다.","고객에 의한 고객에 의한 내부 이해관계자에 의한","설문 조사에 참여하는 사람들에 의해 기업 내부","비즈니스 프로세스를 통해 소셜 미디어를 통해","그리고 이 모든 자연어에서","정보가 숨어있다","더 나은 결정을 내리는 데 사용할 수 있는 텍스트 데이터","그러나 컴퓨터는 이것을 보고 수행하는 데 능숙하지 않습니다.","다음과 같이 표현되는 언어에 대한 수학","대신 언어는","극적으로 어떤 종류의","기계 판독 가능한 숫자 표현","그것이 내가 무엇인지 더 많이 보인다","준비를 위해 여기 화면에 표시","거의 모든 종류의 모델에 대해","그래서 나는 상당한 시간을 일했다.","사람들이 할 수 있는 소프트웨어","탐색적 데이터 분석, 시각화, 요약","깔끔한 텍스트 데이터와 같은 작업","행당 하나의 관찰이 있는 형식","그리고 나는 텍스트 분석을 위해 깔끔한 데이터 원칙을 사용하는 것을 좋아합니다.","특히 분석의 탐색 단계에서","모델을 만들 때가 되면","종종 기본 수학적 구현이 실제로 필요로 하는 것","일반적으로 다음과 같은 것입니다.","이 특정 표현에 대한 방법을 문서 용어 행렬이라고 합니다.","따라서 정확한 표현은 다음과 다를 수 있습니다.","내가 여기서 보여준 것","내가 여기에 있는 것은 우리가 무게를 다는 것입니다","이 행렬의 각 행이 문서이기 때문에","각 열은 단어입니다.","토큰과 숫자는 다음을 나타냅니다.","각 문서가 몇 번인지 계산합니다.","각 단어를 사용하여 다른 방식으로 가중치를 부여할 수 있습니다.","카운트 대신 TF-IDF 사용","또는 시퀀스 정보를 유지할 수 있습니다.","딥 러닝 모델 구축에 관심이 있지만 기본적으로","모든 종류의 텍스트 모델링","Naive Bayes와 같은 단순한 모델에서","텍스트에 잘 맞는 모델","워드 임베딩까지 정말","최첨단 작업이다.","오늘날 텍스트 데이터의 변환기처럼 발생","우리는 무겁게해야합니다","기능 엔지니어 및 프로세스 언어","어떤 종류의 표현에 그것을 얻을","머신 러닝 알고리즘에 적합","그래서 저는 R의 오픈 소스 프레임워크에서 작업합니다.","Tidymodels라고 하는 모델링 및 머신 러닝과","나는 오늘 Tidymodels 코드를 사용하는 것을 보여줄 것이다.","Tidymodels 프로젝트의 특정 목표 중 일부는","현실을 위한 일관되고 유연한 프레임워크","당신이 알고있는 세계 모델링 실무자 사람들","실제 데이터를 다루는","이제 막 시작하는 사람들","모델링 경험이 풍부한 사람과","목표는 이기종을 조화시키는 것입니다.","R 내에 존재하고 우수한 통계 관행을 장려하기 위한 인터페이스","제가 작업하는 것을 보여드릴 수 있어서 기쁩니다.","빌드 및 텍스트에 적용하는 방법","모델링하지만 많은 이야기를 할 것입니다.","오늘은 Tidymodels에만 국한되지 않습니다.","또는 R에게도. 나는 이것이 R 사용자라는 것을 알고 있습니다.","그룹이지만 우리가 이야기하고 집중할 것은","조금 더 개념적이고 기본적인","기계 학습을 위해 텍스트를 예측자로 변환하는 방법","아직 Tidymodels 및 Tidymodels에 대해 이야기하고 싶지 않다면 흥분됩니다.","이전에 사용한 메타 패키지입니다.","Tidyverse와 유사한 방식으로","메타 패키지이므로 입력한 적이 있다면","라이브러리 Tidyverse를 사용한 다음","시각화를 위한 ggplot2","데이터 조작을 위한 dplyr","Tidymodels도 비슷한 방식으로 작동합니다.","그 안에 다른 패키지가 있습니다","다양한 용도로 사용되는","따라서 전처리 또는 기능","엔지니어링은 보다 광범위한 모델 프로세스의 일부입니다.","프로세스가 실제로 시작된다는 것을 알고 있습니다.","탐색적 데이터 분석으로","어떤 종류의 모델을 결정하는 데 도움이","우리는 구축 할 것입니다 그리고 그것은 온다","내가 생각하는 완성","당신이 할 때 모델 평가","모델이 얼마나 잘 수행되었는지 측정","소프트웨어의 한 조각으로서의 Tidymodels는","각각의 패키지로 구성","우리의 샘플과 같은 특정 초점이 있습니다","데이터를 다시 샘플링하기 위한 것입니다.","부트스트랩 재샘플 생성","교차 검증은 모두 다른 리샘플링","사용할 수 있는 리샘플의 종류","모델 훈련 및 평가","tune 패키지는 하이퍼 매개변수용입니다.","이름에서 짐작할 수 있듯이 튜닝","이 패키지 중 하나는 기능용입니다.","데이터 전처리를 위한 엔지니어링","피쳐 엔지니어링은","레시피라고 하는","그래서 Tidymodels에서 우리는 이 아이디어를","데이터 전처리 및 기능","개념의 공학","선택할 수 있는 단계가 있는 전처리 레시피","성분 또는 변수","사용할 단계를 정의합니다.","그것은 당신의 조리법에 들어갑니다","그런 다음 훈련을 사용하여 준비합니다.","모든 데이터에 적용할 수 있습니다.","테스트 데이터 또는 예측 시점의 새 데이터와 같은 데이터 세트","그래서 우리가","모든 종류의 모델링에 사용","텍스트 데이터를 포함한 모양 및 크기","따라서 일부 기술과 접근 방식은","텍스트 데이터를 사전 처리하는 데 사용하는","다른 종류의 데이터와 동일합니다.","텍스트가 아닌 데이터처럼 사용할 수 있습니다.","숫자 데이터 범주형 데이터 일부","일부는 동일","그러나 당신이 알아야 할 몇 가지","좋은 일을 할 수 있는","이 과정에서 텍스트가 다릅니다.","그리고 무엇의 성격에 따라","언어 데이터는 다음과 같습니다.","그래서 저는 제 책을 썼습니다.","공동 저자 Emile Hvitfeldt 감독","텍스트 분석 및 R을 위한 머신 러닝","그리고 책의 첫 번째 1/3을 완전히","우리가 어떻게 변화하는지에 중점을 둡니다.","우리가 가지고 있는 자연어","모델링을 위한 피쳐로 텍스트 데이터","중간 섹션은 우리가 사용하는 방법에 관한 것입니다","이러한 기능은","더 간단하거나 더 전통적인 기계","정규화와 같은 학습 모델","회귀 또는 지원 벡터 기계 및","그런 다음 책의 마지막 3분의 1","우리가 딥 러닝을 사용하는 방법에 대해 이야기합니다.","텍스트 데이터가 있는 모델로 딥 러닝","모델은 여전히 이러한 종류의","자연어에서 변형","이러한 종류의 모델에 대한 입력으로 기능에","딥 러닝 모델은 종종","본질적으로 기능의 구조를 학습","텍스트에서","더 전통적이거나 더 단순한 기계","학습 모델은","그래서 이 책은 이제 완성되었고","이달부터 11월까지 가능","사람들은 첫 번째 종이를 얻습니다.","사본 및 또한 이 책은 유효합니다","smalltar.com에서 전체","따라서 텍스트를 처음 다루는 경우","이를 이해하는 데이터","기본적인 전처리 접근법","텍스트가 가능하도록 설정합니다.","효과적인 모델을 훈련하기 위해","당신이 정말로 텍스트에 경험이 있다면","데이터를 많이 다루었다면","이미 당신은 아마 우리처럼","기존 리소스 또는 문헌을 알고 있는지 여부","그것은 책이나 튜토리얼 또는 블로그 게시물입니다.","에 관해서는 매우 희박하다.","방법에 대한 자세한 사려 깊은 탐구","이러한 전처리 단계가 작동합니다.","이 기능에서 선택한 방법","엔지니어링 단계는 모델 출력에 영향을 미칩니다.","그래서 통과하자","이 중 몇 가지는 기본과 같은","피쳐 엔지니어링 접근 방식 및 방법","그들은 일하고 그들이하는 일부터 시작하겠습니다.","토큰화","따라서 일반적으로","자연에서 정보를 전송","언어 대 기계 학습 기능","정말 모든 종류의 텍스트 분석","탐색적 데이터 분석 포함","또는 모델을 구축합니다. 뭐든지 토큰화","토큰화에서 우리는 입력을 받습니다.","문자열 일부 문자 벡터 및 일부","토큰 유형의 종류","의미 있는 텍스트 단위","우리는 단어에 관심이 있습니다","그리고 우리는 입력 조각을","유형에 해당하는 토큰","우리는 관심이있다","가장 일반적으로 의미 있는 단위 또는","텍스트를 분할하려는 토큰 유형","의 단위로 단어는","그래서 이것은 보일 수 있습니다","직설적이거나 명백하지만","명확하게 정의하기 어렵다","많은 또는 대부분의 언어에 대한 단어는 무엇입니까","많은 언어가 공백을 사용하지 않습니다","단어 사이에 전혀","도전을 제시하는","토큰화를 위해","영어와 한국어와 같은 공백을 사용하십시오","종종 모호한 특정 예가 있습니다.","영어로 된 수축처럼 like don't","어느 것이어야","당신은 아마 더 정확하게 알고","두 단어를 방법으로 간주","입자는 한국어로 사용됩니다.","대명사와 부정 단어가 어떻게","같은 로맨스 언어로 작성","그들이 갇혀있는 이탈리아어와 프랑스어","함께 그리고 정말로 아마도 그들은 그래야만 할 것입니다","두 단어로 간주","당신이 무엇인지 파악한 후에","할 것이고 당신은 몇 가지 선택을 할 것입니다","텍스트를 토큰화하면 켜집니다.","사용할 수 있는 방법","탐색적 데이터 분석에서 또는","감독되지 않은 알고리즘 또는 기능","예측 모델링을 위해","우리는 여기서 그리고 이것들이 무엇인지에 대해 이야기하고 있습니다","결과가 여기에 표시되므로 이러한 결과는","에 대해 학습된 회귀 모델에서","미디어에 대한 설명","영국 테이트 컬렉션의 아트웍에서","우리가 무엇을 예측하고 있는지","예측은 몇 년","를 기반으로 만들어진 예술 작품이었습니다.","작품이 만들어진 매체","와 매체는 다음과 같이 설명됩니다.","약간의 텍스트","그래파이트를 사용하여 만든 작품을 여기서 볼 수 있습니다.","수채화와 조각이 더 가능성이 높았습니다.","더 일찍 생성될","그것이 올 가능성이 더 높더라도","사용하여 만든 오래된 예술 및 예술 작품에서","사진 화면 포인트 또는 미안 화면 인쇄","스크린 인쇄와 똥","반짝이는 가능성이 더 높습니다.","나중에 생성됨. 거기에 이것은 더 가능성이 있습니다","현대 미술에서 오는 현대 미술","이 텍스트를 토큰화하는 방법","당신은 우리가 자연적인 인간으로 시작했다는 것을 알고 있습니다","작성하는 사람들의 생성된 텍스트","언론에 대한 설명","이 예술 작품은","그리고 우리가 그 자연스러운 것을 토큰화한 방법","우리가 시작한 인간 생성 텍스트","우리가 배운 것에 큰 영향을 미칩니다","다른 방식으로 토큰화하면","우리는 달라졌을 것이다","결과는 다음과 같은 성능 측면에서","정확하게 예측할 수 있었습니다","연도를 예측하고","모델을 어떻게 해석하는지","우리가 그것으로부터 배울 수 있다는 것","이것은 토큰화의 한 종류입니다.","한 단어지만 우리도","대신 토큰화하는 다른 방법","한 단어로 분해하거나","n-gram으로 토큰화할 수 있는 유니그램","따라서 n-gram은 다음의 연속 시퀀스입니다.","주어진 텍스트 시퀀스의 N개 항목","그래서 이것은 같은 작은 조각을 보여줍니다","이 동물을 설명하는 텍스트의 비트","bi-gram 또는 n-gram으로 나눈다.","두 개의 토큰이 있으므로 단어가 어떻게","bi-gram이 겹쳐서 단어 collard","첫 번째 bigrams 모두에 나타납니다.","칼라 페 케리 페 케리도","그래서 n-그램으로 참조","토큰화는 텍스트를 따라 슬라이드하여","겹치는 토큰 세트 생성","이것은 같은 것에 대한 트라이 그램을 보여줍니다.","따라서 유니그램을 사용하는 한 단어는","더 빠르고 효율적이지만","단어 순서에 대한 정보 캡처","더 높은 값을 사용하고 있습니다.","둘, 셋 또는 그 이상 유지","단어에 대한 더 복잡한 정보","순서와 개념","여러 단어로 된 구문으로 설명된","그러나 토큰의 벡터 공간","극적으로 증가","이는 토큰 감소에 해당합니다.","우리는 각 토큰을 그다지 세지 않습니다","여러 번 그리고 그 의미에 따라","귀하의 특정 데이터 세트","좋은 결과를 얻지 못할 수도 있습니다","서로 다른 정도를 결합하여","n-grams를 사용하면 다음을 추출할 수 있습니다.","텍스트와 다른 수준의 세부 정보를 제공하므로 유니그램","어떤 개별 단어가 있는지 말할 수 있습니다","많이 사용되었다","그 단어 중 일부는 간과 될 수 있습니다","바이그램 또는 트라이그램 크라운의 경우","자주 다른 단어와 함께 나타나지 마십시오","이 플롯은 에 대한 모델 성능을 비교합니다.","예측하는 올가미 회귀 모델","대법원 판결의 해","미국 대법원의 의견","세 가지 다른 정도의 n-gram","우리가 여기서 하는 것은 우리가 취하고 있는 것입니다","United의 글의 텍스트","주 대법원과 우리는 예측하고 있습니다","언제 했어 그 문자가 언제였더라","우리가 몇 살인지 예측할 수 있도록 작성되었습니다","텍스트의 일부는 텍스트의 내용에서 가져온 것입니다.","그래서 토큰의 수를 유지","유니그램만 사용하여 1000에서 상수","이 코퍼스에 대해 가장 잘 수행됩니다.","미국 대법원의 의견","에 따라 항상 그런 것은 아닙니다.","데이터 세트를 사용하는 모델의 종류","그 자체로 우리는 최고의 성능을 볼 수 있습니다","유니 그램과 바이 그램을 결합하거나 아마도","다른 옵션","이 경우 통합하려는 경우","좀 더 복잡한 정보","우리는 바이그램과","아마도 우리가 필요로 할 트라이그램","수를 늘리다","모델의 토큰","따라서 결과를 볼 때 염두에 두십시오.","이와 같이 n-gram을 식별하는 것은","계산적으로 비싸다","이것은 특히 비교된다","모델과 같은 양의 개선","우리가 자주 보는 모델 성능에서","우리가 겸손한 것을 알고 있다면","빅그램을 추가하여 개선합니다.","얼마나 많은지 기억하는 것이 중요합니다.","우리가 비교하는 개선 사항","얼마나 걸립니까","바이그램을 식별한 다음 훈련","예를 들어 이 데이터 세트에 대한 모델","우리가 개최한 대법원의 의견","토큰의 수는 일정하므로","모델 훈련에는 동일한 수의 토큰이 있습니다.","바이그램과 유니그램을 함께 사용하면 훈련 시간이 두 배 더 걸립니다.","기능 엔지니어링을 수행하고","유니그램 뿐만 아니라 훈련 및 추가","트라이 그램에서도 거의 5가 걸립니다.","유니그램 교육만큼","혼자이므로 이것은 계산적으로","값비싼 일","다른 방향으로 가고","보다 작은 단위로 토큰화할 수 있습니다.","이와 같은 말을 라고 한다.","캐릭터 대상 포진 그래서 우리는 단어를","칼라 페커리","단어를 보는 대신","우리는 내려가서 하위 단어를 볼 수 있습니다","정보는 여러 가지가 있습니다","단어를 하위 단어로 나누는 방법","머신러닝에 적합한","그리고 종종 이러한 종류의 접근 방식","또는 알고리즘은 다음과 같은 이점이 있습니다.","알려지지 않은 단어나 새로운 단어를 인코딩할 수 있음","예측 시간에 그래서 언제 그것이 될 때","새로운 데이터에 대한 예측 시간","드문 일이 아닙니다.","그 당시의 새로운 어휘가 되고","훈련에서 그들을 보지 못했다면","우리가 무엇을 할 것인지 알고 있는 데이터","우리가 훈련할 때 그 새로운 단어에 대해","하위 단어 정보를 자주 사용하여","하위 단어를 본 경우 새 단어를 처리하십시오.","훈련 데이터 세트에서","이러한 종류의 하위 단어 정보를 사용하여","형태소를 통합하는 방법이다","당신이 알고 있는 우리의 모델에 시퀀스","이것의 다양한 종류에 적용되는 것입니다","영어뿐만 아니라 다양한 언어","따라서 이 결과는","데이터 세트가 있는 분류 모델","매우 짧은 텍스트는 이름일 뿐입니다.","미국의 우체국은 너무 짧습니다.","모델의 목표는","하와이에 위치한 우체국","태평양 한가운데 또는","그것은 미국의 나머지 부분에 위치","그래서 모델에 대한 기능을 만들었습니다.","이 우체국 이름의 하위 단어","그리고 우리는 이름이","h와 p로 시작하거나 에일을 포함하는","하위 단어는 하와이에 있을 가능성이 더 높습니다.","하위 단어 a 및 d 및 ri 및 ing은 다음과 같습니다.","우체국에서 올 확률이 더 높습니다.","하와이 밖에 있는","이것은 우리가","다르게 토큰화하고 우리는","새로운 것을 배우다 우리는 무언가를 배울 수 있다","Tidymodels에서 우리는 이 모든 것을 수집합니다.","토큰화에 대한 결정의 종류","그리고 다음과 같은 코드","그래서 우리는 무엇을 지정하는 조리법으로 시작합니다","우리가 사용할 변수 또는 성분","그런 다음 이러한 전처리를 정의합니다.","처음에는 이렇게 단계를 밟아도","그리고 틀림없이 당신은 간단하고 기본적인 것을 알고 있습니다","우리가 내리는 선택이 우리의","모델링 결과를 크게","다음 전처리 um 단계","내가 말하고 싶은 것은 스톱 워드","그래서 일단 텍스트를 분할하면","토큰으로 우리는 종종 그렇지 않다는 것을 발견합니다","모든 단어는 같은 양의","정보","모두 실제로 기계 학습 작업을 위해","거의 또는","의미 있는 정보가 없을 수도 있습니다","불용어","그래서 이것은 불용어 목록 중 하나입니다","사용할 수 있는","한국어이므로 일반적인 조언과 관행입니다.","그냥 제거하기만 하면 제거","많은 경우 이러한 불용어를 제거하십시오.","자연어 처리 작업","내가 여기서 보여주는 것","더 짧은 것 중 하나의 전체입니다.","사용되는 영어 불용어 목록","정말 광범위하게 나와 같은 단어라는 것을 알 수 있습니다.","me 내 대명사 접속사 및 of","그리고 이것은 매우 일반적인 단어입니다","매우 중요하다고 여겨지지 않는","그냥 제거하기로 결정","불용어는 종종 더 많이 관련되고","아마도 당신이 생각하는 것보다","많이 반영된 것보다","거기에 있는 자원의","그래서 거의 항상 현실 세계 NLP","실무자는 미리 만들어진 불용어 목록을 사용합니다.","그래서 이 플롯은","세 가지 공통에 대한 교차점 설정","영어로 된 불용어 목록","화난 음모라고 불리는 것에서","따라서 세 개의 목록을","눈덩이 목록 스마트 및 ISO 목록","그래서 당신은 길이를 볼 수 있습니다","목록은 길이로 표시됩니다.","막대 그리고 우리는 본다","단어가 공통되는 교차점","이 목록에서 수직으로","막대가 있으므로 목록의 길이가 상당히 다릅니다.","또한 모든 항목이","같은 단어 집합","기억해야 할 중요한 것","불용어 사전은","그들은 일부에서 생성되지 않습니다","중립적인 완벽한 설정이지만 대신","그들은 상황에 따라 다릅니다","그들은 이 두 가지 모두에 편향될 수 있습니다.","그것들은 목록이기 때문에 사실입니다.","대규모 언어 데이터 세트에서 생성","사용된 데이터의 특성을 반영합니다.","그들의 창조는 이것이다.","영어로 된 10개의 단어","언어 스마트 사전에는 없지만","영어 눈덩이 사전","그래서 그것들이 모두 수축이라는 것을 알아차리세요.","하지만 그건 눈덩이 때문이 아니야","교환은 수축을 포함하지 않습니다","많은 사람들이","그녀가 이 목록에 있다는 것은","그 목록에는 그가 있지만 그것은 없다","그녀의 목록","그래서 이것은 그 예입니다","내가 언급한 편향은 다음과 같은 이유로 발생합니다.","이 목록은 대용량 데이터에서 생성됩니다.","텍스트 사전 제작자가 가장 많이 보는 세트","어떤 큰 말뭉치에서 자주 사용되는 단어","그들이 끊는 언어","그런 다음 무엇을 포함할지 또는","당신이 알고 제외","그들이 작성한 목록을 기반으로","가지고 있고 당신은 여기에서 끝납니다. 왜냐하면","많은 대규모 언어 데이터 세트에서","당신은 더 많은 표현을 가지고 있습니다","당신이 끝내는 남자들","이 같은 상황에서 stopword","목록에는 그가 있지만 그녀는 없습니다","많은 결정을 내릴 때","언어로 모델링 또는 분석","실무자로서 우리는 결정해야합니다","특정 도메인에 적합한 것","이것이 사실일 때도 사실로 밝혀졌습니다","불용어 목록을 선택하기 위해 온다","그래서 Tidymodels에서 우리는","불용어 제거와 같은 전처리 단계","추가 단계를 추가하여","조리법 그래서 먼저 우리는 무엇을 지정","우리가 사용할 변수를 토큰화했습니다.","텍스트 및 이제 우리는 제거하고 있습니다","기본값만 사용하는 불용어","우리는 어떤 단계도 통과하지 않기 때문에","우리가 할 수 있는 다른 주장","기본이 아닌 단계 또는 사용자 정의를 사용하십시오.","그것이 우리 도메인에 가장 적절한지 나열하십시오.","이 플롯은 모델 성능을 비교합니다.","년을 예측하기 위해","동일한 데이터 세트","3명의 대법원 의견","다른 길이의 다른 불용어 사전","따라서 눈덩이 사전에는","가장 적은 수의 단어와 이것에서","최상의 성능을 내는 경우","따라서 더 적은 수의 불용어를 제거하면","여기 최고의 성능 그래서 이것은","특정 결과를 일반화할 수 없음","사실을 제외한 모든 데이터 세트와 컨텍스트","다른 세트를 제거하는","불용어는 눈에 띄게 다를 수 있습니다","모델에 미치는 영향은 상당히","양도 가능하므로 알 수 있는 유일한 방법","가장 좋은 것은 시도하는 것입니다","몇 가지 옵션과 기계 참조","일반적으로 학습.","이것은 우리와 같은 경험적 분야입니다.","우리는 종종 사전에 이유가 없다는 것을 모릅니다","가장 좋은 일이 무엇인지 알고","그래서 일반적으로 우리는","다른 옵션을 시도하여","최고가 되십시오. 그 다음 세 번째","내가 말하고 싶은 전처리 단계","텍스트가 형태소 분석 중이기 때문에","그래서 우리가 텍스트를 자주 다룰 때","문서에는 다른 버전의","하나의 기본 단어는 종종 어간이라고 합니다.","영어로 예를 들면","차이에 관심이 없다면","복수와 동물 사이","특이하고 우리는 둘 다 함께 치료하고 싶습니다","그 아이디어는 스테밍의 핵심입니다","그래서 아무도 없다","텍스트를 줄기로 만드는 올바른 방법 또는 올바른 방법","이 플롯은 세 가지 접근 방식을 보여줍니다.","영어로 어간","헤이부터 시작하여 마지막 s를 제거합시다.","복수에 대한 더 복잡한 규칙","중간에 복수 엔딩 처리","하나는 s 스테머라고합니다","그것은 일종의 규칙의 집합과 같습니다.","그리고 마지막은 가장 잘 알려진","아마도 가장 잘 알려진","영어로 형태소 분석 구현","포터 알고리즘이라고 함","여기에서 Porter가","다른 둘과 가장 다른","여기 데이터에서 상위 20개 단어에","내가 가지고 있는 동물 설명 세트","우리는 종이라는 단어를 사용하여","다른 동물 포식자 취급","이런 종류의 단어 모음","삶을 살아라 치료받은 삶을 살아라","따라서 실무자들은 일반적으로","텍스트 데이터의 형태소 분석에 관심이 있기 때문에","우리가 믿는 토큰을 함께 버킷","하는 방식으로 함께 속해있다.","우리는 인간 사용자로서","우리가 이와 같은 접근 방식을 사용할 수 있도록","단계별로 꽤","이를 기반으로 하는 규칙을 일반적으로","형태소 분석 또는 그것은 상당히","본질적으로 알고리즘 같은","먼저 이렇게 하고 다음에는 이렇게 하세요","또는 표제어를 사용할 수 있습니다","일반적으로 큰 사전을 기반으로 합니다.","단어 및 그것은 다음과 같이 통합됩니다.","어떤 단어가 함께 속해 있는지에 대한 언어적 이해","따라서 대부분의 기존 접근 방식은","이런 종류의 한국어 작업은","다음을 기반으로 하는 제한된 표제어입니다.","사전 및 훈련된 사전","대규모 언어 데이터 세트 사용","도움이 될 것 같습니다.","당신이 이것에 대해 들었을 때 당신은","오 예 좋은 소리, 똑똑한 소리","특히 텍스트 데이터의 경우 일반적으로","많은 토큰이 있는 기능에 압도됨","이것은 일반적으로 상황","텍스트 데이터를 다룰 때","여기에 동물 설명 데이터가 있습니다.","그리고 나는 그것의 행렬 표현을 만들었습니다.","우리가 일반적으로 일부에서 사용하는 것처럼","기계 학습 알고리즘","그리고 얼마나 많은 기능이 있는지 보세요","16,000개 거의 17,000개 기능","그것은 기능의 수입니다","모델에 들어갈 것입니다","희소성을 봐","98% 희소, 매우 높음","희소 데이터이므로 이것은 희소성입니다.","기계에 들어갈 데이터","우리를 구축하기 위한 학습 알고리즘","지도 머신 러닝 모델","우리가 단어를 중단하면","여기에서 형태소 분석에 대한 접근 방식을 사용하면","우리는 다음과 같이 단어 기능의 수를 줄입니다.","수천 개의 희소성이 불행히도","많이 변경하지만 우리는 숫자를 줄였습니다","기능을 버킷팅하여","우리의 형태소 분석 알고리즘이","당신이 알 수 있도록 함께 속해","상식이 말한다","단어 수 줄이기 기능","너무 극적으로 수행 할 것입니다","기계 학습 모델의 성능 향상","그러나 그것은 우리가","중요한 정보를 잃지 않았습니다","형태소 분석을 통해 형태소 분석 또는","표제어는 종종 일부에서 매우 도움이 될 수 있습니다.","컨텍스트가 있지만 이러한 작업에 사용되는 일반적인 알고리즘은","다소 공격적이다","그리고 그들은 감도를 선호하도록 만들어졌습니다.","또는 회상 또는 참 양성률 및","이것은 비용으로","특이성 또는 정밀도 또는 진실","감독되는 기계에서 음수 비율","학습 컨텍스트 이것이 하는 일","모델의 긍정적인 예측에 영향을 줍니다.","정밀도 또는 그 능력에 가치를 둡니다.","참음성 라벨을 잘못 붙이지 않기 위해","내가 옳았으면 좋겠어","그래서 당신은 이것을 더 구체적으로 만들기 위해 알고 있습니다.","형태소 분석은 모델의 능력을 향상시킬 수 있습니다.","긍정적인 사례를 찾기 위해","동물에 대한 설명은","특정 식이요법과 관련된 경우","그것이 우리가 모델링하는 것입니다.","텍스트가 생략되었습니다.","결과 모델은 기능을 잃습니다.","부정적인 예에 레이블을 지정","에 관한 것이 아닌 설명을 말하다","우리가 찾던 바로 그 다이어트","이것은 다음과 같은 경우에 진정한 도전이 될 수 있습니다.","텍스트 데이터 종류의 훈련 모델","그 균형이 거기에 있다는 것을 발견하기 때문에","종종 우리는 가지고 있지 않습니다","우리가 이것을 변경할 수 있는 다이얼","이러한 형태소 분석 알고리즘에 대한 형태소 분석","아주 기본적인 전처리만 해도","내가 여기에 표시하는 것과 같은 텍스트의 경우","이 기능 엔지니어링 레시피는","계산적으로 비싸다","그리고 실무자가 선택하는","제거할지 여부를 결정합니다.","불용어 또는 줄기 텍스트는","머신 러닝 모델에 미치는 영향","모든 종류의","더 간단한 모델","보다 전통적인 기계 학습 모델","또는 딥 러닝 모델이 의미하는 바는","가격 우선 순위 우리가","실무자가 학습을 좋아하는 것처럼","기능에 대해 가르치고 쓰기","텍스트를 위한 엔지니어링 단계","더 나은 견고성에 기여합니다.","우리 분야의 통계 실습","나는 텍스트의 희소성 앞에서 언급했다.","데이터로 돌아가고 싶습니다.","텍스트 데이터 중 하나이기 때문에","언어가 작동하는 방식 때문에 특성 정의","우리는 몇 가지 단어를 많이 사용합니다","그리고 나서 많은 단어들은 단지","몇 번 몇 번만","실제 자연어 세트로","당신은 보이는 관계로 끝납니다","이 플롯처럼 보이는 이와 같이","희소성이 어떻게 변하는지에 대한 용어","더 많은 문서 추가","그리고 말뭉치에 대한 더 독특한 단어","그래서 희소성은 당신만큼 정말 빨리 올라갑니다.","더 독특한 단어와 메모리를 추가","처리하는 데 필요한","이 문서 세트는 매우 빠르게 올라갑니다.","따라서 전문 데이터를 사용하더라도","희소 데이터와 같은 희소 데이터를 저장하기 위한 구조","당신은 여전히 성장하는 매트릭스","이러한 데이터를 처리하는 데 필요한 메모리","매우 비선형적인 방식으로 설정되며 여전히 매우 자랍니다.","매우 빠르므로 매우 오래 걸릴 수 있습니다.","모델을 훈련하는 데 오랜 시간이 걸리거나","당신은 기억을 초과","당신이 가야 할 당신의 기계에서 사용 가능","값비싼 클라우드로","큰 메모리 상황은 이것이 진짜 도전이 될 수 있습니다.","그리고 이 도전","벡터의 동기 부여 뒤에 있는 것입니다.","모델을 위한 언어","언어학자들은 오랫동안 일해 왔다.","할 수 있는 모델을 위한 벡터 언어","텍스트 데이터를 나타내는 차원 수 줄이기","사람들이 언어를 사용하는 방식에 따라","그래서 이 인용문은 1957년으로 거슬러 올라갑니다.","그래서 여기에서 우리가 사용하는 아이디어는","데이터가 매우 희박한 것처럼","하지만 우리는 단어를 사용하지 않습니다","무작위로 독립적이지 않은 단어","서로 독립적으로 사용되지 않음","그러나 오히려 관계가 있습니다.","단어가 함께 사용되는 방식 사이에 존재","이러한 관계를 사용하여","희소한 고차원을 변환하기 위해","특별한 밀도로 공간","낮은 차원 공간","우리는 여전히 100과 같습니다","치수이지만 수천 개보다 훨씬 낮습니다.","수십만 수십만","여기에서 우리가 사용하는 아이디어는","통계 모델링은 아마도","단어 수와 행렬 분해","아마도 신경과 관련된 더 멋진 수학","네트워크를 통해","차원 공간과 우리는 새로운","저차원 저차원","새로워진 특별한 공간","공간은 벡터를 기반으로 생성됩니다.","정보를 통합하다","어떤 단어가 함께 사용되는지에 대해","당신은 그것이 유지하는 회사에 의해 단어를 알게 될 것입니다","따라서 큰 데이터 세트의 텍스트가 필요합니다.","이런 종류의 단어를 만들거나 배우다","벡터 또는 단어 임베딩","그래서 지금 보여드리는 이 테이블은","임베딩 세트에서 가져온 것입니다.","데이터 세트 또는 불만 모음을 사용하여 만들었습니다.","미국 소비자에 대한 불만","금융 보호국","그래서 이것은 정부 기관입니다.","불평하고 말할 수 있는 미국","뭔 상관이야","신용카드와 같은 금융상품","모기지 학자금 대출","금융과 같은 일","그들은 뭔가 갔다와 같은 제품","내 신용 카드에 문제가 생겼습니다.","회사가 가지고 있는 내 모기지와 관련하여 잘못되었습니다.","불공평해서 와서 불평한다","그래서 나는 모든 불만을 받아들이고","그것은 우리의 고차원 공간이고","저차원 공간을 구축","우리는 그 공간을 보고 이해할 수 있습니다.","어떤 단어가 서로 관련이 있는지","이 공간에서 그래서 새로운 공간에서","월이라는 단어 임베딩으로 정의","년 월 복수와 같은 단어에 가장 가깝습니다.","월 할부금 그래서 이것들은 단어입니다","의 맥락에서 의미가 있습니다.","신용 카드 또는 모기지와 같은 금융 상품","이러한 임베딩에 의해 정의된 새로운 공간에서","단어 오류는 단어에 가장 가깝습니다.","사무적 실수처럼 사무적 실수처럼","문제 결함 또는 결함이 내","모기지 명세서","또는 당신을 오해하는 잘못된 의사 소통","이것들은 다음과 같은 단어라는 것을 알고 있습니다.","비슷한 방식으로 사용되므로","임베딩을 직접 만들 필요가 없습니다.","많은 데이터가 필요하기 때문에","단어를 사용할 수 있도록","사전 훈련된 임베딩","즉, 다른 사람이 만든","엄청난 양의 데이터를 기반으로","그들은 접근할 수 있고 당신은 아마 그렇지 않을 것입니다","데이터 세트 중 하나를 살펴보겠습니다.","동일한 단어 오류에 대한 결과를 보여 주는 이 표를 살펴보겠습니다.","하지만 장갑 임베딩의 경우","장갑 임베딩은","생성된 사전 훈련된 임베딩","다음과 같은 매우 큰 데이터 세트를 기반으로","모든 wikipedia 모든 Google 뉴스 데이터 세트","인터넷의 거대한 범위가 그랬던 것처럼","이러한 임베딩을 생성하기 위해","그래서 여기에서 가장 가까운 단어 중 일부는 비슷합니다.","이전에 있지만 우리는 그렇지 않은 사람들에게","더 이상 해당 도메인의 일부가 특정","성직자의 불일치와 같은 맛","이제 우리는 좋아합니다","당신은 알고 있지만 지금 우리는 잘못된 의사 소통","계산과 확률이 있다","사람들이 이야기하지 않은 것","그들의 금융 상품 불만","그래서 이것은 정말 하이라이트","이전에 여기에서 어떻게 작동하는지","우리는 우리 자신의 것을 만들었고 우리는 할 수 있었습니다","있었던 관계를 배우기 위해","이 맥락에 따라 여기에 우리가 간다","더 일반적인 집합으로","다른 곳에서 배웠다","그래서 임베딩은 훈련되거나 학습됩니다.","방대한 양의 텍스트 데이터와","그 말뭉치의 특징은","임베딩의 일부","따라서 일반적으로 기계 학습은","무엇이든 간에 절묘하게 민감하다.","그것은 당신의 훈련 데이터에 있고 이것은","언제보다 더 명확하지 않습니다","텍스트 데이터 다루기","아마도 단어 임베딩은","이 고전적인 예 중 하나처럼","이것이 사실인 경우","이것은 어떤 인간이 어떻게","말뭉치의 편견 또는 편견","임베딩에 각인됩니다.","그래서 사실 우리가 이것들 중 일부를 볼 때","가장 일반적으로 사용 가능한 임베딩","거기에 편견이 있다는 것은 우리가 그것을 볼 수 있다는 것입니다","아프리카계 미국인 이름","아프리카계 미국인에게 더 흔합니다.","그들이 연결된 미국","유럽보다 더 불쾌한 감정","이러한 임베딩 공간의 미국 이름","여성의 이름이 더 관련이 있습니다.","가족 및 남성의 이름은 경력과 더 관련이 있습니다.","여성과 관련된 용어가 더 많습니다.","예술 및 용어와 관련된","남성과 관련된 것이 더 관련이 있습니다.","과학을 사용하면 실제로 편향이 있다는 것이 밝혀졌습니다.","워드 임베딩에 내재된","단어 임베딩 자체를 사용할 수 있습니다.","변화를 정량화하기 위해","시간이 지남에 따라 사회적 태도","그래서 워드 임베딩은","과장되거나 극단적인 예일 수 있습니다.","하지만 모든 기능이","우리가 내리는 엔지니어링 결정","그것은 텍스트 데이터에 오면 중요한","결과에 미치는 영향","우리가 보는 모델 성능 측면에서","또한 얼마나 적절하거나","공정한 우리 모델은","그래서 그것이 올 때 모든 것을 주었다.","텍스트 데이터 전처리","필요한 기능 생성","당신은 많은 옵션과 꽤","약간의 책임이 있으므로 내 조언은","항상 더 간단한 모델로 시작하여","당신은 꽤 깊이 이해할 수 있습니다","좋은 통계를 채택하십시오","훈련하고 조정할 때의 연습","모델에 속지 않도록 모델","성능 향상","다른 접근을 시도할 때","또한 모델 설명 가능성을 사용하기 위해","도구와 프레임워크를 통해","덜 간단하게 이해","당신이 시도하는 모델","그래서 내 동료와 나는 썼다.","이 모든 주제와 방법에 대해","그렇다면 Tidymodels와 함께 사용하십시오.","당신은 사용하기를 좋아하고 우리는 계속 그렇게 할 것입니다","그것으로 나는 말할 것이다","너무너무 감사하고 하고싶습니다","꼭 다시","R 사용자 그룹의 주최자에게 감사","한국에서는 팀원들에게 감사 인사를 전하고 싶습니다.","Rstudio의 Tidymodels 팀은 다음과 같이","제 공동 저자인 EMIL HVITFELDT도 마찬가지입니다."],"header":"translatedText","minWidth":100,"align":"center","headerStyle":{"background":"#f7f7f8"}}],"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"highlight":true,"bordered":true,"dataKey":"9fc5263285fc6a8a79a3bcf303361d9d","key":"9fc5263285fc6a8a79a3bcf303361d9d"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
</div>
<div id="translation-export" class="section level2" number="5.3">
<h2 number="5.3"><span class="header-section-number">5.3</span> <code>.srt</code> 번역자막 내보내기</h2>
<p>마지막으로 이렇게 기계 번역을 완료한 <code>.srt</code> 파일을 내보내서 기계번역을 입혀보도록 하자.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true"></a>keynote_kor_srt &lt;-<span class="st"> </span>translation_obj <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">select</span>(translatedText) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">bind_cols</span>(keynote_gl_tbl <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>translation)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true"></a><span class="st">  </span><span class="kw">select</span>(n, start, end, <span class="dt">subtitle =</span> translatedText)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true"></a><span class="co"># 기대했던 결과가 아님!!! </span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true"></a>keynote_kor_srt <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true"></a><span class="st">  </span>srt<span class="op">::</span><span class="kw">write_srt</span>(<span class="st">&quot;data/julia_silge/15_silge_kor_google.srt&quot;</span>)</span></code></pre></div>
</div>
</div>

&nbsp;
<hr />
<p style="text-align: center;">데이터 과학자 <a href="https://github.com/statkclee/">이광춘</a> 저작</p>
<p style="text-align: center;">
    <span style="color: #808080;"><em>kwangchun.lee.7@gmail.com</em></span>
</p>
<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
<!-- Add font awesome icons -->
<p style="text-align: center;">
    <a href="https://education.rstudio.com/trainers/people/lee+kwangchun/" class="fa fa-registered"></a>
    <a href="https://www.facebook.com/groups/tidyverse/" class="fa fa-facebook"></a>
    <a href="https://www.linkedin.com/in/kwangchunlee/" class="fa fa-linkedin"></a>
    <a href="https://github.com/statkclee/" class="fa fa-github"></a>
</p>
&nbsp;


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
