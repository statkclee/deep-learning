1
00:00:01,120 --> 00:00:08,639
안녕하세요제이름은줄리아실기입니다. 저는데이터과학자이자소프트웨어엔지니어로 RStudio에서근무하고있습니다.

2
00:00:06,080 --> 00:00:18,080
우선 이런 행사에 저를 초대해주신 한국 R 사용자회 주최측에  정말 감사드립니다.

3
00:00:15,519 --> 00:00:29,359
저는 오늘 텍스트 데이터를 사용해 머신러닝을 위한 입력변수(feature)들을 만드는 것에 대해 이야기 하고자 합니다.

4
00:00:27,199 --> 00:00:50,719
기계학습 모형을 학습시킬 준비를 하거나 텍스트 분석 프로젝트를 시작하는 경우 모두, 텍스트 데이터를 기계학습 알고리즘의 입력 변수로 변환하는 과정을 잘 이해하는 것은 많은 도움이 됩니다.

5
00:00:48,399 --> 00:01:06,640
또는, 데이터 과학자의 업무 상에서나 아니면 일상 생활에서도 점점 더 많이 접하게 되는 기계학습 모형들이 어떻게 작동하는 지를 이해하는 데에 많은 도움이 됩니다

6
00:01:03,359 --> 00:01:15,759
우리가 텍스트를 위한 머신러닝 모델을 만드는 일은, 그것이 지도 학습 모델이든 비지도 학습 모델이든, 지금 슬라이드에 보이는 것과 같은 텍스트 데이터에서 시작합니다.

7
00:01:11,600 --> 00:01:24,560
이 예문은 어떤 동물을 설명하고 있는데요,  이 강연에서 몇 번 더 나오는 예시입니다.

8
00:01:21,040 --> 00:01:34,400
영어 사용자인 저에게는 상당히 친숙한 형태의 글인데요,

9
00:01:30,960 --> 00:01:43,920
저는 인간의 언어를 사용하는 사람이기 때문에 이 글을 보고 읽을 수 있고 큰 소리로 말할 수 있으며,

10
00:01:41,280 --> 00:01:45,759
이 글이 어떤 의미인지 해석하고 이해할 수 있습니다.

11
00:01:43,920 --> 00:01:59,840
이러한 자연어 데이터는 모든 언어와 모든 맥락에서 매 시간 생성되고 있는데요,

12
00:01:56,880 --> 00:02:26,720
헬스케어, 테크, 금융권 등 분야와 상관없이 모든 기관에서 이러한 텍스트 데이터들은 고객에 의해, 파트너 사에 의해, 내부 이해관계자들에 의해, 설문조사에 참여한 사람들에 의해, 소셜미디어에 의해 비즈니스 과정에 의해 생성되고 있습니다.

13
00:02:24,319 --> 00:02:35,360
그리고 이렇게 생성되는 모든 텍스트들에는 우리가 더 나은 결정을 내리는 데 사용할 수 있는 정보들이 숨겨져 있죠.

14
00:02:33,519 --> 00:02:47,440
그러나 컴퓨터는 이렇게 텍스트로 표현된  글을 이해하거나 분석하지 못합니다.

15
00:02:45,440 --> 00:02:55,440
대부분의 모델들에서 이 글을 분석하기 위해서는

16
00:02:53,680 --> 00:03:01,840
화면에 보이는 것처럼  기계가 이해할 수 있는  숫자들로 변환해주어야 합니다.

17
00:02:59,760 --> 00:03:22,159
저는 사람들이 하나의 행에 하나의 관찰 데이터가 있는  깔끔한 형식(tidy format)의 텍스트 데이터로 탐색적 데이터 분석, 시각화, 요약 작업을 수행할 수 있도록 해주는 소프트웨어를 만드는 일에 많은 시간을 투자했습니다.

18
00:03:19,040 --> 00:03:34,879
저는 깔끔한 데이터 원칙(tidy data principles)을 적용해 텍스트 분석을 하는 것을 좋아합니다. 특히 탐색적 데이터 분석 단계에서 말이죠.

19
00:03:32,640 --> 00:03:52,319
모델을 만들 때 모델 내의 수학을 구현하기 위해서는 지금 보고 계시는 것과 같은 문서 단어 행렬 (document term matrix)이 필요합니다.

20
00:03:49,760 --> 00:03:55,760
구체적인 행렬은 제가 보여드리는 것과 다를 수는 있습니다.

21
00:03:53,840 --> 00:04:04,720
지금 보여드리고 있는 것은 갯수로 가중치를 부여하는 방식을 사용한 것인데, 각각의 행은 각각의 문서를 나타내고,

22
00:04:02,640 --> 00:04:07,439
각각의 열은 단어(토큰 token)를 나타내며,

23
00:04:04,720 --> 00:04:19,359
각 숫자는 각 단어가 각 문서에서 몇 번 쓰였는지를 나타내고 있는데요,갯수를 세는 대신 TF-IDF 를 이용하는 등 다른 방식으로 가중치를 부여하거나,

24
00:04:16,720 --> 00:04:41,120
또 딥러닝 모델을 구축하는 데 관심이 있다면 시퀀스 정보를 유지할 수도 있을 것입니다. 어쨌든 가장 중요한 것은, 우리가 사용할 모델이 아주 간단하고 텍스트 데이터와 단어 임베딩에서 잘 동작하는 나이브 베이즈 모델이든, 현재 가장 훌륭해보이는 트랜스포머 기반 모델이든 상관없이

25
00:04:39,520 --> 00:04:53,680
우리는 머신러닝 알고리즘에 사용될 수 있도록 자연어를 적절하게 바꿔주는 일과 변수 가공을 진행하는 것에 심혈을 기울여야한다는 것입니다.

26
00:04:50,960 --> 00:05:04,800
저는 모델링과 머신러닝을 위해 만들어진 R 오픈소스 프레임워크인 Tidymodels를 작업하고 있으며 오늘 발표에서 보여드릴 예제들은 이 Tidymodels코드를 사용합니다.

27
00:05:02,320 --> 00:05:21,360

Tidymodels 프로젝트의 목표 중 하나는 모델링을 지금 막 시작하는 사람들부터 매우 경험이 있는 사람들까지 실제로 데이터를 가지고 일하고 있는 실무자들에게 일관되고 유연한 프레임워크를 제공하는 것입니다.

28
00:05:19,199 --> 00:05:37,360
R에 있는 서로 다른 인터페이스들을 조화시켜 더 좋은 통계 작업들을 장려하는 것이죠.

29
00:05:35,360 --> 00:05:52,800
제가 작업하고 구축해 온 것들과 이를 어떻게 텍스트 모델링에 적용할 수 있는지 보여드릴 수 있어서 기쁩니다. 그렇지만 오늘 저의 발표는 Tidymodels에만, 혹은 R에만 국한되는 이야기는 아닙니다.

30
00:05:50,880 --> 00:06:00,479
물론 여러분이 R 사용자 그룹이라는 것을 알고 있지만, 오늘 제가 이야기 하고자 하는 것은 조금 더 개념적이고 기본적인 것입니다.

31
00:05:57,440 --> 00:06:06,800
오늘의 주제는 바로 '텍스트를 어떻게 머신러닝을 위한 독립변수로 만들 것인가' 입니다.

32
00:06:04,560 --> 00:06:19,720
그래도 Tidymodels 에 대해 이야기할 수 있어서 기쁜데요, Tidyverse가 메타 패키지 인 것처럼 Tidymodels 역시 하나의 메타 패키지 입니다.

33
00:06:16,319 --> 00:06:33,039
library(tidyverse)를 입력한 뒤, ggplot2를 이용해 시각화를 하거나 dplyr을 이용해 데이터 가공을 하는 방식과 비슷한 방식으로 Tidymodels를 사용할 수 있습니다.

34
00:06:30,560 --> 00:06:39,840
Tidymodels 안에는 각 목적에 맞게 사용될 수 있는 여러 패키지가 들어있습니다.

35
00:06:37,280 --> 00:06:48,000
전처리 또는 변수 가공은 보다 광범위한 모델을 만드는 과정의 일부입니다.

36
00:06:45,280 --> 00:06:56,240
모델을 만드는 것은 실제로 어떤 종류의 모델을 구축할 것인지 결정하는 것을 도와주는 탐색적 데이터 분석 (EDA)에서 시작하며

37
00:06:54,080 --> 00:07:03,120
모델을 만들고 1차 모델 개발 완료가 됩니다.

38
00:06:59,280 --> 00:07:09,280
만들어진 모델의 성능을 측정하는 단계에서 비로소 완료된다고 할 수 있겠습니다.

39
00:07:06,479 --> 00:07:38,560
소프트웨어로써의 Tidymodels는 각각의 목적을 가진 R 패키지들로 이루어져 있습니다. rsample의 경우에는 재표본추출(resample)을 위한 것입니다. 부트스트랩을 위한 재표본추출, 교차 검증을 위한 재표본추출과 같이 모델을 학습시키고 평가하기 위해 사용할 수 있는 모든 종류의 재표본추출을 위해 사용됩니다. tune 패키지의 경우, 이름에서 예상할 수 있듯, 하이퍼 파라미터 튜닝을 위해 사용됩니다.

40
00:07:34,800 --> 00:07:52,720
Tidymodels에 속한 패키지 중 하나는 데이터 전처리 및 변수 가공을 위한 패키지인데, 그 이름은 recipe입니다.

41
00:07:49,360 --> 00:08:25,759
Tidymodels에서 우리는  데이터 전처리와 변수 가공을 마치 일련의 단계를 가진 전처리 '레시피' 안에서 생각해보았습니다.  여러분은 요리에 들어갈 변수라는 재료를 고르고, 레시피에 들어갈 요리 과정들을 명확하게 정의한 뒤, 학습 데이터를 이용해 요리를 준비하고, 이를 테스트 데이터나 예측 시의 새 관측 데이터에 적용합니다.

42
00:08:23,039 --> 00:08:34,560
이 때 모델링에 사용할 들어갈 변수나 재료는 텍스트 데이터를 포함해 다양한 형태와 사이즈를 지닌 데이터가 될 수 있을 것입니다.

43
00:08:32,000 --> 00:09:05,279
어떤 테크닉과 접근들에서는 텍스트 데이터의 전처리가 텍스트가 아닌 데이터, 숫자형 데이터, 범주형 데이터 등 다른 데이터들의 전처리와 똑같을 수도 있습니다. 하지만 언어 데이터의 특성상 모델링에서 좋은 작업을 수행하기 위해서는 언어만을 위한 전처리 방식이 필요합니다.

44
00:09:03,680 --> 00:09:14,720
저는 Emil Hvitfeldt와 함께 "supervised machine learning for text analysis in r" 이라는 책을 저술했는데요,

45
00:09:11,360 --> 00:09:25,920
이 책의 첫 삼분의 일은 텍스트 데이터 속 자연어를 어떻게 모델링을 위한 변수로 변환할 것인가에 관한 내용입니다.

46
00:09:23,600 --> 00:09:40,240
중간 부분은 이러한 변수들을 정규화 회귀모형(regularized regression)이나 서포트 벡터 머신(SVM)처럼 간단하거나 비교적 전통적인 머신러닝 모델에서 어떻게 사용할 것인가에 관한 내용입니다.

47
00:09:37,360 --> 00:10:15,279
이 책의 마지막 삼분의 일은 딥러닝 모델에서 텍스트 데이터를 사용하는 방법을 이야기합니다.  딥러닝 모델들도 앞서 말씀 드렸던 모델과 마찬가지로 자연어를 변수로 변환하여 입력으로 넣어주어야 합니다. 하지만 딥러닝 모델들은 앞서 말씀드린 간단하거나 비교적 예전 모델들은 하지 못했던 텍스트 피처의 구조를 학습할 수 있습니다.

48
00:10:12,800 --> 00:10:20,560
이 책은 지금 완성되었고 11월을 시작하는 현재 시점에 시판되고 있습니다.

49
00:10:18,480 --> 00:10:30,720
사람들은 첫 인쇄본을 받고 있는데요, URL은 smltar.com입니다.

50
00:10:28,560 --> 00:10:43,839
만일 여러분이 텍스트 데이터를 처음 다뤄보신다면, 이러한 기본적인 텍스트 전처리 접근법에 대한 이해가 여러분이 효과적인 모델을 학습시키는데에 도움이 될 것입니다.

51
00:10:41,760 --> 00:10:58,000
만일 여러분이 텍스트 데이터 경험이 많은 사람이라면, 아마 우리가 그랬던 것처럼 느끼셨을 겁니다.

52
00:10:54,480 --> 00:11:17,519
책이든 튜토리얼이든 블로그 글이든 전처리 과정이 어떻게 되고 그 과정에서 내리는 선택들이 모델 결과에 어떤 영향을 주는지에 대해 자세하고 깊게 다루는 자료들이 생각보다 많이 없다는 것을 말이죠.

53
00:11:15,600 --> 00:11:28,800
이제 전처리에 대해 좀 더 자세히 이야기 해봅시다. 기본적인 변수 가공(fature engineering) 접근법들을 살펴보며, 그들이 어떻게 동작하는지, 또 무엇을 하는지 알아봅시다.

54
00:11:27,040 --> 00:11:31,920
토큰화(tokenization)에서 시작해봅시다.

55
00:11:28,800 --> 00:11:52,320
토큰화는 우리가 하려고 텍스트 분석의 종류와 상관 없이, 그 분석이 탐색적 데이터 분석이든 모델을 만드는 것이든,

56
00:11:49,040 --> 00:11:55,360
일반적으로 자연어를 머신러닝의 변수로 바꾸기 위한 첫번째 단계입니다.

57
00:11:52,320 --> 00:12:02,079
토큰화에서는 우리는 문자열이나 문자 벡터를 입력으로 넣어주고, 또 토큰의 유형도 입력으로 넣어줍니다.

58
00:11:59,440 --> 00:12:12,480
토큰은 우리가 관심이 있는 글의 단위로, 예를 들면 단어 같은 것입니다. 토큰화는 글을 우리가 관심있는 유형의 토큰으로 분해합니다.

59
00:12:09,519 --> 00:12:19,040
우리가 일반적으로 텍스트를 토큰이라는 어떤 단위로 쪼갤 때 가장 많이 사용하는 토큰의 유형은 바로 '단어'입니다.

60
00:12:17,839 --> 00:12:30,880
단어를 기준으로 글을 쪼개는 것이 간단하거나 명백해 보일 수 있지만, 많은 언어들, 심지어는 대부분의 언어들에서 단어가 무엇인지 명확하게 정의하기는 어렵습니다.

61
00:12:29,519 --> 00:12:40,959
단어 사이에 띄어쓰기를 하지 않는 많은 언어들에서 단어를 기준으로 토큰화 하는 것은 어렵고,

62
00:12:38,240 --> 00:13:04,079
또 영어나 한국어처럼 띄어쓰기를 하는 언어라도 영어의 축약형처럼 단어를 구분해내기 모호한 경우, 한국어의 보조사(particle)처럼 붙어있지만 사실은  두 단어로 간주하는 것이 더 정확한 경우,

63
00:13:01,519 --> 00:13:13,839
이탈리아어, 프랑스어와 같은 로망스어군 언어(Romance Language)의 대명사나 부정어처럼 붙어있지만 두 개의 다른 단어로 간주되어야 하는 경우와 같은 특정한 경우들 때문에 단어를 기준으로 토큰화 하는 것은 어려운 일입니다.

64
00:13:12,399 --> 00:13:32,720
만일 여러분이 무엇을 할 것인지 정했고, 몇 가지 선택을 거쳐 결국 텍스트를 토큰화하는데 성공했다면, 이제 여러분은 이것을 탐색적 데이터 분석이나 비지도 학습 알고리즘에서 사용할 수 있으며 지금부터 얘기할 내용인 예측 모델의 변수로도 사용할 수 있습니다.

65
00:13:30,320 --> 00:13:45,199
여기에 표시되는 내용은 영국의 Tate 컬렉션에 있는 예술 작품의 소재에 대해 설명하는 글들을 회귀 모형으로 훈련한 결과입니다.

66
00:13:43,279 --> 00:14:02,079
우리가 예측하고자 하는 것은 작품이 몇 년도에 창작되었는지와 작품의 소재입니다. 작품의 소재는 짧은 글로 묘사되어 있고요.

67
00:14:00,079 --> 00:14:29,920
보시다시피 연필로 그려진 그림, 수채화, 판화는 비교적 예전에 창작되었을 가능성이 높았고, 사진, 스크린 프린팅(screen printing),  코끼리 똥(dung), 반짝이를 사용한 예술은

68
00:14:27,760 --> 00:14:37,519
비교적 최근에 창작된 현대 미술일 가능성이 높았습니다.

69
00:14:33,440 --> 00:15:17,279
텍스트를 토큰화하는 방법은 우리는 예술 작품의 소재에 대한  자연어 텍스트에서 시작합니다. 자연어를 토큰화하는 방식은 예측 모형 학습 결과에 큰 영향을 미칩니다. 만약 다른 방식으로 토큰화했다면 연도예측의 정확도, 우리가 어떻게 이 모델을 해석할 것인지, 그리고 어떤 것들을 배울 수 있는지와 같은 성능 측면에서 다른 결과가 나올 것입니다.

70
00:15:15,279 --> 00:15:31,279
앞선 사례는하나의 단어를 기준으로 토큰화하는 것의 한 방법입니다. 단일 단어 또는 유니그램(unigram)으로 분해하는 대신, n-그램이라는 다른 토큰화 방식을 사용할 수도 있습니다.

71
00:15:27,440 --> 00:15:37,920
n-그램은 주어진 텍스트 시퀀스 속 연속된 n개의 시퀀스를 말합니다.

72
00:15:35,199 --> 00:15:54,959
다음은 동물 설명문 텍스트가 바이그램(Bi-gram), 즉 2개의 토큰으로 이루어진 n-gram으로 쪼개진 모습입니다. 바이그램 간의 단어들이 어떻게 겹치는지 주목해보세요.

73
00:15:51,519 --> 00:16:11,120
collard'라는 단어는 첫 번째 바이그램인 'the collared'와 두 번째 바이그램인 'collared peccary'에 모두 나타납니다. 그리고 'peccary' 또한 두 번째 바이그램과 세 번째 바이그램 두 곳에서 나타납니다. 이와 같이, n-gram 토큰화는 텍스트를 따라 미끄러짐 이동(slide)을 하면서 겹치는 토큰 집합을 만듭니다.

74
00:16:07,040 --> 00:16:14,480
다음은 같은 텍스트에 대한 트라이그램(tri-gram) 결과를 보여줍니다.

75
00:16:11,120 --> 00:16:23,360
유니그램(uni-gram)을 사용하면 더 빠르고 효율적이지만 단어 순서에 대한 정보는 잡아내지 못합니다.

76
00:16:20,560 --> 00:16:41,440
n-gram에서 n에 2, 3 혹은 그보다 더 높은 값을 사용하면, 여러 개의 단어로 구성된 구(phrase)가 지닌 단어 순서나 개념과 같이 보다 복잡한 정보를 보존할 수 있습니다.

77
00:16:37,680 --> 00:16:56,480
하지만, 이 경우 토큰 벡터 공간이 극적으로 증가하는 반면, 각 토큰(n-gram)이 텍스트 내에서 발견되는 횟수는 감소하게 됩니다. 이는 데이터에 따라

78
00:16:55,120 --> 00:17:00,399
좋은 예측 결과를 얻지 못할 수 있음을 의미합니다.

79
00:16:58,160 --> 00:17:14,079
다양한 n값을 이용한 n-gram들을 함께 이용함으로써 다양한 수준의 세부 정보를 텍스트로부터 추출할 수 있습니다. 유니그램은 어떤 개별 단어가 많이 사용되었는지 알려줄 수 있습니다.

80
00:17:11,439 --> 00:17:23,360
이러한 단어 중 일부는, 다른 특정 단어들과 함께 나타나는 경우가 그리 많지 않은 경우, 바이그램이나 트라이그램에서는 간과될 수 있습니다.

81
00:17:23,520 --> 00:17:39,679
다음 그림은 미국 대법원 의견의 연도를 예측하는 Lasso 회귀 모형들의 성능을 보여주고 있는데요, 모형 세 가지 다른 수준의 n-gram에 기반해 모형의 성능을 비교합니다.

82
00:17:37,840 --> 00:17:51,840
여기서 우리가 수행하는 작업은  미국 대법원 문서를 가져온 뒤 그 문서가 언제 작성되었는지 예측하는 것입니다.

83
00:17:49,760 --> 00:17:58,799
즉, 텍스트의 내용을 통해, 텍스트가 얼마나 오래된 것인지 예측할 수 있을까 하는 문제입니다

84
00:17:55,360 --> 00:18:13,600
사용할 토큰의 개수를 1000개로 제한했을 때, 유니그램만 사용한 모형이 이 미국 대법원 문서 연도 예측 문제에서는 가장 좋은 성능을 보였습니다

85
00:18:11,280 --> 00:18:25,760
하지만 유니그램을 사용하는 것이 항상 가장 좋은 성능을 보이는 것은 아닙니다. 데이터 세트 자체, 그리고 사용하는 모형에 따라, 유니그램과 바이그램을 결합한 경우나, 혹은 또 다른 옵션을 선택한 경우가  더 나은 예측 성능을 보일 수 있습니다.

86
00:18:23,679 --> 00:18:43,039
이 때 바이그램과 트라이그램에 담겨있는 좀 더 복잡한 정보를 모형에 반영하려면, 아마도 모형의 토큰 수를 상당히 늘려야 할 것입니다.

87
00:18:40,160 --> 00:18:51,039
이와 같은 결과를 볼 때, n-그램을 식별하는데에는 비용이 많이 든다는 점을 명심하세요.

88
00:18:48,960 --> 00:19:00,640
우리는 비용과 모델의 성능 향상도를 비교해야 합니다.

89
00:18:58,720 --> 00:19:15,840
바이그램(Bi-gram)을 추가했을 때 모델의 성능이 아주 조금만 개선된다면 어떨지와 같이, 바이그램을 식별하고 학습시키는 데에 드는 시간 대비 바이그램을 사용한 모델의 성능이 얼마나 좋아졌는지를 고려해야합니다.

90
00:19:13,200 --> 00:19:27,120
예를 들어, 토큰수를 일정하게 유지한 대법원 의견 데이터셋의 경우, 모델 학습에는 동일한 수의 토큰이 포함되었습니다.

91
00:19:25,919 --> 00:19:46,799
하지만 바이그램과 유니그램을 함께 사용했을 때  변수 가공을 하는데 걸리는 시간은 유니그램만 사용했을 때 걸리는 시간의 두배입니다. 또한 트라이그램까지 사용하면 유니그램만 학습하는 것보다 거의 5배 시간이 걸립니다.

92
00:19:44,320 --> 00:19:56,559
즉, 컴퓨터 계산 비용이 많이 드는 작업인 것입니다. 또 다른 방향으로 접근해보자면,

93
00:19:53,760 --> 00:20:05,440
단어보다 작은 단위로도 토큰화를 할 수 있습니다. 화면에 보이는 것과 같은 것들을 'character shingles'이라고 합니다.

94
00:20:03,280 --> 00:20:13,120
"collared peccary"라는 단어를 뽑고, 단어를 보는 대신 내려가서 하위 단어 정보를 살펴볼 수 있습니다.

95
00:20:10,880 --> 00:20:20,880
단어를 기계 학습에 적합한 하위 단어로 나누는 방법에는 여러 가지가 있는데요,

96
00:20:18,480 --> 00:20:31,360
종종 이런 접근 방식이나 알고리즘은 예측 시 알 수 없는 단어나 새로운 단어를 인코딩할 수 있다는 이점이 있습니다.

97
00:20:29,039 --> 00:20:41,200
새로운 데이터에 대한 예측을 해야 할 때, 새로운 단어들이 종종 등장하곤 하는데요,

98
00:20:39,200 --> 00:20:49,360
만일 그 단어가 훈련 데이터에 없었다면, 우리는 어떻게 이 단어를 처리해야할까요?

99
00:20:45,600 --> 00:20:58,400
하위 단어(subword) 정보를 사용하여 모델을 학습시켰고, 하위 단어에 그 새로운 단어의 하위단어와 비슷한 것이 있다면,  우리는 이새로운 단어를 처리할 수 있습니다.

100
00:20:55,760 --> 00:21:13,360
이렇게 하위 단어(subword) 정보를 사용해 형태학적 시퀀스를 다양한 종류의 모형에 사용할 수 있습니다. 이는 영어뿐만 아니라 다양한 언어에도 적용되는 방식입니다.

101
00:21:11,919 --> 00:21:22,960
지금 보시는 것은 매우 짧은 텍스트 데이터 셋을 분석한 분류 모형의 결과입니다.

102
00:21:20,000 --> 00:21:27,120
이들은 미국에 있는 우체국 이름인데요, 매우 짧습니다.

103
00:21:24,240 --> 00:21:41,360
모형의 목적은 우체국이 태평양 한가운데 하와이에 있는지, 아니면 하와이가 아닌 미국의 나머지 지역에 위치하고 있는지를 예측하는 것이였습니다.

104
00:21:38,159 --> 00:21:49,200
저는 우체국 이름의 하위 단어(subword)를 변수로 만들었는데요,

105
00:21:46,480 --> 00:22:00,240
이름에 'h'와 'p'로 시작하거나 'ale' 라는 하위 단어(subword)를 포함하는 우체국은 하와이에 있을 가능성이 더 높다는 것을 알게 되었습니다.

106
00:21:57,440 --> 00:22:11,440
또한 이름에 하위단어  'and', 'ri' 및 'ing'를 포함하고 있는 경우는 하와이가 아닌 미국 지역에 위치한 우체국의 이름일 가능성이 높았습니다 .

107
00:22:09,360 --> 00:22:19,600
방금 살펴본 사례는 토큰화 방식에 따라 우리가 배울 수 있는 것이 달라진다는 것을 보여주는 예시였습니다.

108
00:22:18,559 --> 00:22:29,919
`Tidymodels`에서는 토큰화를 둘러싼 우리의 모든 결정들이 기록됩니다. 코드는 다음과 같습니다.

109
00:22:26,880 --> 00:22:38,640
사용할 변수 즉 재료를 지정하는  레시피(recipe)로 시작한 다음, 이러한 전처리 단계를 정의합니다.

110
00:22:35,679 --> 00:22:50,400
첫번째 단계에서, 그리고  정말 간단하고 기본적인 단계에서도 데이터 과학자가 내리는 결정은 모델의 결과에 큰 영향을 미칩니다.

111
00:22:47,120 --> 00:22:56,240
다음으로 이야기하고자 하는 전처리 단계는 불용어(stopwords)에 관한 것입니다.

112
00:22:53,600 --> 00:23:11,840
텍스트를 토큰으로 분할시키면, 모든 단어가 기계 학습 작업에 대해 동일한 양의 정보를 전달하지 않는다는 사실을 종종 발견합니다.

113
00:23:09,520 --> 00:23:18,880
흔히 쓰이는 단어들 중 의미 있는 정보가 거의 또는 전혀 전달되지 않는 단어를 "불용어(stopwords)"라고 합니다.

114
00:23:16,080 --> 00:23:22,720
다음은 한국어의 불용어 목록 중 하나입니다.

115
00:23:20,400 --> 00:23:36,640
많은 자연어 처리 작업에서 이런 불용어를 제거하는 것은 일반적인 과정입니다.

116
00:23:35,039 --> 00:23:44,320
제가 여기서 보여드리는 것은 정말 널리 사용되는 영어의 짧은 불용어 목록 중 하나입니다.

117
00:23:41,919 --> 00:24:00,400
"I", "me", "my" 같은 대명사, "and", "of", "the"와 같은 접속사는 그다지 중요하지 않은 매우 흔한 단어입니다.

118
00:23:57,679 --> 00:24:15,840
불용어를 단순히 제거하기로 한 것은 흔히 찾아볼 수 있는 많은 자료들에 설명된 것보다 더 복잡하고 어려울 수 있습니다.

119
00:24:12,000 --> 00:24:22,400
거의 대부분의 자연어처리 실무자들은 미리 만들어진 불용어 목록을 사용합니다.

120
00:24:20,000 --> 00:24:32,960
이 그림은 "Upset Plot"이라고 불리며,  일반적으로 사용되는 영어 불용어 목록에 대한 교집합을 시각화합니다.

121
00:24:31,200 --> 00:24:40,159
세 가지 목록의 이름은 "snowball", "smart",  "iso" 인데요,

122
00:24:37,440 --> 00:24:51,760
막대 길이를 통해 목록의 길이가 표시되었고,  수직 막대들로부터 목록들 간 공통된 단어들을 나타내고 있는데요,

123
00:24:48,799 --> 00:25:00,720
목록들의 길이가 모두 다르고, 모두 동일한 단어 집합을 포함하지 않는다는 것을 알 수 있습니다.

124
00:24:58,320 --> 00:25:20,480
불용어 말뭉치에 대해 기억해야 할 중요한 사항은 불용어 말뭉치는 완벽히 중립적인 상태에서 만들어진 것이 아니라, 특정 문맥 속에서 형성되었다는 것입니다.

125
00:25:16,960 --> 00:25:28,559
그래서 이러한 불용어 말뭉치는 편향되었을 수도 있습니다. 이 말뭉치들 역시 이들이 생성된 자연어 데이터 셋의

126
00:25:27,039 --> 00:25:37,440
특성을 반영하고 있습니다.

127
00:25:34,000 --> 00:25:45,600
다음은 smart말뭉치에는 있지만 snowball 말뭉치에는 없는 10개의 단어입니다.

128
00:25:43,039 --> 00:25:52,240
이 단어들은 모두 축약형임에 주목하세요. 하지만, 이 차이는  snowball에 축약형이 포함되어 있지 않기 때문은 아닙니다.

129
00:25:50,000 --> 00:26:05,600
nowball은 많은 축약형을 포함하고 있습니다. 또한, she's가 목록에 있는 것에 주목해보세요. 이는, snowball 말뭉치에는 he's는 포함되어 있지만 she's는 포함되어 있지 않다는 것을 의미합니다

130
00:26:03,360 --> 00:26:07,679
이것은 편향의 한 예입니다.

131
00:26:05,600 --> 00:26:13,520
이러한 편향은 해당 목록이 매우 큰 텍스트 데이터 세트로부터 생성되기 때문에 발생합니다.

132
00:26:11,440 --> 00:26:22,480
말뭉치 생산자(lexicon creator)는 큰 말뭉치(corpus) 내에서 가장 자주 사용되는 단어를 살펴보고,

133
00:26:19,760 --> 00:26:33,840
사용 빈도의 기준치를 정한 뒤, 어떤 단어들을 포함시키고 제외시킬지를 결정하여 말뭉치를 만듭니다.

134
00:26:29,919 --> 00:26:54,799
많은 대규모 언어 데이터 셋에서 남성을 더 많이 표현하기 때문에 불용어 목록에 "he's"가 있지만 "she's"는 없는 상황이 발생합니다.

135
00:26:51,840 --> 00:27:07,679
우리는 실무자로서 특정 영역에 적합한 것을 기준으로 모델링과 분석에서의 많은 것들을 결정해야 합니다.

136
00:27:05,279 --> 00:27:12,960
불용어 목록을 선택하는 경우에도 마찬가지입니다.

137
00:27:10,720 --> 00:27:21,760
Tidymodels에서는 레시피(recipe)에 추가적으로 단계를 더하여 불용어 제거와 같은 전처리 단계를 구현할 수 있습니다.

138
00:27:19,200 --> 00:27:35,360
우선 우리가 사용할 변수들을 정의한 뒤, 텍스트를 토큰화하고, 불용어들을 제거합니다. 다른 인수(argument)를 주지 않았기 때문에 기본으로 설정된 과정을 따라 불용어들이 제거될 것입니다.

139
00:27:32,640 --> 00:27:42,399
물론 기본으로 설정된 것과 다른 과정으로 불용어들을 제거할 수도 있으며, 우리의 특수한 상황에 적합하도록 불용어 리스트를 커스터마이징 할 수도 있습니다.

140
00:27:42,880 --> 00:28:02,080
이 표는 앞서 봤던 고등 법원 데이터를 사용해 작성년도를 예측하는 과제를 위해 각기 다른 길이의 불용어 사전을 이용한 모델의 성능을 비교하고 있습니다.

141
00:27:59,279 --> 00:28:06,960
스노우볼 말뭉치가 가장 적은 수의 단어를 포함하고 있었는데

142
00:28:04,480 --> 00:28:10,320
가장 좋은 성능을 보였습니다.

143
00:28:06,960 --> 00:28:16,399
이 과제에서는 불용어를 적게 없앨수록 모델의 성능이 더 좋게 나왔습니다.

144
00:28:12,799 --> 00:28:29,919
불용어를 적게 없앨수록 모델의 성능이 더 좋아진다는 것은 모든 데이터나 문맥에 적용되는 것은 아니지만, 어떤 불용어를 줄이느냐에 따라 모델의 성능이 눈에 띄게 달라진다는 점 자체는 명백한 사실입니다.

145
00:28:26,799 --> 00:28:41,360
뭐가 더 모델의 성능을 좋게 하는지 아는 방법은 여러 시도를 직접 해보는 방법 밖에 없습니다.

146
00:28:39,279 --> 00:28:49,919
머신러닝은 사실 경험적 영역입니다. 뭐가 가장 좋은 방법인지, 그리고 그 이유는 무엇인지에 대해 우리는 알 수 없습니다.

147
00:28:47,440 --> 00:29:01,039
여러 다른 옵션들을 시도해보아야 무엇이 가장 나은 옵션인지 알 수 있습니다.

148
00:28:58,799 --> 00:29:07,760
이제부터 세번째 전처리 과정인 어간추출(stemming) 에 대해 이야기하고자 합니다.

149
00:29:05,279 --> 00:29:18,559
우리가 텍스트를 분석할 때는 한 어간(stem)에 대한 여러 버전의 단어를 포함하고 있는 있는 문서들이 많습니다.

150
00:29:15,440 --> 00:29:31,279
영어로 예를 들어보자면, 우리가 '동물들(animals)'이라는 복수형과  '동물(animal)'이라는 단수형을 똑같이 묶어 취급하고 싶다고 해봅시다.

151
00:29:27,919 --> 00:29:33,200
이런게 바로 어간추출의 핵심입니다.

152
00:29:31,279 --> 00:29:40,320
어간추출에 있어서 딱 정해진 옳은 방식은 없습니다.

153
00:29:37,279 --> 00:29:44,880
이 그래프는 세 개의 다른 영어 어간 추출 접근법을 보여주고 있는데요,

154
00:29:42,480 --> 00:29:57,520
단순히 복수형의 마지막 's'를 지워버리는 방법부터 좀 더 복잡하게 복수형 말미를 처리하는 접근법까지 있습니다. 가운데는 s어간추출 (s stemmer)이라고 불리는데

155
00:29:54,480 --> 00:30:01,360
몇 가지의 규칙들을 가지고 있습니다.

156
00:29:58,640 --> 00:30:09,919
마지막은 어간추출 구현을 가장 잘했다고 알려진 모델 중 하나인 포터 알고리즘(Porter algorithm)입니다.

157
00:30:07,279 --> 00:30:19,840
앞서 보여드렸던 동물 묘사 텍스트 예시의 상위 20단어들에 대해 포터 어간추출방식이 나머지 두개의 어간추출 방식과 가장 다르다는 것을 볼 수 있습니다.

158
00:30:16,799 --> 00:30:25,600
species(종)이라는 단어가 어떻게 다르게 추출되었는지도 볼 수 있고요,

159
00:30:24,240 --> 00:30:40,559
animal(동물)과 predator(포식자)라는 단어도 다르게 취급되었고, live(살다), living(살다), life(삶), lives(살다)와 같은 단어의 묶음도 다르게 취급되었습니다.

160
00:30:38,960 --> 00:30:53,360
어간추출을 통해 단어들을 묶을 수 있고, 그렇게 같이 묶인 단어들을 인간들이 언어를 이해하는 방식으로 이해할 수 있다고 믿기 때문에 실무자들은 어간추출에 관심이 많습니다.

161
00:30:51,600 --> 00:31:09,600
우선 이거 해, 그 다음에 이거 해, 이런 알고리즘적인 단계별 규칙에 기반한 접근법을 어간 추출(stemming)이라고 하는데

162
00:31:07,039 --> 00:31:14,720
여러분은 어간 추출을 사용하실 수도 있고,

163
00:31:12,080 --> 00:31:31,120
사전을 이용해 어떤 단어들이 같이 묶일 수 있는지를 언어학적인 이해에 기반해 접근하는 표제어 추출(lemmatization)을 사용하실 수도 있습니다.

164
00:31:28,559 --> 00:31:47,600
한국어에 대해 이런 과제를 수행하기 위한 대부분의 접근법은 사전과 많은 양의 단어 학습에 기반한 표제어 추출 접근법입니다.

165
00:31:45,679 --> 00:32:04,080
이런 어간추출이나 표제어 추출이 아주 도움되는 좋은 과정이라는 생각이 드실 겁니다. 왜냐하면 텍스트 데이터는 일반적으로 토큰의 갯수에 따라 엄청나게 많은 변수를 다뤄야 하기 때문이죠.

166
00:32:02,320 --> 00:32:09,440
이건 텍스트 데이터를 다루는 가장 전형적인 상황입니다.

167
00:32:05,840 --> 00:32:18,240
저는 동물 묘사 데이터로 행렬을 만들었습니다. 행렬은 전형적으로 머신러닝 알고리즘에서 많이 이용하죠.

168
00:32:16,080 --> 00:32:31,600
얼마나 많은 변수(feature)들이 여기 있는지 보세요. 거의 17,000개에 가까운 피처가 있습니다.

169
00:32:29,679 --> 00:32:36,640
저게 모델로 들어가는 변수의 갯수입니다.

170
00:32:33,519 --> 00:32:53,760
희소성(sparsity) 좀 보세요. 98%입니다. 이것은 엄청난 희소 행렬입니다. 이게 저희 머신 러닝 알고리즘에 들어가서 지도 학습 머신 러닝 모델을 만들 데이터라니요.

171
00:32:51,600 --> 00:33:02,240
만일 제가 여기서 단어의 어간을 이용한다면, 어간 추출 접근법을 사용한다면, 우리는 단어 변수의 갯수를 수 천개 줄일 수 있습니다.

172
00:33:00,159 --> 00:33:16,880
어간추출을 통해 여전히 희소성은 그리 많이 낮출 수 없어 안타깝지만, 어간추출 알고리즘이 단어를 묶는 방식을 통해 변수의 갯수를 아주 많이 줄일 수 있습니다.

173
00:33:15,200 --> 00:33:29,039
일반적으로 단어를 상징하는 변수의 갯수를 줄이는 것은 머신러닝 모델의 성능을 좋게한다고 하는데요,

174
00:33:26,640 --> 00:33:51,039
이는 어간 추출을 통해 어떠한 중요한 정보를 잃지 않았음을 가정합니다. 어간 추출이나 표제어 추출은 어떠한 맥락에서는 많은 도움이 될 수 있지만, 이를 수행하는 전형적인 알고리즘들은 과하게 추출을 수행하는 경향이 있습니다.

175
00:33:48,320 --> 00:33:59,279
이 모델들은 민감도(sensitivity, recall, true positive rate) 개선에 집중하고 있는데,

176
00:33:56,880 --> 00:34:07,200
이는 특이도(specificity, true negative rate)나 정밀도(precision)를 악화시키죠.

177
00:34:04,640 --> 00:34:25,760
지도학습 머신러닝에서 어간 추출은 모델의 양성예측값 (PPV; Positive Predictive Value), 정밀도(precision), 혹은 음성을 양성이라고 판단하지 않는 능력 (제가 맞게 말했으면 좋겠네요!) 등에 영향을 미칩니다.

178
00:34:22,800 --> 00:34:43,200
좀 더 구체적으로 말해보자면, 어간 추출은 모델이 양성을 찾아내는 능력을 향상 시킬 수 있습니다. 예를 들어 동물 설명과 특정 섭취 질환이 관련있다면 이를 잘 찾아낼 수 있게 됩니다.

179
00:34:40,720 --> 00:34:52,240
그러나 만일 텍스트가 과하게 어간 추출되면, 모델은 음성을 찾아내는 능력을 잃습니다. 즉 그 특정 섭취 질환과 관련이 없는 동물 설명문을 찾아내는 능력을 잃습니다.

180
00:34:49,760 --> 00:35:08,400
어간 추출 알고리즘을 미세하게 조정할 방법이 없기 때문에, 텍스트 데이터로 모델을 학습 시킬 때, 이러한 균형을 찾는 것은 아주 어렵습니다.

181
00:35:05,119 --> 00:35:17,440
지금 보여드리는 것과 같이 아주 간단한 텍스트의 전처리도 방식도 내부적으로는 굉장히 많은 계산을 필요로 할 수 있습니다.

182
00:35:15,280 --> 00:35:42,560
이렇게 불용어를 제거할 것인지, 또 어간 추출을 할 것인지에 대해 우리가 내리는 결정은 모든 모델들에게, 그 모델이 아주 간단하고 전통적인 모델이든 딥러닝이든, 아주 큰 영향을 미칠 수 있습니다.

183
00:35:39,040 --> 00:35:58,560
이것은 우리가 이런 텍스트 변수 가공(feature engineering)에 대해 배우고 가르치고 쓰는 것이 실제로 더  강건한(robust) 통계 분석들에 기여할 수 있다는 것을 시사하기도 합니다.

184
00:35:56,320 --> 00:36:14,320
제가 아까 언급했던 텍스트 데이터의 희소성(sparsity)에 대해 좀 더 이야기 하고 싶은데요, 희소성(sparsity)는 텍스트 데이터 만의 정말 고유한 특징입니다.

185
00:36:10,400 --> 00:36:25,119
정말 소수의 단어는 아주 많이 언급되고, 정말 많은 단어들은 몇 번 사용되지 않는 말의 특성 때문에

186
00:36:22,640 --> 00:36:27,520
더 많은 문서를 추가하거나 새로운 단어들을 말뭉치(corpus)에 추가하면

187
00:36:25,119 --> 00:36:38,320
희소성(sparsity)은 지금 보이는 표에 나타나는 것과 같은 관계를 보입니다.

188
00:36:35,680 --> 00:36:53,520
새로운 단어들을 넣을수록 희소성은 빠르게 증가합니다. 그리고 이 문서를 처리하는 데 필요한 메모리 용량도 빠르게 증가합니다.

189
00:36:51,920 --> 00:37:13,119
희소행렬(sparse matrix)처럼 높은 데이터를 위한 데이터 구조를 선택한다고 하더라도, 이 데이터를 처리하기 위해 필요한 메모리의 용량은 여전히 비선형적으로 매우 빠르게 증가합니다.

190
00:37:09,920 --> 00:37:23,839
그리고 이렇게 큰 용량의 데이터를 사용하면 모델을 학습시키는 데 많은 시간이 소요될 수도 있고, 아니면 여러분의 컴퓨터 용량보다도 더 커서

191
00:37:21,760 --> 00:37:31,200
비싼 돈을 주고 클라우드 서비스의 메모리를 사용해야 하는 등 많은 어려움을 야기할 것입니다.

192
00:37:27,680 --> 00:37:43,520
이러한 어려움들은 모델에 벡터를 쓰는 방식에 대해 생각해보게 했습니다.

193
00:37:40,240 --> 00:37:58,160
언어학자들은 사람들이 어떻게 언어를 구사하는지를 바탕으로 오랫동안 모델을 위한 벡터를 연구하며 어떻게 하면 텍스트 데이터의 차원을 낮출 수 있는지 고민했습니다.

194
00:37:55,280 --> 00:38:04,960
이 인용문은 1957년으로 거슬러 올라갑니다.

195
00:38:02,960 --> 00:38:15,440
여기에서 제시하는 아이디어는, 데이터에 나타나는 것처럼 우리는 단어들을 희소하게 사용하지만, 단어들을 무작위로 사용하지는 않는다는 것입니다.

196
00:38:12,800 --> 00:38:17,359
우리가 사용하는 단어들은 독립적이지 않습니다.

197
00:38:15,440 --> 00:38:25,920
단어들은 독립적으로 사용되지 않으며 같이 사용된 단어들은 관계가 있습니다.

198
00:38:23,359 --> 00:38:40,160
우리는 이 관계를 사용해 고차원의 희소(sparse)한 데이터를 낮은 차원의 조밀한 데이터로 변환할 수 있습니다.

199
00:38:37,359 --> 00:38:49,839
여전히 몇 백 차원 정도일 수는 있지만 몇 천, 몇 만 차원에 비하면 아주 낮은 차원입니다.

200
00:38:48,160 --> 00:39:05,839
우리는  단어 갯수와 행렬분해 혹은 좀 더 인공 신경망을 필요로 하는 좀 더 복잡한 수학을 다룰 수 있는 통계 모델에 정말 고차원의 벡터 공간(space)을 넣고,

201
00:39:03,680 --> 00:39:21,839
낮은 차원의 벡터 공간을만들 수 있습니다. 이 벡터 공간은 어떤 단어들이 같이 쓰이는지에 대한 정보를 담고 있는 벡터를 사용해서 생성되었다는 점에서 특별합니다.

202
00:39:18,560 --> 00:39:26,480
우리는 단어가 곁에 두는 친구를 보면 그 단어를 알 수 있습니다.

203
00:39:24,079 --> 00:39:35,119
이런 단어 벡터(word vector)나 단어 임베딩(word embedding)을 생성하거나 학습하기 위해서는 큰 텍스트 데이터가 필요합니다.

204
00:39:32,000 --> 00:39:53,839
제가 지금 보여드리는 이 표는 미국의 소비자금융보호국(consumer financial protection bureau)을 향한 컴플레인이 담긴 말뭉치로 부터 생성된 임베딩입니다.

205
00:39:51,680 --> 00:40:13,119
소비자금융보호국은 미국 정부 부처로, 사람들이 신용 카드, 모기지, 학생 대출과 같은 금융 상품에 대해  컴플레인하고 신고할 수 있는 기관입니다.

206
00:40:11,119 --> 00:40:24,000
예를 들어 신용카드에 뭔가 문제가 생겼다든지, 모기지에 문제가 생겼다든지, 어떤 회사가 공정하지 않다든지와 같은 컴플레인들이죠.

207
00:40:20,000 --> 00:40:27,839
저는 이 모든 컴플레인을 이용해 모델을 만들었습니다.

208
00:40:25,440 --> 00:40:32,400
고차원 벡터 공간을 저차원 벡터 공간으로 만들죠.

209
00:40:30,160 --> 00:40:44,079
그리고 그 공간을 들여다보면 우리는 어떤 단어들이 이 공간 내의 어떤 다른 단어들과 연관이 있는지 이해할 수 있습니다.

210
00:40:40,240 --> 00:41:09,680
새로운 임베딩 벡터 공간은 월(month)이라는 단어는 년도(year), 월의 복수형(months), 월간(montly), 할부(installment payment) 같은 단어들과 가장 가깝다고 정의되었고, 이는 신용카드나 모기지 등 금융 상품 맥락에서 일리가 있습니다.

211
00:41:06,720 --> 00:41:26,240
에러(error)는 실수(mistake),  사무적 오류가 있다 할 때 사무적(clerical), 문제(problem), 모기지에 일시적인 오류가 있었다 할 때 일시적 오류 (glitch),

212
00:41:23,440 --> 00:41:30,880
오해(miscommunication, misunderstanding) 같은 단어들과 가깝습니다.

213
00:41:28,319 --> 00:41:33,599
이런 단어들은 비슷하게 사용됩니다.

214
00:41:31,599 --> 00:41:42,240
이런 임베딩을 만들기 위해서는 상당한 크기의 데이터가 필요하기 때문에 여러분들은 임베딩을 직접 만드실 필요는 없습니다.

215
00:41:39,520 --> 00:41:54,079
여러분은 접근 권한이 없는 큰 말뭉지 데이터에 대해 접근 권한을 가지고 있는 사람이 이를 이용해 만든 임베딩을 사용하면 됩니다.

216
00:41:51,359 --> 00:41:55,599
누군가가 미리 만들어둔 임베딩도 살펴봅시다.

217
00:41:54,079 --> 00:42:04,560
이 표는 에러(error)와 비슷한 단어를 글로브 임베딩(glove embedding)에서 찾은 결과입니다.

218
00:42:02,800 --> 00:42:27,280
글로브 임베딩은 모든 위키피디아 데이터, 구글의 모든 뉴스 데이터와 같이 엄청나게 큰 데이터를 사용해 만들어진 임베딩입니다.

219
00:42:24,800 --> 00:42:40,839
어떤 단어들은 앞서 봤던 리스트에도 있었지만, 이 리스트에는 이제 사무적(clerical)이나 오해(miscommunication), 불일치(discrepancy)와 같이 금융 분야에서 주로 사용되는 단어들은 없습니다.

220
00:42:38,640 --> 00:42:54,319
그리고 계산(calculation), 확률(probability)과 같이 사람들이 금융 상품에 대한 컴플레인에서는 자주 말하지는 않는 단어들이 표에 있습니다.

221
00:42:51,599 --> 00:43:10,400
여기서 우리는 이런 것들이 어떻게 동작하는지 알 수 있습니다. 직접 말뭉치를 가지고 임베딩을 만든 앞 예시에서는 아주 구체적인 맥락 속에서 단어들의 관계를 알 수있었고,

222
00:43:06,800 --> 00:43:16,720
뒤에 나온 예시에서는 일반적인 데이터 셋으로 누군가가 학습시킨 것을 사용했습니다.

223
00:43:13,119 --> 00:43:27,359
임베딩은 텍스트 데이터의 아주 큰 말뭉치를 이용해 학습되고, 이 때 이 말뭉치의 특성이 임베딩의 일부가 됩니다.

224
00:43:24,240 --> 00:43:35,839
일반적으로 기계 학습은 훈련 데이터에 많은 영향을 받습니다.

225
00:43:32,240 --> 00:43:40,319
그리고 이러한 민감도는 텍스트 데이터를 다룰 때 보다 명확하게 드러납니다.

226
00:43:38,160 --> 00:44:01,440
아마도 단어 임베딩은 그런 것의 아주 고전적인 예시일 것입니다. 말뭉치 속에 담긴 사람들이 가지고 있는 편견이나 편향들이 임베딩에도 반영된다는 것이 밝혀졌습니다.

227
00:43:59,040 --> 00:44:14,640
우리가 쉽게 접할 수 있는 임베딩에서도 그 편향을 찾을 수 있습니다.

228
00:44:12,200 --> 00:44:23,280
우리는 임베딩에서 아프리카계 미국인들이 주로 쓰는 이름이

229
00:44:19,680 --> 00:44:30,240
유럽계 미국인이 쓰는 이름들보다 부정적인 감정과의 연관성이 더 높았음을 볼 수 있습니다.

230
00:44:27,680 --> 00:44:38,640
여성의 이름은 가족과 더 밀접하게 연관되어 있고, 남성의 이름은 경력과 더 밀접한 연관을 가지고 있습니다.

231
00:44:36,079 --> 00:44:44,079
여성이라는 단어는 예술과 더 관련성이 높았고, 남성이라는 단어는 과학과의 관련성이 더 높았습니다.

232
00:44:42,640 --> 00:45:05,040
실제로 단어 임베딩에는 우리가 가지고 있는 편향이 너무 잘 드러나서, 사회적 태도들의 변화를 수치화하는 데 임베딩을 사용할 수도 있습니다.

233
00:45:01,440 --> 00:45:11,119
단어 임베딩은 어쩌면 좀 과장되거나 극단적인 예시일 수도 있지만

234
00:45:08,160 --> 00:45:31,599
어쨌든 텍스트 데이터에 대한 변수 가공 단계에서 내리는 모든 결정들이 모델의 성능이든, 이 모델의 적절성과 공평성이든, 모델의 결과에 아주 큰 영향을 미친다는 것은 사실입니다.

235
00:45:29,520 --> 00:45:43,520
이 점을 고려했을 때, 텍스트 데이터를 통해 필요한 변수들을 만들어내는 전처리 단계에서 우리에게는 아주 많은 선택지와 또 그에 따른 상당한 책임이 있다는 점을 아셨으면 좋겠습니다.

236
00:45:39,680 --> 00:45:59,040
제 조언은 여러분이 꽤 깊이 이해할 수 있는 간단한 모델에서부터 시작하시라는 것입니다. 모델을 학습시키거나 튜닝할 때에 반드시 통계적으로 타당한 것들을 선택하세요.

237
00:45:56,720 --> 00:46:10,400
그래서 다양한 시도에 따른 모델의 성능 향상에 대해 잘 이해하고 있을 수 있도록 해야 합니다.

238
00:46:07,119 --> 00:46:20,240
모델 설명 가능성 (model explainability)도구나 프레임워크를 사용해서 좀 명확히 이해되지 않는 모델들에 대해 이해해보세요.

239
00:46:18,000 --> 00:46:30,319
제 동료와 저는 이러한 것들을 주제로 글을 썼고, 어떻게 Tidymodels로 이런 과정들을 거칠 수 있는지에 대해서도 썼으며, 앞으로도 계속해서 쓸 것이니 참고하실 분들은 참고해주세요.

240
00:46:28,720 --> 00:46:38,960
그럼 제 이야기는 여기서 이만 마무리 하겠습니다. 들어주셔서 정말 감사하고,

241
00:46:36,560 --> 00:46:45,119
또 한국 R 사용자회 주최측 분들께 다시 한 번 감사드립니다.

242
00:46:42,560 --> 00:46:51,240
끝으로 RStudio의 Tidymodels 팀 동료들에게도, 또 공동저자인 Emil Hvitfeldt에게도 고맙다는 말을 전하고 싶습니다.
