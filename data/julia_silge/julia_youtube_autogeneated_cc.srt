1
00:00:01,120 --> 00:00:06,080
hi my name is julia silgi i'm a data

2
00:00:04,160 --> 00:00:08,639
scientist and software engineer at r

3
00:00:06,080 --> 00:00:11,200
studio and i'd like to thank um the or

4
00:00:08,639 --> 00:00:13,519
the organizers of the r user group in

5
00:00:11,200 --> 00:00:15,519
korea so much for having me

6
00:00:13,519 --> 00:00:18,080
um today to speak to you

7
00:00:15,519 --> 00:00:20,080
i am so happy to be uh speaking

8
00:00:18,080 --> 00:00:21,119
specifically today about

9
00:00:20,080 --> 00:00:24,000
creating

10
00:00:21,119 --> 00:00:25,279
features for machine learning from text

11
00:00:24,000 --> 00:00:27,199
data

12
00:00:25,279 --> 00:00:29,359
for a couple of reasons

13
00:00:27,199 --> 00:00:31,679
having a better understanding of what we

14
00:00:29,359 --> 00:00:34,480
do to take text data

15
00:00:31,679 --> 00:00:36,880
and then to make it appropriate as an

16
00:00:34,480 --> 00:00:37,920
input for machine learning algorithms

17
00:00:36,880 --> 00:00:39,680
has

18
00:00:37,920 --> 00:00:42,879
many benefits

19
00:00:39,680 --> 00:00:45,039
both if you are directly getting ready

20
00:00:42,879 --> 00:00:48,399
to train a model or if you're at the

21
00:00:45,039 --> 00:00:50,719
beginning of some text analysis project

22
00:00:48,399 --> 00:00:52,800
or if you are trying to understand the

23
00:00:50,719 --> 00:00:55,440
behavior of a model that you're

24
00:00:52,800 --> 00:00:57,920
interacting with some way which is

25
00:00:55,440 --> 00:01:00,480
something that we do in our work as data

26
00:00:57,920 --> 00:01:03,359
scientists or in our

27
00:01:00,480 --> 00:01:06,640
in our daily lives more and more

28
00:01:03,359 --> 00:01:09,360
so when we build models for text either

29
00:01:06,640 --> 00:01:11,600
supervised or unsupervised

30
00:01:09,360 --> 00:01:15,759
we start with something

31
00:01:11,600 --> 00:01:17,600
like this this is some example text data

32
00:01:15,759 --> 00:01:21,040
that i'll use a couple of times during

33
00:01:17,600 --> 00:01:24,560
this talk that um describes some animals

34
00:01:21,040 --> 00:01:26,720
some animals i'm using some text data so

35
00:01:24,560 --> 00:01:30,960
this you know to me as an english

36
00:01:26,720 --> 00:01:34,400
speaker um looks familiar um like i'm i

37
00:01:30,960 --> 00:01:36,720
i um as someone who uses a human

38
00:01:34,400 --> 00:01:39,520
language so i look at this and i can

39
00:01:36,720 --> 00:01:41,280
read it i could speak it aloud and i

40
00:01:39,520 --> 00:01:43,920
understand

41
00:01:41,280 --> 00:01:45,759
i can interpret it what it means

42
00:01:43,920 --> 00:01:50,000
so this kind of data this sort of

43
00:01:45,759 --> 00:01:51,200
natural language data is being generated

44
00:01:50,000 --> 00:01:53,600
all the time

45
00:01:51,200 --> 00:01:56,880
in all kinds of languages

46
00:01:53,600 --> 00:01:59,840
in all kinds of contexts so

47
00:01:56,880 --> 00:02:00,960
whether you work in healthcare

48
00:01:59,840 --> 00:02:02,640
in tech

49
00:02:00,960 --> 00:02:05,040
in finance

50
00:02:02,640 --> 00:02:08,479
basically any kind of organization this

51
00:02:05,040 --> 00:02:09,360
sort of text data is being generated by

52
00:02:08,479 --> 00:02:11,920
um

53
00:02:09,360 --> 00:02:15,200
by customers

54
00:02:11,920 --> 00:02:17,599
by clients by internal stakeholders

55
00:02:15,200 --> 00:02:19,200
inside of a business by people taking

56
00:02:17,599 --> 00:02:20,000
surveys

57
00:02:19,200 --> 00:02:24,319
by

58
00:02:20,000 --> 00:02:26,720
via social media via business processes

59
00:02:24,319 --> 00:02:29,200
and and in all this natural language

60
00:02:26,720 --> 00:02:30,879
there's information

61
00:02:29,200 --> 00:02:33,519
latent in

62
00:02:30,879 --> 00:02:35,360
that text data that can be used to make

63
00:02:33,519 --> 00:02:36,400
better decisions

64
00:02:35,360 --> 00:02:39,680
however

65
00:02:36,400 --> 00:02:41,280
computers are not great at um

66
00:02:39,680 --> 00:02:44,319
at you know looking at this and doing

67
00:02:41,280 --> 00:02:45,440
math on language as it's represented

68
00:02:44,319 --> 00:02:47,440
like this

69
00:02:45,440 --> 00:02:50,560
and instead language has to be

70
00:02:47,440 --> 00:02:53,680
dramatically transformed to some kind of

71
00:02:50,560 --> 00:02:55,440
machine readable numeric representation

72
00:02:53,680 --> 00:02:57,280
that looks more like this what i'm

73
00:02:55,440 --> 00:02:59,760
showing here on the screen to be ready

74
00:02:57,280 --> 00:03:01,840
for almost any kind of model

75
00:02:59,760 --> 00:03:05,040
so i spent a fair amount of time working

76
00:03:01,840 --> 00:03:07,680
on software for people to be able to do

77
00:03:05,040 --> 00:03:09,840
exploratory data analysis

78
00:03:07,680 --> 00:03:11,760
visualization

79
00:03:09,840 --> 00:03:15,120
summarization

80
00:03:11,760 --> 00:03:17,920
tasks like that with text data in a tidy

81
00:03:15,120 --> 00:03:19,040
format where we have one observation per

82
00:03:17,920 --> 00:03:22,159
row

83
00:03:19,040 --> 00:03:24,480
and i love using tidy data principles

84
00:03:22,159 --> 00:03:27,280
for text analysis

85
00:03:24,480 --> 00:03:29,680
especially during those exploratory

86
00:03:27,280 --> 00:03:32,640
phases of an analysis

87
00:03:29,680 --> 00:03:34,879
when it comes time to build a model

88
00:03:32,640 --> 00:03:38,000
often what the underlying

89
00:03:34,879 --> 00:03:40,480
mathematical implementation really needs

90
00:03:38,000 --> 00:03:43,040
is typically something like this

91
00:03:40,480 --> 00:03:44,959
which is a

92
00:03:43,040 --> 00:03:47,440
way to

93
00:03:44,959 --> 00:03:49,760
uh this particular representation is

94
00:03:47,440 --> 00:03:52,319
called the document term matrix so the

95
00:03:49,760 --> 00:03:53,840
exact representation may differ from

96
00:03:52,319 --> 00:03:55,760
what i've shown here

97
00:03:53,840 --> 00:03:58,400
what i have here is we're weighting

98
00:03:55,760 --> 00:04:02,640
things by counts so each each row in

99
00:03:58,400 --> 00:04:04,720
this matrix is a document each

100
00:04:02,640 --> 00:04:07,439
column is a is a word

101
00:04:04,720 --> 00:04:10,080
um a token and the numbers represent

102
00:04:07,439 --> 00:04:12,319
counts how many times does each document

103
00:04:10,080 --> 00:04:13,519
use each word you could weight it in a

104
00:04:12,319 --> 00:04:16,720
different way

105
00:04:13,519 --> 00:04:19,359
using say tf idf instead of counts

106
00:04:16,720 --> 00:04:20,799
or you might keep sequence information

107
00:04:19,359 --> 00:04:23,120
if you're interested in building a deep

108
00:04:20,799 --> 00:04:25,199
learning model but basically for all

109
00:04:23,120 --> 00:04:27,919
kinds of text modeling

110
00:04:25,199 --> 00:04:30,639
from simpler models like naive bayes

111
00:04:27,919 --> 00:04:33,199
models which work well for text

112
00:04:30,639 --> 00:04:35,680
to word embeddings to really the most

113
00:04:33,199 --> 00:04:38,320
state-of-the-art kind of work that's

114
00:04:35,680 --> 00:04:39,520
happening today like transformers for

115
00:04:38,320 --> 00:04:41,120
text data

116
00:04:39,520 --> 00:04:44,720
we have to heavily

117
00:04:41,120 --> 00:04:47,280
feature engineer and process language to

118
00:04:44,720 --> 00:04:49,360
get it to some kind of representation

119
00:04:47,280 --> 00:04:50,960
that's suitable for machine learning

120
00:04:49,360 --> 00:04:53,680
algorithms

121
00:04:50,960 --> 00:04:54,560
so i work on an open source framework in

122
00:04:53,680 --> 00:04:56,160
r

123
00:04:54,560 --> 00:04:58,880
for modeling and machine learning that's

124
00:04:56,160 --> 00:05:01,120
called tidy models and the examples that

125
00:04:58,880 --> 00:05:02,320
i'll be showing today use tidy models

126
00:05:01,120 --> 00:05:04,800
code

127
00:05:02,320 --> 00:05:07,360
some of the specific goals of the tidy

128
00:05:04,800 --> 00:05:10,639
models project are to provide a

129
00:05:07,360 --> 00:05:13,120
consistent flexible framework for real

130
00:05:10,639 --> 00:05:16,240
world modeling practitioners people who

131
00:05:13,120 --> 00:05:19,199
are you know doing their um

132
00:05:16,240 --> 00:05:21,360
that are dealing with real world data

133
00:05:19,199 --> 00:05:22,800
um those who are just starting out to

134
00:05:21,360 --> 00:05:24,479
those who are very experienced in

135
00:05:22,800 --> 00:05:27,120
modeling and

136
00:05:24,479 --> 00:05:28,880
the goal is to harmonize the

137
00:05:27,120 --> 00:05:32,240
heterogeneous

138
00:05:28,880 --> 00:05:35,360
interfaces that exist within r and to

139
00:05:32,240 --> 00:05:37,360
encourage good statistical practice

140
00:05:35,360 --> 00:05:38,880
i'm glad to get to show you some of what

141
00:05:37,360 --> 00:05:41,360
i work on

142
00:05:38,880 --> 00:05:43,840
and build and how we apply it to text

143
00:05:41,360 --> 00:05:47,120
modeling but a lot of what i will talk

144
00:05:43,840 --> 00:05:48,160
about today isn't very specific to tidy

145
00:05:47,120 --> 00:05:50,880
models

146
00:05:48,160 --> 00:05:52,800
or even to r i know this is an r user

147
00:05:50,880 --> 00:05:54,720
group but what we're going to talk about

148
00:05:52,800 --> 00:05:57,440
and focus on are

149
00:05:54,720 --> 00:06:00,479
is a little more conceptual and basic

150
00:05:57,440 --> 00:06:04,560
how do we transform text

151
00:06:00,479 --> 00:06:06,800
into predictors for machine learning

152
00:06:04,560 --> 00:06:08,800
i am excited though to talk about tidy

153
00:06:06,800 --> 00:06:11,280
models and tidy models if you have not

154
00:06:08,800 --> 00:06:13,440
used it before is a meta package

155
00:06:11,280 --> 00:06:16,319
in a similar way that the tidy verse is

156
00:06:13,440 --> 00:06:19,720
a meta package so if you've ever typed

157
00:06:16,319 --> 00:06:23,199
library tidy verse and then you've used

158
00:06:19,720 --> 00:06:25,680
ggplot2 for visualization

159
00:06:23,199 --> 00:06:29,680
d plier for data manipulation

160
00:06:25,680 --> 00:06:30,560
tidy models works in a similar way

161
00:06:29,680 --> 00:06:33,039
um

162
00:06:30,560 --> 00:06:36,000
there are different packages inside of

163
00:06:33,039 --> 00:06:37,280
it that are used for um

164
00:06:36,000 --> 00:06:39,840
different purposes

165
00:06:37,280 --> 00:06:43,440
so the pre-processing or the feature

166
00:06:39,840 --> 00:06:45,280
engineering is part of a broader model

167
00:06:43,440 --> 00:06:48,000
process

168
00:06:45,280 --> 00:06:51,120
you know it that process starts really

169
00:06:48,000 --> 00:06:52,000
with with exploratory data analysis

170
00:06:51,120 --> 00:06:54,080
um

171
00:06:52,000 --> 00:06:56,240
that helps us decide what kind of model

172
00:06:54,080 --> 00:06:59,280
we will build and then it comes to

173
00:06:56,240 --> 00:07:03,120
completion i think i would argue with

174
00:06:59,280 --> 00:07:06,479
model evaluation when you uh you you

175
00:07:03,120 --> 00:07:09,280
measure how well your model performed

176
00:07:06,479 --> 00:07:11,919
tidy models as a piece of software is

177
00:07:09,280 --> 00:07:14,479
made up of our packages each of which

178
00:07:11,919 --> 00:07:15,599
has a specific focus

179
00:07:14,479 --> 00:07:17,199
like

180
00:07:15,599 --> 00:07:20,800
our sample

181
00:07:17,199 --> 00:07:23,440
is for re-sampling uh data to be able to

182
00:07:20,800 --> 00:07:25,919
create um bootstrap free samples

183
00:07:23,440 --> 00:07:27,680
cross-validation resamples all different

184
00:07:25,919 --> 00:07:29,520
kinds of re-samples you might want to

185
00:07:27,680 --> 00:07:31,919
use to

186
00:07:29,520 --> 00:07:34,800
train and evaluate models

187
00:07:31,919 --> 00:07:38,560
the tune package is for hyper parameter

188
00:07:34,800 --> 00:07:40,800
tuning as you might guess from the name

189
00:07:38,560 --> 00:07:43,520
one of these packages is for feature

190
00:07:40,800 --> 00:07:45,680
engineering for a data preprocessing

191
00:07:43,520 --> 00:07:47,120
feature engineering and it is the one

192
00:07:45,680 --> 00:07:49,360
that is called

193
00:07:47,120 --> 00:07:52,720
recipes

194
00:07:49,360 --> 00:07:55,280
so in tidy models we capture this idea

195
00:07:52,720 --> 00:07:57,199
of data pre-processing and feature

196
00:07:55,280 --> 00:08:01,199
engineering in the concept of a

197
00:07:57,199 --> 00:08:03,360
pre-processing recipe that has steps

198
00:08:01,199 --> 00:08:05,360
so you choose

199
00:08:03,360 --> 00:08:06,319
ingredients

200
00:08:05,360 --> 00:08:07,599
or

201
00:08:06,319 --> 00:08:10,319
variables

202
00:08:07,599 --> 00:08:11,599
that you're going to use then you define

203
00:08:10,319 --> 00:08:13,680
the steps

204
00:08:11,599 --> 00:08:16,080
that go into your recipe

205
00:08:13,680 --> 00:08:18,319
then you prepare them using training

206
00:08:16,080 --> 00:08:21,520
data and then you can apply that to any

207
00:08:18,319 --> 00:08:23,039
data set like testing data or new data

208
00:08:21,520 --> 00:08:25,759
at prediction time

209
00:08:23,039 --> 00:08:28,319
so the variables or ingredients that we

210
00:08:25,759 --> 00:08:30,080
use in modeling come in all kinds of

211
00:08:28,319 --> 00:08:32,000
shapes and sizes

212
00:08:30,080 --> 00:08:34,560
including text data

213
00:08:32,000 --> 00:08:37,919
so some of the techniques and approaches

214
00:08:34,560 --> 00:08:39,200
that we use for pre-processing text data

215
00:08:37,919 --> 00:08:41,519
are the same

216
00:08:39,200 --> 00:08:44,480
um as for any other kind of data that

217
00:08:41,519 --> 00:08:46,640
you might use like non-text data uh

218
00:08:44,480 --> 00:08:48,480
numeric data categorical data some for

219
00:08:46,640 --> 00:08:51,040
some of it is the same

220
00:08:48,480 --> 00:08:53,040
but some of what you need to know to be

221
00:08:51,040 --> 00:08:55,120
able to do a good job

222
00:08:53,040 --> 00:08:58,080
of um

223
00:08:55,120 --> 00:09:00,800
in this process for text um is different

224
00:08:58,080 --> 00:09:03,680
and is specific to the nature of what

225
00:09:00,800 --> 00:09:05,279
language data is like

226
00:09:03,680 --> 00:09:08,480
and so i've written a book with my

227
00:09:05,279 --> 00:09:11,360
co-author emile vedfeldt on supervised

228
00:09:08,480 --> 00:09:14,720
machine learning for text analysis and r

229
00:09:11,360 --> 00:09:18,160
and fully the first third of the book

230
00:09:14,720 --> 00:09:20,800
focuses on how we transform

231
00:09:18,160 --> 00:09:23,600
the natural language that we have in

232
00:09:20,800 --> 00:09:25,920
text data into features for modeling

233
00:09:23,600 --> 00:09:28,800
the middle section is about how we use

234
00:09:25,920 --> 00:09:30,800
these features in

235
00:09:28,800 --> 00:09:33,360
simpler or more traditional machine

236
00:09:30,800 --> 00:09:35,839
learning models like regularized

237
00:09:33,360 --> 00:09:37,360
regression or support vector machines

238
00:09:35,839 --> 00:09:40,240
and um

239
00:09:37,360 --> 00:09:42,800
uh then the last third of the book

240
00:09:40,240 --> 00:09:46,000
talks about how we use um deep learning

241
00:09:42,800 --> 00:09:48,640
models with text data so deep learning

242
00:09:46,000 --> 00:09:51,279
models still require these kinds of

243
00:09:48,640 --> 00:09:54,480
transformations from natural language

244
00:09:51,279 --> 00:09:56,080
into um into features for

245
00:09:54,480 --> 00:09:58,880
um

246
00:09:56,080 --> 00:10:01,600
as input for these kinds of models but

247
00:09:58,880 --> 00:10:03,040
deep learning models are

248
00:10:01,600 --> 00:10:05,920
often able to

249
00:10:03,040 --> 00:10:07,920
inherently learn structure of features

250
00:10:05,920 --> 00:10:10,240
from text in ways that those

251
00:10:07,920 --> 00:10:12,800
more traditional or simpler machine

252
00:10:10,240 --> 00:10:15,279
learning models are not

253
00:10:12,800 --> 00:10:17,440
so this book is now complete and

254
00:10:15,279 --> 00:10:18,480
available as of this month as of

255
00:10:17,440 --> 00:10:20,560
november

256
00:10:18,480 --> 00:10:23,600
um folks are getting their first paper

257
00:10:20,560 --> 00:10:26,320
copies and also this book is available

258
00:10:23,600 --> 00:10:28,560
in its entirety at

259
00:10:26,320 --> 00:10:30,720
smalltar.com

260
00:10:28,560 --> 00:10:33,519
so if you're new to dealing with text

261
00:10:30,720 --> 00:10:35,760
data understanding these um these

262
00:10:33,519 --> 00:10:38,720
fundamental pre-processing approaches

263
00:10:35,760 --> 00:10:41,760
for text will set you up for being able

264
00:10:38,720 --> 00:10:43,839
to train effective models

265
00:10:41,760 --> 00:10:45,200
if you're really experienced with text

266
00:10:43,839 --> 00:10:47,680
data if you've dealt with it a lot

267
00:10:45,200 --> 00:10:48,959
already you've probably noticed like we

268
00:10:47,680 --> 00:10:51,360
have

269
00:10:48,959 --> 00:10:52,560
that um that the existing

270
00:10:51,360 --> 00:10:54,480
um

271
00:10:52,560 --> 00:10:58,000
you know resources or literature whether

272
00:10:54,480 --> 00:11:01,680
that's books or tutorials or blog posts

273
00:10:58,000 --> 00:11:02,880
um is is quite sparse when it comes to

274
00:11:01,680 --> 00:11:05,279
um

275
00:11:02,880 --> 00:11:07,600
detailed thoughtful explorations of how

276
00:11:05,279 --> 00:11:11,120
these pre-processing steps work

277
00:11:07,600 --> 00:11:14,000
and how choices made in these feature

278
00:11:11,120 --> 00:11:15,600
engineering steps impact our our model

279
00:11:14,000 --> 00:11:17,519
output

280
00:11:15,600 --> 00:11:20,000
so let's walk through

281
00:11:17,519 --> 00:11:21,360
several of some of these like basic

282
00:11:20,000 --> 00:11:23,360
um

283
00:11:21,360 --> 00:11:25,440
feature engineering approaches and how

284
00:11:23,360 --> 00:11:27,040
they work and what they do

285
00:11:25,440 --> 00:11:28,800
let's start out with

286
00:11:27,040 --> 00:11:31,920
tokenization

287
00:11:28,800 --> 00:11:33,839
so typically one of the first steps in

288
00:11:31,920 --> 00:11:36,800
transfer information from natural

289
00:11:33,839 --> 00:11:38,320
language to machine learning feature

290
00:11:36,800 --> 00:11:40,480
for

291
00:11:38,320 --> 00:11:44,480
really any kind of text analysis

292
00:11:40,480 --> 00:11:47,040
including exploratory data analysis

293
00:11:44,480 --> 00:11:49,040
or building a model anything

294
00:11:47,040 --> 00:11:52,320
is tokenization

295
00:11:49,040 --> 00:11:55,360
in tokenization we take an input some

296
00:11:52,320 --> 00:11:57,120
string some character vector and some

297
00:11:55,360 --> 00:11:59,440
kind of token type

298
00:11:57,120 --> 00:12:02,079
um some meaningful unit of text that

299
00:11:59,440 --> 00:12:05,600
we're interested in like like a word

300
00:12:02,079 --> 00:12:07,680
and we split the input into pieces into

301
00:12:05,600 --> 00:12:09,519
tokens that correspond to the type that

302
00:12:07,680 --> 00:12:12,480
we're interested in

303
00:12:09,519 --> 00:12:15,120
so most commonly the meaningful unit or

304
00:12:12,480 --> 00:12:17,839
type of token that we want to split text

305
00:12:15,120 --> 00:12:19,040
into units of is a word

306
00:12:17,839 --> 00:12:21,440
so this might seem you know

307
00:12:19,040 --> 00:12:24,480
straightforward or obvious but it turns

308
00:12:21,440 --> 00:12:28,160
out it's difficult to clearly define

309
00:12:24,480 --> 00:12:29,519
what a word is for many or even most

310
00:12:28,160 --> 00:12:30,880
languages

311
00:12:29,519 --> 00:12:34,000
so um

312
00:12:30,880 --> 00:12:35,839
so many languages do not use white space

313
00:12:34,000 --> 00:12:38,240
between words at all

314
00:12:35,839 --> 00:12:40,959
which um you know presents a challenge

315
00:12:38,240 --> 00:12:44,480
for tokenization um even languages that

316
00:12:40,959 --> 00:12:45,519
do use white space like like english and

317
00:12:44,480 --> 00:12:48,160
korean

318
00:12:45,519 --> 00:12:49,839
often have particular examples that are

319
00:12:48,160 --> 00:12:53,120
ambiguous

320
00:12:49,839 --> 00:12:55,040
like contractions in english like didn't

321
00:12:53,120 --> 00:12:56,800
which should be um

322
00:12:55,040 --> 00:12:59,200
uh you know maybe more accurately

323
00:12:56,800 --> 00:13:01,519
considered two words um the way the way

324
00:12:59,200 --> 00:13:04,079
particles are used in korean

325
00:13:01,519 --> 00:13:06,240
and how pronouns and negation words are

326
00:13:04,079 --> 00:13:08,000
written in romance languages like

327
00:13:06,240 --> 00:13:09,920
italian and french where they're stuck

328
00:13:08,000 --> 00:13:12,399
together and really maybe they should be

329
00:13:09,920 --> 00:13:13,839
considered two words

330
00:13:12,399 --> 00:13:15,519
once you have figured out what you're

331
00:13:13,839 --> 00:13:17,839
going to do and you make some choices

332
00:13:15,519 --> 00:13:20,480
and you tokenize your text then it's on

333
00:13:17,839 --> 00:13:23,839
its way to being able to be used

334
00:13:20,480 --> 00:13:26,880
in exploratory data analysis or

335
00:13:23,839 --> 00:13:28,399
unsupervised algorithms or as features

336
00:13:26,880 --> 00:13:30,320
for predictive modeling which is what

337
00:13:28,399 --> 00:13:32,720
we're talking about here and what these

338
00:13:30,320 --> 00:13:35,200
results show here so these results are

339
00:13:32,720 --> 00:13:38,000
from a regression model trained on

340
00:13:35,200 --> 00:13:40,639
descriptions of media

341
00:13:38,000 --> 00:13:42,079
from artwork in the tate collection in

342
00:13:40,639 --> 00:13:43,279
the uk

343
00:13:42,079 --> 00:13:45,199
so

344
00:13:43,279 --> 00:13:47,680
what we're predicting in what we are

345
00:13:45,199 --> 00:13:48,880
predicting is when

346
00:13:47,680 --> 00:13:52,320
what year

347
00:13:48,880 --> 00:13:53,120
was um a piece of art created based on

348
00:13:52,320 --> 00:13:56,320
the

349
00:13:53,120 --> 00:13:58,560
the medium that the artwork was created

350
00:13:56,320 --> 00:14:00,079
with and the medium is described with a

351
00:13:58,560 --> 00:14:02,079
little bit of text

352
00:14:00,079 --> 00:14:04,079
so we see here that artwork created

353
00:14:02,079 --> 00:14:07,440
using graphite

354
00:14:04,079 --> 00:14:09,279
watercolor and engraving was more likely

355
00:14:07,440 --> 00:14:10,959
to be created earlier

356
00:14:09,279 --> 00:14:13,360
that though that is more likely to come

357
00:14:10,959 --> 00:14:15,360
from older art and artwork that is

358
00:14:13,360 --> 00:14:17,839
created using

359
00:14:15,360 --> 00:14:17,839
photography

360
00:14:17,920 --> 00:14:22,880
screen point or sorry screen print

361
00:14:20,639 --> 00:14:24,720
screen printing

362
00:14:22,880 --> 00:14:27,760
and and dung

363
00:14:24,720 --> 00:14:29,920
and glitter um are more likely to be

364
00:14:27,760 --> 00:14:32,560
created later there this is more likely

365
00:14:29,920 --> 00:14:33,440
to come from contemporary art uh modern

366
00:14:32,560 --> 00:14:37,519
art

367
00:14:33,440 --> 00:14:38,399
so the the way that um we tokenize this

368
00:14:37,519 --> 00:14:39,440
text

369
00:14:38,399 --> 00:14:41,519
um

370
00:14:39,440 --> 00:14:43,839
you know we started with natural human

371
00:14:41,519 --> 00:14:47,519
generated texts of people writing out

372
00:14:43,839 --> 00:14:49,760
the descriptions of the the media

373
00:14:47,519 --> 00:14:50,959
that these art pieces of art were

374
00:14:49,760 --> 00:14:53,120
created with

375
00:14:50,959 --> 00:14:55,440
and the way we tokenized that natural

376
00:14:53,120 --> 00:14:57,760
human generated text that we started

377
00:14:55,440 --> 00:14:59,839
with has a big impact on what we learned

378
00:14:57,760 --> 00:15:00,720
from it if we tokenized in a different

379
00:14:59,839 --> 00:15:01,839
way

380
00:15:00,720 --> 00:15:04,880
um we would have gotten different

381
00:15:01,839 --> 00:15:05,839
results um in terms of performance like

382
00:15:04,880 --> 00:15:07,760
how

383
00:15:05,839 --> 00:15:10,320
accurately we were able to predict

384
00:15:07,760 --> 00:15:12,240
predict the year and also in terms of

385
00:15:10,320 --> 00:15:15,279
how we interpret the model like what is

386
00:15:12,240 --> 00:15:17,279
it that we're able to learn from it

387
00:15:15,279 --> 00:15:19,760
so this is one kind of tokenization to

388
00:15:17,279 --> 00:15:21,760
the single word but we also

389
00:15:19,760 --> 00:15:24,000
we all another way to tokenize instead

390
00:15:21,760 --> 00:15:27,440
of breaking up into single words or

391
00:15:24,000 --> 00:15:31,279
unigrams we can tokenize to n grams

392
00:15:27,440 --> 00:15:35,199
so an n gram is a continuous sequence of

393
00:15:31,279 --> 00:15:37,920
n items from a given sequence of texts

394
00:15:35,199 --> 00:15:41,680
so this shows that same piece of little

395
00:15:37,920 --> 00:15:44,880
bit of text i'm describing this animal

396
00:15:41,680 --> 00:15:47,440
divided up into bi-grams or n-grams of

397
00:15:44,880 --> 00:15:49,360
two tokens so notice how the words in

398
00:15:47,440 --> 00:15:51,519
the bi-grams overlap

399
00:15:49,360 --> 00:15:54,959
so the word collard

400
00:15:51,519 --> 00:15:56,000
appears in both of the first bigrams the

401
00:15:54,959 --> 00:15:58,480
collared

402
00:15:56,000 --> 00:16:00,800
collared peccary peccary also also

403
00:15:58,480 --> 00:16:03,920
referred referred to so engram

404
00:16:00,800 --> 00:16:07,040
tokenization slides along the text to

405
00:16:03,920 --> 00:16:11,120
create overlapping sets of tokens

406
00:16:07,040 --> 00:16:14,480
this shows trigrams for the same thing

407
00:16:11,120 --> 00:16:17,680
um so using unigrams um one word is

408
00:16:14,480 --> 00:16:20,560
faster and more efficient but we don't

409
00:16:17,680 --> 00:16:23,360
capture information about word order

410
00:16:20,560 --> 00:16:25,839
i'm using a higher value for n you know

411
00:16:23,360 --> 00:16:28,160
two or three or even more

412
00:16:25,839 --> 00:16:30,320
um keeps keeps this

413
00:16:28,160 --> 00:16:32,240
more complex information about word

414
00:16:30,320 --> 00:16:33,120
order and concepts

415
00:16:32,240 --> 00:16:36,160
um

416
00:16:33,120 --> 00:16:37,680
that are described in multi-word

417
00:16:36,160 --> 00:16:41,440
phrases

418
00:16:37,680 --> 00:16:43,839
but the vector space of tokens

419
00:16:41,440 --> 00:16:47,199
increases dramatically

420
00:16:43,839 --> 00:16:50,480
that corresponds to a reduction in token

421
00:16:47,199 --> 00:16:52,880
counts we don't count each token as very

422
00:16:50,480 --> 00:16:55,120
many times and that means depending on

423
00:16:52,880 --> 00:16:56,480
your particular data set

424
00:16:55,120 --> 00:16:58,160
you might not be able to get good

425
00:16:56,480 --> 00:17:00,399
results

426
00:16:58,160 --> 00:17:03,519
so combining different degrees of

427
00:17:00,399 --> 00:17:06,400
engrams can allow you to extract

428
00:17:03,519 --> 00:17:07,760
different levels of detail from text so

429
00:17:06,400 --> 00:17:09,679
unigrams

430
00:17:07,760 --> 00:17:11,439
can tell you which individual words have

431
00:17:09,679 --> 00:17:14,079
been used a lot of times

432
00:17:11,439 --> 00:17:15,760
um and some of those words might be

433
00:17:14,079 --> 00:17:18,319
overlooked

434
00:17:15,760 --> 00:17:19,919
in bigram or trigram crowns if they

435
00:17:18,319 --> 00:17:23,360
don't co-appear

436
00:17:19,919 --> 00:17:23,360
with other words as often

437
00:17:23,520 --> 00:17:28,960
this plot compares model performance for

438
00:17:26,720 --> 00:17:32,080
a lasso regression model predicting the

439
00:17:28,960 --> 00:17:34,640
year of supreme court opinions the

440
00:17:32,080 --> 00:17:37,840
united states supreme court opinions um

441
00:17:34,640 --> 00:17:39,679
with three different degrees of engrams

442
00:17:37,840 --> 00:17:42,880
what we're doing here is we are taking

443
00:17:39,679 --> 00:17:44,240
the text of the writings of the united

444
00:17:42,880 --> 00:17:45,440
states supreme court and we're

445
00:17:44,240 --> 00:17:47,120
predicting

446
00:17:45,440 --> 00:17:49,760
um

447
00:17:47,120 --> 00:17:51,840
when when did it when was that text

448
00:17:49,760 --> 00:17:54,080
written so can we predict how old a

449
00:17:51,840 --> 00:17:55,360
piece of text is from the contents of

450
00:17:54,080 --> 00:17:58,799
the text

451
00:17:55,360 --> 00:18:01,840
so holding uh the number of tokens

452
00:17:58,799 --> 00:18:04,480
constant at a thousand

453
00:18:01,840 --> 00:18:08,080
using unigrams alone

454
00:18:04,480 --> 00:18:10,160
performs best for this corpus of um

455
00:18:08,080 --> 00:18:11,280
opinions from the united states supreme

456
00:18:10,160 --> 00:18:13,600
court

457
00:18:11,280 --> 00:18:16,240
this is not always the case depending on

458
00:18:13,600 --> 00:18:18,799
the kind of model you use the data set

459
00:18:16,240 --> 00:18:21,679
itself we might see the best performance

460
00:18:18,799 --> 00:18:22,640
combining unigrams and diagrams or maybe

461
00:18:21,679 --> 00:18:23,679
some other

462
00:18:22,640 --> 00:18:25,760
option

463
00:18:23,679 --> 00:18:28,720
in this case if we wanted to incorporate

464
00:18:25,760 --> 00:18:30,480
some of that more complex information

465
00:18:28,720 --> 00:18:34,000
that we have in the bigrams and the

466
00:18:30,480 --> 00:18:36,799
trigrams we probably would need to

467
00:18:34,000 --> 00:18:40,160
increase the number of

468
00:18:36,799 --> 00:18:43,039
tokens in the model quite a bit

469
00:18:40,160 --> 00:18:46,000
so keep in mind when you look at results

470
00:18:43,039 --> 00:18:48,960
like these that identifying engrams is

471
00:18:46,000 --> 00:18:51,039
computationally expensive

472
00:18:48,960 --> 00:18:51,760
um and this is especially compared to

473
00:18:51,039 --> 00:18:54,640
the

474
00:18:51,760 --> 00:18:57,600
amount of like a model the improvement

475
00:18:54,640 --> 00:18:58,720
in model performance that we often see

476
00:18:57,600 --> 00:19:00,640
like if we

477
00:18:58,720 --> 00:19:03,600
if we see some you know modest

478
00:19:00,640 --> 00:19:05,760
improvement by adding in bigrams it's

479
00:19:03,600 --> 00:19:08,640
important to keep in mind how much

480
00:19:05,760 --> 00:19:11,039
improvement we see relative to

481
00:19:08,640 --> 00:19:13,200
how long it takes to

482
00:19:11,039 --> 00:19:15,840
identify bigrams and then train that

483
00:19:13,200 --> 00:19:18,480
model so for example for this data set

484
00:19:15,840 --> 00:19:20,799
of supreme court opinions where we held

485
00:19:18,480 --> 00:19:24,480
the number of tokens constant so the

486
00:19:20,799 --> 00:19:25,919
model training had the same number of

487
00:19:24,480 --> 00:19:27,120
tokens in it

488
00:19:25,919 --> 00:19:31,280
um

489
00:19:27,120 --> 00:19:33,280
using bigrams plus unigrams takes twice

490
00:19:31,280 --> 00:19:35,200
as long to train

491
00:19:33,280 --> 00:19:38,240
to do the feature engineering and the

492
00:19:35,200 --> 00:19:42,000
training than only unigrams and adding

493
00:19:38,240 --> 00:19:44,320
in trigrams as well takes almost five

494
00:19:42,000 --> 00:19:46,799
times as long as training on unigrams

495
00:19:44,320 --> 00:19:50,640
alone so this is a computationally

496
00:19:46,799 --> 00:19:50,640
expensive thing to do

497
00:19:51,760 --> 00:19:56,559
going in the other direction

498
00:19:53,760 --> 00:19:58,960
um we can tokenize to units smaller than

499
00:19:56,559 --> 00:20:00,880
words so like these are what are called

500
00:19:58,960 --> 00:20:03,280
character shingles

501
00:20:00,880 --> 00:20:05,440
so um we take words

502
00:20:03,280 --> 00:20:08,480
the collared peccary

503
00:20:05,440 --> 00:20:10,880
and we can instead of looking at words

504
00:20:08,480 --> 00:20:13,120
we can go down and look at sub word

505
00:20:10,880 --> 00:20:16,000
information there's multiple different

506
00:20:13,120 --> 00:20:17,440
ways to break words up into sub words

507
00:20:16,000 --> 00:20:18,480
that are appropriate for machine

508
00:20:17,440 --> 00:20:20,880
learning

509
00:20:18,480 --> 00:20:23,440
and often these kinds of um approaches

510
00:20:20,880 --> 00:20:25,200
or algorithms have the benefit of being

511
00:20:23,440 --> 00:20:28,159
able to encode

512
00:20:25,200 --> 00:20:29,039
unknown uh or new words

513
00:20:28,159 --> 00:20:31,360
um

514
00:20:29,039 --> 00:20:33,760
at prediction time so when it when it's

515
00:20:31,360 --> 00:20:36,400
time to make a prediction on new data

516
00:20:33,760 --> 00:20:39,200
it's not it's not uncommon for there to

517
00:20:36,400 --> 00:20:41,200
be new vocabulary words at that time and

518
00:20:39,200 --> 00:20:42,640
if we didn't see them in the training

519
00:20:41,200 --> 00:20:45,600
data you know what are we going to do

520
00:20:42,640 --> 00:20:49,360
about those new words when we um train

521
00:20:45,600 --> 00:20:52,000
using sub word information often we can

522
00:20:49,360 --> 00:20:52,880
handle those new words if we saw the sub

523
00:20:52,000 --> 00:20:54,960
word

524
00:20:52,880 --> 00:20:55,760
in our training data set

525
00:20:54,960 --> 00:20:58,400
so

526
00:20:55,760 --> 00:21:01,440
using this kind of sub word information

527
00:20:58,400 --> 00:21:04,720
is a way to incorporate morphological

528
00:21:01,440 --> 00:21:06,240
sequences into our models of you know

529
00:21:04,720 --> 00:21:08,080
various kinds of

530
00:21:06,240 --> 00:21:09,280
this is something that applies to

531
00:21:08,080 --> 00:21:11,919
uh you know

532
00:21:09,280 --> 00:21:13,360
various languages not just english

533
00:21:11,919 --> 00:21:16,240
so these results are for a

534
00:21:13,360 --> 00:21:20,000
classification model with a data set of

535
00:21:16,240 --> 00:21:22,960
very short texts it's just the names of

536
00:21:20,000 --> 00:21:24,240
post offices in the united states so

537
00:21:22,960 --> 00:21:27,120
super short

538
00:21:24,240 --> 00:21:28,799
and the goal of the model was to predict

539
00:21:27,120 --> 00:21:31,520
did the um

540
00:21:28,799 --> 00:21:32,880
did the po is the post office located in

541
00:21:31,520 --> 00:21:35,280
hawaii

542
00:21:32,880 --> 00:21:37,280
in the middle of the pacific ocean or is

543
00:21:35,280 --> 00:21:38,159
it located in the rest of the united

544
00:21:37,280 --> 00:21:41,360
states

545
00:21:38,159 --> 00:21:42,159
so i created features for the model that

546
00:21:41,360 --> 00:21:46,480
are

547
00:21:42,159 --> 00:21:49,200
sub words of these post office names um

548
00:21:46,480 --> 00:21:53,120
and we end up learning that the names

549
00:21:49,200 --> 00:21:54,240
that start with h and p or contain that

550
00:21:53,120 --> 00:21:57,440
ale

551
00:21:54,240 --> 00:22:00,240
sub word are more likely to be in hawaii

552
00:21:57,440 --> 00:22:03,360
and the sub words a and d

553
00:22:00,240 --> 00:22:04,559
and ri and ing

554
00:22:03,360 --> 00:22:06,480
are are

555
00:22:04,559 --> 00:22:09,360
more likely to come from the post office

556
00:22:06,480 --> 00:22:11,440
that are outside of hawaii

557
00:22:09,360 --> 00:22:14,480
so this is an example of how we

558
00:22:11,440 --> 00:22:17,280
tokenized differently and we're able to

559
00:22:14,480 --> 00:22:18,559
learn something

560
00:22:17,280 --> 00:22:19,600
new we're able to learn something

561
00:22:18,559 --> 00:22:21,840
different

562
00:22:19,600 --> 00:22:24,320
so in tidy models we collect all these

563
00:22:21,840 --> 00:22:26,880
kinds of decisions about tokenization

564
00:22:24,320 --> 00:22:29,919
and code that looks like this

565
00:22:26,880 --> 00:22:30,799
so we start with a recipe that specifies

566
00:22:29,919 --> 00:22:33,760
what

567
00:22:30,799 --> 00:22:35,679
variables or ingredients that we'll use

568
00:22:33,760 --> 00:22:38,640
and then we define these preprocessing

569
00:22:35,679 --> 00:22:41,600
steps so even at this first

570
00:22:38,640 --> 00:22:44,159
and arguably you know simple and basic

571
00:22:41,600 --> 00:22:47,120
step the choices that we make affect our

572
00:22:44,159 --> 00:22:50,400
modeling results in a big way

573
00:22:47,120 --> 00:22:52,080
the next pre-processing um step that i

574
00:22:50,400 --> 00:22:53,600
want to talk about is

575
00:22:52,080 --> 00:22:56,240
stop words

576
00:22:53,600 --> 00:23:00,320
so once we have split text

577
00:22:56,240 --> 00:23:03,360
into um tokens we often find that not

578
00:23:00,320 --> 00:23:06,720
all words carry the same amount of

579
00:23:03,360 --> 00:23:09,520
information if maybe any information at

580
00:23:06,720 --> 00:23:11,840
all actually for a machine learning task

581
00:23:09,520 --> 00:23:13,919
so common words that carry little or

582
00:23:11,840 --> 00:23:16,080
perhaps no meaningful information are

583
00:23:13,919 --> 00:23:18,880
called stuff words

584
00:23:16,080 --> 00:23:20,400
so this is one of the stop word lists

585
00:23:18,880 --> 00:23:22,720
that's available for

586
00:23:20,400 --> 00:23:24,960
korean so it's common advice and

587
00:23:22,720 --> 00:23:28,000
practice to um

588
00:23:24,960 --> 00:23:31,200
to say hey just remove just remove

589
00:23:28,000 --> 00:23:31,919
remove these stop words for um for a lot

590
00:23:31,200 --> 00:23:35,039
of

591
00:23:31,919 --> 00:23:36,640
natural language processing tasks

592
00:23:35,039 --> 00:23:39,600
what i'm showing here

593
00:23:36,640 --> 00:23:41,919
is the entirety of one of the shorter

594
00:23:39,600 --> 00:23:44,320
english stop word lists that's used

595
00:23:41,919 --> 00:23:45,120
really broadly so you know it's words

596
00:23:44,320 --> 00:23:46,159
like

597
00:23:45,120 --> 00:23:47,200
um

598
00:23:46,159 --> 00:23:52,080
i

599
00:23:47,200 --> 00:23:54,400
me my pronouns conjunctions and of the

600
00:23:52,080 --> 00:23:57,679
and these are you know very common words

601
00:23:54,400 --> 00:24:00,400
that are not considered super important

602
00:23:57,679 --> 00:24:04,400
the decision though to just remove stop

603
00:24:00,400 --> 00:24:07,279
words is often more involved and

604
00:24:04,400 --> 00:24:08,960
perhaps more fraught than what you'll

605
00:24:07,279 --> 00:24:12,000
than what you'll find reflected in a lot

606
00:24:08,960 --> 00:24:15,840
of resources that are out there

607
00:24:12,000 --> 00:24:18,880
so almost all the time uh real world nlp

608
00:24:15,840 --> 00:24:20,000
practitioners use pre-made stop word

609
00:24:18,880 --> 00:24:22,400
lists

610
00:24:20,000 --> 00:24:26,400
so this plot visualizes

611
00:24:22,400 --> 00:24:28,720
set intersections for three common stop

612
00:24:26,400 --> 00:24:31,200
word lists in english

613
00:24:28,720 --> 00:24:32,960
in what is called an upset plot

614
00:24:31,200 --> 00:24:37,440
so the three lists are called the

615
00:24:32,960 --> 00:24:40,159
snowball list smart and the iso list

616
00:24:37,440 --> 00:24:41,840
so um you can see the the lengths of the

617
00:24:40,159 --> 00:24:43,679
list are represented by the length of

618
00:24:41,840 --> 00:24:46,400
the bars and then we see the

619
00:24:43,679 --> 00:24:48,799
intersections which words are in common

620
00:24:46,400 --> 00:24:51,760
on these lists by the by the vertical

621
00:24:48,799 --> 00:24:52,960
bars so the lengths of the list are

622
00:24:51,760 --> 00:24:55,760
quite different

623
00:24:52,960 --> 00:24:58,320
and also notice they don't all contain

624
00:24:55,760 --> 00:25:00,720
the same sets of words

625
00:24:58,320 --> 00:25:03,120
the important thing to remember about

626
00:25:00,720 --> 00:25:04,240
stop word lexicons

627
00:25:03,120 --> 00:25:06,880
is that they

628
00:25:04,240 --> 00:25:10,000
they are not created in some

629
00:25:06,880 --> 00:25:11,600
neutral perfect setting

630
00:25:10,000 --> 00:25:12,799
but instead

631
00:25:11,600 --> 00:25:16,080
they are

632
00:25:12,799 --> 00:25:16,960
um they are context specific

633
00:25:16,080 --> 00:25:20,480
um

634
00:25:16,960 --> 00:25:23,600
they they can be biased uh both of these

635
00:25:20,480 --> 00:25:27,039
things are true because they are lists

636
00:25:23,600 --> 00:25:28,559
created from large data sets of language

637
00:25:27,039 --> 00:25:30,640
so they reflect

638
00:25:28,559 --> 00:25:32,159
the characteristics of the data used in

639
00:25:30,640 --> 00:25:34,000
their creation

640
00:25:32,159 --> 00:25:37,440
so this is

641
00:25:34,000 --> 00:25:40,559
the ten words that are in the english

642
00:25:37,440 --> 00:25:43,039
language smart lexicon but not in the

643
00:25:40,559 --> 00:25:45,600
english snowball lexicon

644
00:25:43,039 --> 00:25:47,520
so notice that they're all contractions

645
00:25:45,600 --> 00:25:50,000
but that's not because the snowball

646
00:25:47,520 --> 00:25:52,240
exchange doesn't include contractions it

647
00:25:50,000 --> 00:25:54,400
has a lot of them

648
00:25:52,240 --> 00:25:55,440
also notice that it has

649
00:25:54,400 --> 00:25:56,960
um

650
00:25:55,440 --> 00:25:58,960
that she's

651
00:25:56,960 --> 00:26:01,360
is on this list and so that means that

652
00:25:58,960 --> 00:26:03,360
that list has he's but it does not have

653
00:26:01,360 --> 00:26:05,600
the list she's

654
00:26:03,360 --> 00:26:07,679
so this is an example of that of that

655
00:26:05,600 --> 00:26:11,440
bias i mentioned that occurs because

656
00:26:07,679 --> 00:26:13,520
these lists are created from large data

657
00:26:11,440 --> 00:26:16,400
sets of text

658
00:26:13,520 --> 00:26:19,760
lexicon creators look at the most

659
00:26:16,400 --> 00:26:22,480
frequent words in some big corpus of

660
00:26:19,760 --> 00:26:24,720
language they make a cut off

661
00:26:22,480 --> 00:26:26,080
and then some decisions about what to

662
00:26:24,720 --> 00:26:28,400
include or

663
00:26:26,080 --> 00:26:29,919
exclude uh you know

664
00:26:28,400 --> 00:26:33,840
based you know on the list that they

665
00:26:29,919 --> 00:26:36,799
have and you end up here so because

666
00:26:33,840 --> 00:26:39,760
um in many large data sets of language

667
00:26:36,799 --> 00:26:42,720
you have more representat more you know

668
00:26:39,760 --> 00:26:45,679
representation of um

669
00:26:42,720 --> 00:26:47,440
of of of men um you end up with a

670
00:26:45,679 --> 00:26:51,840
situation like this where a stop word

671
00:26:47,440 --> 00:26:54,799
list will have he's but not she's

672
00:26:51,840 --> 00:26:57,840
so many decisions when it comes to

673
00:26:54,799 --> 00:27:01,120
modeling or analysis with language

674
00:26:57,840 --> 00:27:03,919
we as practitioners have to decide

675
00:27:01,120 --> 00:27:05,279
what is appropriate for our particular

676
00:27:03,919 --> 00:27:07,679
domain

677
00:27:05,279 --> 00:27:10,720
it turns out this is even true when it

678
00:27:07,679 --> 00:27:12,960
comes to picking a stop word list

679
00:27:10,720 --> 00:27:14,720
so in tidy models we can implement a

680
00:27:12,960 --> 00:27:17,120
pre-processing step

681
00:27:14,720 --> 00:27:19,200
like removing stop words

682
00:27:17,120 --> 00:27:21,760
by adding an additional step to our

683
00:27:19,200 --> 00:27:24,720
recipe so first we specified what

684
00:27:21,760 --> 00:27:28,000
variables we would use then we tokenized

685
00:27:24,720 --> 00:27:30,720
the text and now we are removing

686
00:27:28,000 --> 00:27:32,640
stop words here using just the default

687
00:27:30,720 --> 00:27:35,360
step since we are not passing in any

688
00:27:32,640 --> 00:27:37,600
other arguments we could though

689
00:27:35,360 --> 00:27:39,919
use a non-default step or even a custom

690
00:27:37,600 --> 00:27:42,399
list if that was most appropriate to our

691
00:27:39,919 --> 00:27:42,399
domain

692
00:27:42,880 --> 00:27:48,960
this plot compares the model performance

693
00:27:46,799 --> 00:27:50,159
for predicting the year

694
00:27:48,960 --> 00:27:52,399
of um

695
00:27:50,159 --> 00:27:55,039
that same data set of

696
00:27:52,399 --> 00:27:58,240
supreme court opinions with three

697
00:27:55,039 --> 00:27:59,279
different stopward lexicons of different

698
00:27:58,240 --> 00:28:02,080
lengths

699
00:27:59,279 --> 00:28:04,480
so the snowball lexicon contains the

700
00:28:02,080 --> 00:28:06,960
smallest number of words and in this

701
00:28:04,480 --> 00:28:10,320
case it results in the best performance

702
00:28:06,960 --> 00:28:12,799
so removing fewer stop words results in

703
00:28:10,320 --> 00:28:16,399
the best performance here so this

704
00:28:12,799 --> 00:28:19,440
specific result is not generalizable to

705
00:28:16,399 --> 00:28:22,080
all data sets and contexts but the fact

706
00:28:19,440 --> 00:28:23,919
that removing different sets of stop

707
00:28:22,080 --> 00:28:26,799
words can have noticeably different

708
00:28:23,919 --> 00:28:29,919
effects on your model um that is quite

709
00:28:26,799 --> 00:28:33,200
transferable so the only way to know

710
00:28:29,919 --> 00:28:35,360
what is the best thing to do is to try

711
00:28:33,200 --> 00:28:37,600
several options and see so machine

712
00:28:35,360 --> 00:28:39,279
learning in general right is a is a

713
00:28:37,600 --> 00:28:41,360
empirical

714
00:28:39,279 --> 00:28:42,480
this is an empirical field right like we

715
00:28:41,360 --> 00:28:45,679
don't know

716
00:28:42,480 --> 00:28:47,440
we don't often have reasons a priori to

717
00:28:45,679 --> 00:28:49,919
know what will be the best thing to do

718
00:28:47,440 --> 00:28:52,240
and so typically we have to

719
00:28:49,919 --> 00:28:54,960
try a different option to see what will

720
00:28:52,240 --> 00:28:54,960
be the best thing

721
00:28:55,120 --> 00:28:58,799
all right then the the third

722
00:28:57,450 --> 00:29:01,039
[Music]

723
00:28:58,799 --> 00:29:02,080
pre-processing step that i want to talk

724
00:29:01,039 --> 00:29:04,000
about

725
00:29:02,080 --> 00:29:05,279
for text is

726
00:29:04,000 --> 00:29:07,760
stemming

727
00:29:05,279 --> 00:29:10,640
so when we deal with text often

728
00:29:07,760 --> 00:29:14,399
documents contain different versions of

729
00:29:10,640 --> 00:29:15,440
one base word um often called a stem

730
00:29:14,399 --> 00:29:18,559
so what

731
00:29:15,440 --> 00:29:20,000
if say for an english example say if we

732
00:29:18,559 --> 00:29:22,000
aren't interested in the difference

733
00:29:20,000 --> 00:29:23,120
between animals

734
00:29:22,000 --> 00:29:24,480
plural

735
00:29:23,120 --> 00:29:26,480
and animal

736
00:29:24,480 --> 00:29:27,919
singular and we want to treat them both

737
00:29:26,480 --> 00:29:31,279
together

738
00:29:27,919 --> 00:29:33,200
so that idea is at the heart of stemming

739
00:29:31,279 --> 00:29:37,279
so there's no one

740
00:29:33,200 --> 00:29:40,320
right way or correct way to stem text so

741
00:29:37,279 --> 00:29:42,480
this plot shows three approaches for

742
00:29:40,320 --> 00:29:44,880
stemming in english

743
00:29:42,480 --> 00:29:46,240
starting from hey let's just remove a

744
00:29:44,880 --> 00:29:49,039
final s

745
00:29:46,240 --> 00:29:51,760
um to more complex rules about plural

746
00:29:49,039 --> 00:29:54,480
handling plural endings uh that middle

747
00:29:51,760 --> 00:29:57,520
one it is called the s summer

748
00:29:54,480 --> 00:29:58,640
um it's a set of it's like a little set

749
00:29:57,520 --> 00:30:01,360
of rules

750
00:29:58,640 --> 00:30:02,640
and and that last one is the best known

751
00:30:01,360 --> 00:30:04,960
one probably the best-known

752
00:30:02,640 --> 00:30:07,279
implementation of stemming in english

753
00:30:04,960 --> 00:30:09,919
called the porter algorithm

754
00:30:07,279 --> 00:30:12,559
so you can see here that porter stemming

755
00:30:09,919 --> 00:30:14,880
is the most different from the other two

756
00:30:12,559 --> 00:30:16,799
in the top 20 words here from the data

757
00:30:14,880 --> 00:30:19,840
set of animal descriptions that i've

758
00:30:16,799 --> 00:30:21,520
been using we see how the word species

759
00:30:19,840 --> 00:30:24,240
was treated differently

760
00:30:21,520 --> 00:30:25,600
animal predator

761
00:30:24,240 --> 00:30:28,240
this sort of

762
00:30:25,600 --> 00:30:31,440
collection of words

763
00:30:28,240 --> 00:30:33,200
live living life lives that was treated

764
00:30:31,440 --> 00:30:36,240
differently

765
00:30:33,200 --> 00:30:38,960
so practitioners are typically

766
00:30:36,240 --> 00:30:40,559
interested in stemming text data because

767
00:30:38,960 --> 00:30:43,679
it buckets

768
00:30:40,559 --> 00:30:46,960
tokens together that we believe

769
00:30:43,679 --> 00:30:51,600
belong belong together in in a way that

770
00:30:46,960 --> 00:30:53,360
we understand that as um as human users

771
00:30:51,600 --> 00:30:56,799
of language

772
00:30:53,360 --> 00:30:58,320
so we can use uh approaches like this

773
00:30:56,799 --> 00:31:01,519
which um

774
00:30:58,320 --> 00:31:04,159
which are pretty um like step-by-step

775
00:31:01,519 --> 00:31:07,039
rules based this is typically called

776
00:31:04,159 --> 00:31:09,600
stemming um or and it's fairly

777
00:31:07,039 --> 00:31:12,080
algorithmic in nature like um

778
00:31:09,600 --> 00:31:14,720
first do this then do this then do this

779
00:31:12,080 --> 00:31:15,519
or you can use lemmatization

780
00:31:14,720 --> 00:31:17,840
um

781
00:31:15,519 --> 00:31:19,440
which is usually based on large

782
00:31:17,840 --> 00:31:23,519
dictionaries

783
00:31:19,440 --> 00:31:26,559
of words and it incorporates uh like a

784
00:31:23,519 --> 00:31:28,559
linguistic understanding of what words

785
00:31:26,559 --> 00:31:31,120
belong together

786
00:31:28,559 --> 00:31:34,960
so most of the existing approaches for

787
00:31:31,120 --> 00:31:35,919
this kind of tax task in korean

788
00:31:34,960 --> 00:31:38,559
are

789
00:31:35,919 --> 00:31:41,279
are limited lemmatizers based on these

790
00:31:38,559 --> 00:31:42,480
dictionaries and that are trained

791
00:31:41,279 --> 00:31:45,679
um

792
00:31:42,480 --> 00:31:47,600
using large data sets of language

793
00:31:45,679 --> 00:31:49,039
so this seems like it's going to be a

794
00:31:47,600 --> 00:31:50,559
helpful thing to do

795
00:31:49,039 --> 00:31:53,360
when you hear about this you're like oh

796
00:31:50,559 --> 00:31:54,880
yeah sounds sounds good sounds smart

797
00:31:53,360 --> 00:31:56,919
especially because

798
00:31:54,880 --> 00:32:00,480
with text data we are typically

799
00:31:56,919 --> 00:32:02,320
overwhelmed with features with um tokens

800
00:32:00,480 --> 00:32:04,080
with numbers of tokens

801
00:32:02,320 --> 00:32:05,840
like we this is typically the situation

802
00:32:04,080 --> 00:32:09,440
when we're dealing with text data text

803
00:32:05,840 --> 00:32:12,080
data so here we have this um

804
00:32:09,440 --> 00:32:16,080
this and these animal description data

805
00:32:12,080 --> 00:32:18,240
and i made a matrix representation of it

806
00:32:16,080 --> 00:32:20,159
like we would typically use in some

807
00:32:18,240 --> 00:32:24,000
machine learning algorithm

808
00:32:20,159 --> 00:32:25,200
and look how many features there are

809
00:32:24,000 --> 00:32:28,880
16

810
00:32:25,200 --> 00:32:29,679
000 almost 17 000 features

811
00:32:28,880 --> 00:32:31,600
um

812
00:32:29,679 --> 00:32:33,519
that that's the number of features that

813
00:32:31,600 --> 00:32:36,640
would be going into

814
00:32:33,519 --> 00:32:39,919
the um the model look at the sparsity

815
00:32:36,640 --> 00:32:43,440
98 percent sparse that's high very

816
00:32:39,919 --> 00:32:45,360
sparse data so this is the sparsity of

817
00:32:43,440 --> 00:32:47,760
the data that will go into the machine

818
00:32:45,360 --> 00:32:51,600
learning algorithm to build our

819
00:32:47,760 --> 00:32:53,760
supervised machine learning model

820
00:32:51,600 --> 00:32:56,480
if we stem the words

821
00:32:53,760 --> 00:33:00,159
if i use here an approach for stemming

822
00:32:56,480 --> 00:33:02,240
we reduce the number of word features by

823
00:33:00,159 --> 00:33:04,640
many thousands

824
00:33:02,240 --> 00:33:06,480
the sparsity unfortunately did not

825
00:33:04,640 --> 00:33:09,519
change as much but we reduced the number

826
00:33:06,480 --> 00:33:11,600
of features by a lot by bucketing those

827
00:33:09,519 --> 00:33:12,720
words together that our stemming

828
00:33:11,600 --> 00:33:15,200
algorithm

829
00:33:12,720 --> 00:33:16,880
um say belong together so you know

830
00:33:15,200 --> 00:33:19,519
common sense says

831
00:33:16,880 --> 00:33:21,679
reducing the number of words features

832
00:33:19,519 --> 00:33:22,720
so dramatically is going to

833
00:33:21,679 --> 00:33:24,880
perform

834
00:33:22,720 --> 00:33:26,640
improve the performance of our machine

835
00:33:24,880 --> 00:33:29,039
learning model

836
00:33:26,640 --> 00:33:32,559
but that is that does assume that we

837
00:33:29,039 --> 00:33:35,039
have not lost any important information

838
00:33:32,559 --> 00:33:37,120
by by stemming

839
00:33:35,039 --> 00:33:39,279
and it turns out that stemming or

840
00:33:37,120 --> 00:33:41,440
limitization

841
00:33:39,279 --> 00:33:44,399
can often be very helpful in some

842
00:33:41,440 --> 00:33:45,519
contexts but the typical algorithms used

843
00:33:44,399 --> 00:33:48,320
for these

844
00:33:45,519 --> 00:33:51,039
um are somewhat aggressive

845
00:33:48,320 --> 00:33:52,399
and they have been built to favor

846
00:33:51,039 --> 00:33:53,840
sensitivity

847
00:33:52,399 --> 00:33:55,919
or recall

848
00:33:53,840 --> 00:33:56,880
or the true positive rate

849
00:33:55,919 --> 00:33:59,279
and

850
00:33:56,880 --> 00:34:01,760
this is at the expense of the

851
00:33:59,279 --> 00:34:04,640
specificity or the precision or the true

852
00:34:01,760 --> 00:34:07,200
negative rate so in a supervised machine

853
00:34:04,640 --> 00:34:09,679
learning context what this does is this

854
00:34:07,200 --> 00:34:11,359
affects a mod a model's um

855
00:34:09,679 --> 00:34:13,839
positive predictive

856
00:34:11,359 --> 00:34:16,399
value the pres the precision or its

857
00:34:13,839 --> 00:34:19,200
ability to um

858
00:34:16,399 --> 00:34:20,560
to not incorrectly label true negatives

859
00:34:19,200 --> 00:34:22,800
as positive

860
00:34:20,560 --> 00:34:25,760
i hope i got that right um

861
00:34:22,800 --> 00:34:28,960
so you know to make this more concrete

862
00:34:25,760 --> 00:34:31,359
stemming can increase a model's ability

863
00:34:28,960 --> 00:34:33,760
to find the positive examples

864
00:34:31,359 --> 00:34:36,320
of say the animal descriptions that are

865
00:34:33,760 --> 00:34:38,399
associated with say a certain diet if

866
00:34:36,320 --> 00:34:40,720
that's what we're modeling however if

867
00:34:38,399 --> 00:34:43,200
text is over stemmed

868
00:34:40,720 --> 00:34:45,520
the resulting model loses its ability to

869
00:34:43,200 --> 00:34:47,200
label the negative examples

870
00:34:45,520 --> 00:34:49,760
say the descriptions that are not about

871
00:34:47,200 --> 00:34:52,240
that diet that's what we're looking for

872
00:34:49,760 --> 00:34:54,960
and this can be a real challenge when

873
00:34:52,240 --> 00:34:57,200
training models with text data kind of

874
00:34:54,960 --> 00:34:59,359
finding that that balance there because

875
00:34:57,200 --> 00:35:01,280
often we don't we often we don't have a

876
00:34:59,359 --> 00:35:05,119
dial that we can change on these

877
00:35:01,280 --> 00:35:08,400
stemming on these stemming algorithms

878
00:35:05,119 --> 00:35:10,800
so even just very basic pre-processing

879
00:35:08,400 --> 00:35:13,200
for text like what i'm showing here in

880
00:35:10,800 --> 00:35:15,280
this feature engineering recipe can be

881
00:35:13,200 --> 00:35:17,440
computationally expensive

882
00:35:15,280 --> 00:35:20,079
and the choices that a practitioner

883
00:35:17,440 --> 00:35:24,400
makes like whether or not to remove stop

884
00:35:20,079 --> 00:35:27,599
words or to stem text can have dramatic

885
00:35:24,400 --> 00:35:30,320
impact on how machine learning models

886
00:35:27,599 --> 00:35:32,320
of all kinds perform whether those are

887
00:35:30,320 --> 00:35:34,000
simpler models

888
00:35:32,320 --> 00:35:36,000
more traditional machine learning models

889
00:35:34,000 --> 00:35:39,040
or deep learning models

890
00:35:36,000 --> 00:35:42,560
what this means is that

891
00:35:39,040 --> 00:35:45,520
the the price the prioritization that we

892
00:35:42,560 --> 00:35:47,920
as practitioners give to like learning

893
00:35:45,520 --> 00:35:50,079
teaching and writing about feature

894
00:35:47,920 --> 00:35:53,040
engineering steps for text really

895
00:35:50,079 --> 00:35:56,320
contributes to better more robust

896
00:35:53,040 --> 00:35:58,560
statistical practice in our field

897
00:35:56,320 --> 00:36:01,359
i mentioned before the sparsity of text

898
00:35:58,560 --> 00:36:04,079
data and i want to come back to that

899
00:36:01,359 --> 00:36:06,720
because it is one of text data's really

900
00:36:04,079 --> 00:36:08,480
defining characteristics

901
00:36:06,720 --> 00:36:10,400
because of just how

902
00:36:08,480 --> 00:36:14,320
language works

903
00:36:10,400 --> 00:36:16,640
we use um a few words a lot of times

904
00:36:14,320 --> 00:36:19,599
and then a lot of words only just a

905
00:36:16,640 --> 00:36:22,640
couple of times only a few a few times

906
00:36:19,599 --> 00:36:25,119
and with a real set of natural language

907
00:36:22,640 --> 00:36:27,520
you end up with relationships that look

908
00:36:25,119 --> 00:36:30,640
like this that look like these plots in

909
00:36:27,520 --> 00:36:32,400
terms of how the sparsity changes as you

910
00:36:30,640 --> 00:36:35,680
add more documents

911
00:36:32,400 --> 00:36:38,320
and more unique words to a corpus

912
00:36:35,680 --> 00:36:43,040
so the sparsity goes up real fast as you

913
00:36:38,320 --> 00:36:45,760
add more unique words and the memory

914
00:36:43,040 --> 00:36:47,200
that is required to handle

915
00:36:45,760 --> 00:36:48,480
um

916
00:36:47,200 --> 00:36:51,920
uh

917
00:36:48,480 --> 00:36:53,520
you know this this set of documents goes

918
00:36:51,920 --> 00:36:56,320
up very fast

919
00:36:53,520 --> 00:36:57,520
so even if you use specialized data

920
00:36:56,320 --> 00:37:00,079
structures

921
00:36:57,520 --> 00:37:03,280
meant to store sparse data like sparse

922
00:37:00,079 --> 00:37:05,920
matrices you still end up growing the

923
00:37:03,280 --> 00:37:07,599
memory required to handle these data

924
00:37:05,920 --> 00:37:09,920
sets in a very

925
00:37:07,599 --> 00:37:13,119
non-linear way it still grows up very

926
00:37:09,920 --> 00:37:15,520
fast so this means it can take a very

927
00:37:13,119 --> 00:37:16,960
long time to train your model or even

928
00:37:15,520 --> 00:37:19,839
that um

929
00:37:16,960 --> 00:37:21,760
you you know you you outgrow the memory

930
00:37:19,839 --> 00:37:23,839
available on your machine you have to go

931
00:37:21,760 --> 00:37:26,079
to the cloud to an expensive you know

932
00:37:23,839 --> 00:37:27,680
big memory situation this can be a real

933
00:37:26,079 --> 00:37:31,200
challenge

934
00:37:27,680 --> 00:37:34,320
and this challenge it um

935
00:37:31,200 --> 00:37:37,200
is what has behind the motivating

936
00:37:34,320 --> 00:37:39,359
of um vector

937
00:37:37,200 --> 00:37:40,240
languages for models

938
00:37:39,359 --> 00:37:43,520
so

939
00:37:40,240 --> 00:37:46,480
linguists have worked for a long time

940
00:37:43,520 --> 00:37:49,680
on vector languages for models that can

941
00:37:46,480 --> 00:37:51,680
reduce the number of dimensions

942
00:37:49,680 --> 00:37:55,280
representing text data

943
00:37:51,680 --> 00:37:58,160
based on how people use language

944
00:37:55,280 --> 00:38:02,960
so this quote here

945
00:37:58,160 --> 00:38:04,960
goes all the way back to 1957.

946
00:38:02,960 --> 00:38:07,119
so the idea here

947
00:38:04,960 --> 00:38:09,280
is that yeah we use

948
00:38:07,119 --> 00:38:12,800
like the the data is very sparse you

949
00:38:09,280 --> 00:38:15,440
know but we don't use words um

950
00:38:12,800 --> 00:38:17,359
randomly it's not independent the words

951
00:38:15,440 --> 00:38:19,599
are not used independently of each other

952
00:38:17,359 --> 00:38:22,240
but rather there's relationships that

953
00:38:19,599 --> 00:38:23,359
exist between how words are used

954
00:38:22,240 --> 00:38:25,920
together

955
00:38:23,359 --> 00:38:27,599
and we can use those relationships to

956
00:38:25,920 --> 00:38:31,040
create

957
00:38:27,599 --> 00:38:33,599
to transform our sparse high dimensional

958
00:38:31,040 --> 00:38:34,560
space into a special

959
00:38:33,599 --> 00:38:37,359
dense

960
00:38:34,560 --> 00:38:40,160
low dimensional space well lower lower

961
00:38:37,359 --> 00:38:42,480
we still still has like 100 100

962
00:38:40,160 --> 00:38:44,000
dimensions but much lower than the like

963
00:38:42,480 --> 00:38:46,160
many thousands

964
00:38:44,000 --> 00:38:48,160
uh hundreds tens hundreds of thousands

965
00:38:46,160 --> 00:38:49,839
of space so the idea here we use

966
00:38:48,160 --> 00:38:51,839
statistical modeling

967
00:38:49,839 --> 00:38:55,599
maybe just um

968
00:38:51,839 --> 00:38:58,240
uh word counts plus matrix factorization

969
00:38:55,599 --> 00:39:00,480
maybe fancier math that involves neural

970
00:38:58,240 --> 00:39:03,680
networks to take this really high

971
00:39:00,480 --> 00:39:05,839
dimensional space and we create a new

972
00:39:03,680 --> 00:39:08,480
lower dimensional lower dimensional

973
00:39:05,839 --> 00:39:12,000
space that is special because the new

974
00:39:08,480 --> 00:39:15,040
space is created based on vectors that

975
00:39:12,000 --> 00:39:18,560
incorporate information

976
00:39:15,040 --> 00:39:21,839
about which words are used together so

977
00:39:18,560 --> 00:39:24,079
you shall know a word by the company it

978
00:39:21,839 --> 00:39:26,480
keeps

979
00:39:24,079 --> 00:39:29,359
so you need a big data set of text to

980
00:39:26,480 --> 00:39:32,000
create or learn these kinds of word

981
00:39:29,359 --> 00:39:35,119
vectors or word embeddings

982
00:39:32,000 --> 00:39:37,040
so this table that i'm showing right now

983
00:39:35,119 --> 00:39:40,560
it's from a set of embeddings that i

984
00:39:37,040 --> 00:39:44,640
created using a um

985
00:39:40,560 --> 00:39:48,000
a data set or a corpus of complaints

986
00:39:44,640 --> 00:39:49,200
complaints to the united states

987
00:39:48,000 --> 00:39:51,680
consumer

988
00:39:49,200 --> 00:39:53,839
financial protection bureau

989
00:39:51,680 --> 00:39:56,640
so this is a government body in the

990
00:39:53,839 --> 00:39:57,920
united states where people can complain

991
00:39:56,640 --> 00:40:00,720
and say

992
00:39:57,920 --> 00:40:03,680
what is wrong with something to do with

993
00:40:00,720 --> 00:40:04,720
a financial product like like a credit

994
00:40:03,680 --> 00:40:07,599
card

995
00:40:04,720 --> 00:40:09,760
a mortgage a student loan

996
00:40:07,599 --> 00:40:11,119
something to do with like a financial

997
00:40:09,760 --> 00:40:13,119
product they're like something went

998
00:40:11,119 --> 00:40:15,359
wrong with my credit card something went

999
00:40:13,119 --> 00:40:17,520
wrong with my mortgage that company is

1000
00:40:15,359 --> 00:40:20,000
not being fair so you come and you

1001
00:40:17,520 --> 00:40:24,000
complain to it

1002
00:40:20,000 --> 00:40:25,440
um so i took all those complaints um and

1003
00:40:24,000 --> 00:40:27,839
built a

1004
00:40:25,440 --> 00:40:30,160
a so it's our high dimensional space and

1005
00:40:27,839 --> 00:40:32,400
build a low dimensional space

1006
00:40:30,160 --> 00:40:33,920
and we can look in that space and

1007
00:40:32,400 --> 00:40:37,680
understand

1008
00:40:33,920 --> 00:40:40,240
um what words are related to each other

1009
00:40:37,680 --> 00:40:44,079
in this space so in the new space

1010
00:40:40,240 --> 00:40:48,000
defined by the embeddings the word month

1011
00:40:44,079 --> 00:40:49,680
is closest to words like year

1012
00:40:48,000 --> 00:40:50,880
months plural

1013
00:40:49,680 --> 00:40:54,560
monthly

1014
00:40:50,880 --> 00:40:56,480
installments payment so these are words

1015
00:40:54,560 --> 00:40:57,839
that are um that makes sense in the

1016
00:40:56,480 --> 00:41:01,280
context of

1017
00:40:57,839 --> 00:41:03,359
financial products like credit cards or

1018
00:41:01,280 --> 00:41:05,119
mortgages

1019
00:41:03,359 --> 00:41:06,720
in the new space defined by these

1020
00:41:05,119 --> 00:41:09,680
embeddings

1021
00:41:06,720 --> 00:41:11,760
the word error is closest to the words

1022
00:41:09,680 --> 00:41:14,960
like mistake

1023
00:41:11,760 --> 00:41:17,680
clerical like a clerical mistake

1024
00:41:14,960 --> 00:41:19,040
problem glitch oh there was a glitch on

1025
00:41:17,680 --> 00:41:20,880
my

1026
00:41:19,040 --> 00:41:23,440
mortgage statement

1027
00:41:20,880 --> 00:41:26,240
so we see these kinds of uh

1028
00:41:23,440 --> 00:41:28,319
or miscommunication misunderstanding you

1029
00:41:26,240 --> 00:41:30,880
know like these are these are words that

1030
00:41:28,319 --> 00:41:31,599
are used in similar ways

1031
00:41:30,880 --> 00:41:33,599
so

1032
00:41:31,599 --> 00:41:34,800
you don't have to create embeddings

1033
00:41:33,599 --> 00:41:37,200
yourself

1034
00:41:34,800 --> 00:41:39,520
because it requires quite a lot of data

1035
00:41:37,200 --> 00:41:42,240
to make them so you can use word

1036
00:41:39,520 --> 00:41:44,880
embeddings that are pre-trained

1037
00:41:42,240 --> 00:41:47,520
i.e created by someone else

1038
00:41:44,880 --> 00:41:49,839
based on some huge corpus of data that

1039
00:41:47,520 --> 00:41:51,359
they have access to and you probably

1040
00:41:49,839 --> 00:41:54,079
don't

1041
00:41:51,359 --> 00:41:55,599
so let's look at one of those data sets

1042
00:41:54,079 --> 00:41:57,440
let's look at

1043
00:41:55,599 --> 00:42:00,160
this table shows the results for the

1044
00:41:57,440 --> 00:42:02,800
same word error

1045
00:42:00,160 --> 00:42:04,560
but for the glove embeddings so the

1046
00:42:02,800 --> 00:42:07,359
glove embeddings are a set of

1047
00:42:04,560 --> 00:42:09,920
pre-trained embeddings that are created

1048
00:42:07,359 --> 00:42:11,040
based on a very large data set that's

1049
00:42:09,920 --> 00:42:12,800
like

1050
00:42:11,040 --> 00:42:15,280
all of wikipedia

1051
00:42:12,800 --> 00:42:16,640
all of the google news data set

1052
00:42:15,280 --> 00:42:18,480
um

1053
00:42:16,640 --> 00:42:21,440
just just like

1054
00:42:18,480 --> 00:42:24,800
huge swaths of the internet have been

1055
00:42:21,440 --> 00:42:27,280
fed in to create these embeddings

1056
00:42:24,800 --> 00:42:29,520
so some of the closest words

1057
00:42:27,280 --> 00:42:31,680
here are similar

1058
00:42:29,520 --> 00:42:34,800
to those that are before but we no

1059
00:42:31,680 --> 00:42:38,640
longer have some of that domain specific

1060
00:42:34,800 --> 00:42:40,839
flavor like clerical discrepancy

1061
00:42:38,640 --> 00:42:43,280
and now we have um or like

1062
00:42:40,839 --> 00:42:46,720
miscommunication you know but and now we

1063
00:42:43,280 --> 00:42:48,960
have um calculation and probability

1064
00:42:46,720 --> 00:42:51,599
which people were not talking about with

1065
00:42:48,960 --> 00:42:54,319
their financial product complaints

1066
00:42:51,599 --> 00:42:58,240
so this really highlights um

1067
00:42:54,319 --> 00:43:00,800
how these how these work um here before

1068
00:42:58,240 --> 00:43:02,640
we we created our own and we were able

1069
00:43:00,800 --> 00:43:06,800
to learn relationships that were

1070
00:43:02,640 --> 00:43:10,400
specific to this context and here we go

1071
00:43:06,800 --> 00:43:13,119
to the to a more general set that that

1072
00:43:10,400 --> 00:43:16,720
was learned somewhere else

1073
00:43:13,119 --> 00:43:19,599
so embeddings are trained or learned

1074
00:43:16,720 --> 00:43:22,079
from a large corpus of text data and the

1075
00:43:19,599 --> 00:43:24,240
characteristics of that corpus become

1076
00:43:22,079 --> 00:43:27,359
part of the embeddings

1077
00:43:24,240 --> 00:43:30,160
so machine learning in general you know

1078
00:43:27,359 --> 00:43:32,240
is exquisitely sensitive to whatever it

1079
00:43:30,160 --> 00:43:35,839
is that's in your training data and this

1080
00:43:32,240 --> 00:43:38,160
is never more obvious than when um

1081
00:43:35,839 --> 00:43:40,319
dealing with text data

1082
00:43:38,160 --> 00:43:42,400
and and perhaps with word embeddings is

1083
00:43:40,319 --> 00:43:43,760
just like one of these classic examples

1084
00:43:42,400 --> 00:43:46,079
where this is true

1085
00:43:43,760 --> 00:43:48,160
it turns out that um

1086
00:43:46,079 --> 00:43:51,040
this this shows up

1087
00:43:48,160 --> 00:43:54,400
in how any human

1088
00:43:51,040 --> 00:43:56,480
um prejudice or bias in the corpus

1089
00:43:54,400 --> 00:43:59,040
becomes imprinted

1090
00:43:56,480 --> 00:44:01,440
into the embeddings

1091
00:43:59,040 --> 00:44:04,720
so in fact when we look at some of these

1092
00:44:01,440 --> 00:44:07,040
most commonly available embeddings that

1093
00:44:04,720 --> 00:44:09,440
are out there

1094
00:44:07,040 --> 00:44:12,200
bias is um

1095
00:44:09,440 --> 00:44:14,640
we we see that um

1096
00:44:12,200 --> 00:44:17,520
african-american first names that are

1097
00:44:14,640 --> 00:44:19,680
more common for african americans in the

1098
00:44:17,520 --> 00:44:23,280
united states they're associated with

1099
00:44:19,680 --> 00:44:25,520
more unpleasant feelings than european

1100
00:44:23,280 --> 00:44:27,680
american first names in these embedding

1101
00:44:25,520 --> 00:44:30,240
spaces

1102
00:44:27,680 --> 00:44:31,599
women's first names are more associated

1103
00:44:30,240 --> 00:44:33,520
with family

1104
00:44:31,599 --> 00:44:36,079
and men's first names are more

1105
00:44:33,520 --> 00:44:38,640
associated with career

1106
00:44:36,079 --> 00:44:40,560
and terms associated with women are more

1107
00:44:38,640 --> 00:44:42,640
associated with the arts and terms

1108
00:44:40,560 --> 00:44:44,079
associated with men are more associated

1109
00:44:42,640 --> 00:44:44,960
with science

1110
00:44:44,079 --> 00:44:48,319
so

1111
00:44:44,960 --> 00:44:51,440
it turns out actually bias is so

1112
00:44:48,319 --> 00:44:54,160
ingrained in word embeddings that the

1113
00:44:51,440 --> 00:44:56,800
word embeddings themselves can be used

1114
00:44:54,160 --> 00:44:58,480
to quantify change

1115
00:44:56,800 --> 00:45:01,440
in um

1116
00:44:58,480 --> 00:45:05,040
in social attitudes over time

1117
00:45:01,440 --> 00:45:08,160
so word embeddings are

1118
00:45:05,040 --> 00:45:11,119
maybe an exaggerated or extreme example

1119
00:45:08,160 --> 00:45:13,599
but it turns out that all the feature

1120
00:45:11,119 --> 00:45:16,720
engineering decisions that we make when

1121
00:45:13,599 --> 00:45:18,480
it comes to text data have a significant

1122
00:45:16,720 --> 00:45:21,440
effect on our results

1123
00:45:18,480 --> 00:45:22,560
both in terms of the model performance

1124
00:45:21,440 --> 00:45:26,000
that we see

1125
00:45:22,560 --> 00:45:29,520
and also in terms of how appropriate or

1126
00:45:26,000 --> 00:45:31,599
fair our models are

1127
00:45:29,520 --> 00:45:34,560
so given all that when it comes to

1128
00:45:31,599 --> 00:45:37,040
pre-processing your text data

1129
00:45:34,560 --> 00:45:39,680
creating these features that you need

1130
00:45:37,040 --> 00:45:43,520
you have a lot of options and quite a

1131
00:45:39,680 --> 00:45:46,560
bit of responsibility so my advice is

1132
00:45:43,520 --> 00:45:50,480
always start with simpler models that

1133
00:45:46,560 --> 00:45:50,480
you can understand quite deeply

1134
00:45:50,640 --> 00:45:56,720
be sure to adopt good statistical

1135
00:45:53,520 --> 00:45:59,040
practices as you train and tune your

1136
00:45:56,720 --> 00:46:01,839
models so you aren't

1137
00:45:59,040 --> 00:46:04,720
you aren't fooled about model

1138
00:46:01,839 --> 00:46:07,119
performance improvements

1139
00:46:04,720 --> 00:46:10,400
when you try different approaches

1140
00:46:07,119 --> 00:46:12,839
and also to use model explainability

1141
00:46:10,400 --> 00:46:16,079
tools and frameworks so you can

1142
00:46:12,839 --> 00:46:18,000
understand any less straightforward

1143
00:46:16,079 --> 00:46:20,240
models that you try

1144
00:46:18,000 --> 00:46:22,640
so my co-workers and i have written

1145
00:46:20,240 --> 00:46:24,720
about all of these topics um and how to

1146
00:46:22,640 --> 00:46:26,880
use them with tidy models if that's what

1147
00:46:24,720 --> 00:46:28,720
you like to use and we will continue to

1148
00:46:26,880 --> 00:46:30,319
do so

1149
00:46:28,720 --> 00:46:33,040
with that i will say

1150
00:46:30,319 --> 00:46:34,880
thank you so very much and i want to be

1151
00:46:33,040 --> 00:46:36,560
sure to again

1152
00:46:34,880 --> 00:46:38,960
thank the um

1153
00:46:36,560 --> 00:46:42,560
thank the organizers of the r user group

1154
00:46:38,960 --> 00:46:45,119
in korea i want to thank my teammates on

1155
00:46:42,560 --> 00:46:47,440
the tidy models team at our studio as

1156
00:46:45,119 --> 00:46:51,240
well as my co-author

1157
00:46:47,440 --> 00:46:51,240
emil v veldt