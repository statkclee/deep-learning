1
00:00:01,120 --> 00:00:08,639
안녕하세요제이름은줄리아실기입니다. 저는데이터과학자이자소프트웨어엔지니어로 RStudio에서근무하고있습니다.

2
00:00:06,080 --> 00:00:18,080
우선 이런 행사에 저를 초대해주신 한국 R 사용자회 주최측에  정말 감사드립니다.

3
00:00:15,519 --> 00:00:21,169
저는 오늘 텍스트 데이터를 사용해 머신러닝을 위한

4
00:00:21,193 --> 00:00:29,359
입력변수(feature)들을 만드는
것에 대해 이야기 하고자 합니다.

5
00:00:27,199 --> 00:00:38,269
기계학습 모형을 학습시킬 준비를 하거나
텍스트 분석 프로젝트를 시작하는 경우 모두,

6
00:00:38,293 --> 00:00:50,719
텍스트 데이터를 기계학습 알고리즘의 입력 변수로
변환하는 과정을 잘 이해하는 것은 많은 도움이 됩니다.

7
00:00:48,399 --> 00:00:57,508
또는, 데이터 과학자의 업무 상에서나
아니면 일상 생활에서도 점점 더 많이 접하게 되는

8
00:00:57,532 --> 00:01:06,640
기계학습 모형들이 어떻게 작동하는
지를 이해하는 데에 많은 도움이 됩니다

9
00:01:03,359 --> 00:01:09,547
우리가 텍스트를 위한 머신러닝 모델을
만드는 일은, 그것이 지도 학습 모델이든

10
00:01:09,571 --> 00:01:15,759
비지도 학습 모델이든, 지금 슬라이드에 보이는
것과 같은 텍스트 데이터에서 시작합니다.

11
00:01:11,600 --> 00:01:24,560
이 예문은 어떤 동물을 설명하고 있는데요,  이 강연에서 몇 번 더 나오는 예시입니다.

12
00:01:21,040 --> 00:01:34,400
영어 사용자인 저에게는 상당히 친숙한 형태의 글인데요,

13
00:01:30,960 --> 00:01:43,920
저는 인간의 언어를 사용하는 사람이기 때문에 이 글을 보고 읽을 수 있고 큰 소리로 말할 수 있으며,

14
00:01:41,280 --> 00:01:45,759
이 글이 어떤 의미인지 해석하고 이해할 수 있습니다.

15
00:01:43,920 --> 00:01:59,840
이러한 자연어 데이터는 모든 언어와 모든 맥락에서 매 시간 생성되고 있는데요,

16
00:01:56,880 --> 00:02:11,788
헬스케어, 테크, 금융권 등 분야와 상관없이 모든 기관에서
이러한 텍스트 데이터들은 고객에 의해, 파트너 사에 의해,

17
00:02:11,812 --> 00:02:26,720
내부 이해관계자들에 의해, 설문조사에 참여한 사람들에
의해, 소셜미디어에 의해 비즈니스 과정에 의해 생성되고 있습니다.

18
00:02:24,319 --> 00:02:35,360
그리고 이렇게 생성되는 모든 텍스트들에는 우리가 더 나은 결정을 내리는 데 사용할 수 있는 정보들이 숨겨져 있죠.

19
00:02:33,519 --> 00:02:47,440
그러나 컴퓨터는 이렇게 텍스트로 표현된  글을 이해하거나 분석하지 못합니다.

20
00:02:45,440 --> 00:02:55,440
대부분의 모델들에서 이 글을 분석하기 위해서는

21
00:02:53,680 --> 00:03:01,840
화면에 보이는 것처럼  기계가 이해할 수 있는  숫자들로 변환해주어야 합니다.

22
00:02:59,760 --> 00:03:10,948
저는 사람들이 하나의 행에 하나의 관찰 데이터가
있는  깔끔한 형식(tidy format)의 텍스트 데이터로

23
00:03:10,972 --> 00:03:22,159
탐색적 데이터 분석, 시각화, 요약 작업을 수행할 수 있도록
해주는 소프트웨어를 만드는 일에 많은 시간을 투자했습니다.

24
00:03:19,040 --> 00:03:26,295
저는 깔끔한 데이터 원칙(tidy data principles)을 적용해 텍스트 분석을 하는 것을 좋아합니다.

25
00:03:26,319 --> 00:03:34,879
특히 탐색적 데이터 분석 단계에서 말이죠.

26
00:03:32,640 --> 00:03:42,468
모델을 만들 때 모델 내의 수학을
구현하기 위해서는 지금 보고 계시는 것과

27
00:03:42,492 --> 00:03:52,319
같은 문서 단어 행렬 (document
term matrix)이 필요합니다.

28
00:03:49,760 --> 00:03:55,760
구체적인 행렬은 제가 보여드리는 것과 다를 수는 있습니다.

29
00:03:53,840 --> 00:03:59,443
지금 보여드리고 있는 것은 갯수로 가중치를 부여하는 방식을 사용한 것인데,

30
00:03:59,467 --> 00:04:04,720
각각의 행은 각각의 문서를 나타내고,

31
00:04:02,640 --> 00:04:07,439
각각의 열은 단어(토큰 token)를 나타내며,

32
00:04:04,720 --> 00:04:12,028
각 숫자는 각 단어가 각 문서에서 몇번 쓰였는지를 나타내고 있는데요,

33
00:04:12,052 --> 00:04:19,359
갯수를 세는 대신 TF-IDF 를 이용하는 등
다른 방식으로 가중치를 부여하거나,

34
00:04:16,720 --> 00:04:28,433
또 딥러닝 모델을 구축하는 데 관심이 있다면 시퀀스 정보를 유지할
수도 있을 것입니다. 어쨌든 가장 중요한 것은, 우리가 사용할 모델이

35
00:04:28,457 --> 00:04:34,201
아주 간단하고 텍스트 데이터와
단어 임베딩에서 잘 동작하는 나이브 베이즈 모델이든,

36
00:04:34,225 --> 00:04:41,120
현재 가장 훌륭해보이는
트랜스포머 기반 모델이든 상관없이

37
00:04:39,520 --> 00:04:46,682
우리는 머신러닝 알고리즘에 사용될
수 있도록 자연어를 적절하게 바꿔주는

38
00:04:46,706 --> 00:04:53,680
일과 변수 가공을 진행하는 것에
심혈을 기울여야한다는 것입니다.

39
00:04:50,960 --> 00:04:57,868
저는 모델링과 머신러닝을 위해 만들어진
R 오픈소스 프레임워크인 Tidymodels를

40
00:04:57,892 --> 00:05:04,800
작업하고 있으며 오늘 발표에서 보여드릴
예제들은 이 Tidymodels코드를 사용합니다.

41
00:05:02,320 --> 00:05:11,336
Tidymodels 프로젝트의 목표 중 하나는 모델링을
지금 막 시작하는 사람들부터 매우 경험이 있는

42
00:05:11,360 --> 00:05:21,360
사람들까지 실제로 데이터를 가지고 일하고 있는
실무자들에게 일관되고 유연한 프레임워크를 제공하는 것입니다.

43
00:05:19,199 --> 00:05:28,268
R에 있는 서로 다른 인터페이스들을 조화시켜

44
00:05:28,292 --> 00:05:37,360
더 좋은 통계 작업들을 장려하는 것이죠.

45
00:05:35,360 --> 00:05:43,601
제가 작업하고 구축해 온 것들과 이를 어떻게
텍스트 모델링에 적용할 수 있는지 보여드릴 수 있어서

46
00:05:43,625 --> 00:05:52,800
기쁩니다. 그렇지만 오늘 저의 발표는
Tidymodels에만, 혹은 R에만 국한되는 이야기는 아닙니다.

47
00:05:50,880 --> 00:06:00,479
물론 여러분이 R 사용자 그룹이라는 것을 알고 있지만, 오늘 제가 이야기 하고자 하는 것은 조금 더 개념적이고 기본적인 것입니다.

48
00:05:57,440 --> 00:06:06,800
오늘의 주제는 바로 '텍스트를 어떻게 머신러닝을 위한 독립변수로 만들 것인가' 입니다.

49
00:06:04,560 --> 00:06:12,296
그래도 Tidymodels 에 대해 이야기할
수 있어서 기쁜데요, Tidyverse가

50
00:06:12,320 --> 00:06:19,720
메타 패키지 인 것처럼 Tidymodels
역시 하나의 메타 패키지 입니다.

51
00:06:16,319 --> 00:06:24,745
library(tidyverse)를 입력한 뒤,
ggplot2를 이용해 시각화를 하거나 dplyr을

52
00:06:24,769 --> 00:06:33,039
이용해 데이터 가공을 하는 방식과 비슷한
방식으로 Tidymodels를 사용할 수 있습니다.

53
00:06:30,560 --> 00:06:39,840
Tidymodels 안에는 각 목적에 맞게 사용될 수 있는 여러 패키지가 들어있습니다.

54
00:06:37,280 --> 00:06:48,000
전처리 또는 변수 가공은 보다 광범위한 모델을 만드는 과정의 일부입니다.

55
00:06:45,280 --> 00:06:56,240
모델을 만드는 것은 실제로 어떤 종류의 모델을 구축할 것인지 결정하는 것을 도와주는 탐색적 데이터 분석 (EDA)에서 시작하며

56
00:06:54,080 --> 00:07:03,120
모델을 만들고 1차 모델 개발 완료가 됩니다.

57
00:06:59,280 --> 00:07:09,280
만들어진 모델의 성능을 측정하는 단계에서 비로소 완료된다고 할 수 있겠습니다.

58
00:07:06,479 --> 00:07:14,694
소프트웨어로써의 Tidymodels는 각각의 목적을
가진 R 패키지들로 이루어져 있습니다. rsample의

59
00:07:14,718 --> 00:07:22,508
경우에는 재표본추출(resample)을 위한
것입니다. 부트스트랩을 위한 재표본추출, 교차 검증을

60
00:07:22,532 --> 00:07:30,534
위한 재표본추출과 같이 모델을 학습시키고 평가하기
위해 사용할 수 있는 모든 종류의 재표본추출을 위해

61
00:07:30,558 --> 00:07:38,560
사용됩니다. tune 패키지의 경우, 이름에서 예상할
수 있듯, 하이퍼 파라미터 튜닝을 위해 사용됩니다.

62
00:07:34,800 --> 00:07:43,748
Tidymodels에 속한 패키지
중 하나는 데이터 전처리 및

63
00:07:43,772 --> 00:07:52,720
변수 가공을 위한 패키지인데,
그 이름은 recipe입니다.

64
00:07:49,360 --> 00:07:58,097
Tidymodels에서 우리는  데이터
전처리와 변수 가공을 마치 일련의 단계를

65
00:07:58,121 --> 00:08:06,859
가진 전처리 '레시피' 안에서 생각해보았습니다.
여러분은 요리에 들어갈 변수라는

66
00:08:06,883 --> 00:08:15,802
재료를 고르고, 레시피에 들어갈 요리
과정들을 명확하게 정의한 뒤, 학습 데이터를

67
00:08:15,826 --> 00:08:25,759
이용해 요리를 준비하고, 이를 테스트
데이터나 예측 시의 새 관측 데이터에 적용합니다.

68
00:08:23,039 --> 00:08:34,560
이 때 모델링에 사용할 들어갈 변수나 재료는 텍스트 데이터를 포함해 다양한 형태와 사이즈를 지닌 데이터가 될 수 있을 것입니다.

69
00:08:32,000 --> 00:08:39,277
어떤 테크닉과 접근들에서는
텍스트 데이터의 전처리가 텍스트가

70
00:08:39,301 --> 00:08:48,628
아닌 데이터, 숫자형 데이터, 범주형
데이터 등 다른 데이터들의 전처리와

71
00:08:48,652 --> 00:08:56,156
똑같을 수도 있습니다. 하지만
언어 데이터의 특성상 모델링에서

72
00:08:56,180 --> 00:09:05,279
좋은 작업을 수행하기 위해서는
언어만을 위한 전처리 방식이 필요합니다.

73
00:09:03,680 --> 00:09:14,720
저는 Emil Hvitfeldt와 함께 "supervised machine learning for text analysis in r" 이라는 책을 저술했는데요,

74
00:09:11,360 --> 00:09:18,628
이 책의 첫 삼분의 일은 텍스트 데이터 속 자연어를 어떻게

75
00:09:18,652 --> 00:09:25,920
모델링을 위한 변수로 변환할 것인가에 관한 내용입니다.

76
00:09:23,600 --> 00:09:31,908
중간 부분은 이러한 변수들을 정규화
회귀모형(regularized regression)이나 서포트 벡터

77
00:09:31,932 --> 00:09:40,240
머신(SVM)처럼 간단하거나 비교적 전통적인 머신러닝
모델에서 어떻게 사용할 것인가에 관한 내용입니다.

78
00:09:37,360 --> 00:09:46,176
이 책의 마지막 삼분의 일은 딥러닝
모델에서 텍스트 데이터를 사용하는 방법을

79
00:09:46,200 --> 00:09:56,308
이야기합니다.  딥러닝 모델들도 앞서 말씀
드렸던 모델과 마찬가지로 자연어를 변수로

80
00:09:56,332 --> 00:10:06,113
변환하여 입력으로 넣어주어야 합니다.
하지만 딥러닝 모델들은 앞서 말씀드린 간단하거나

81
00:10:06,137 --> 00:10:15,279
비교적 예전 모델들은 하지 못했던
텍스트 피처의 구조를 학습할 수 있습니다.

82
00:10:12,800 --> 00:10:20,560
이 책은 지금 완성되었고 11월을 시작하는 현재 시점에 시판되고 있습니다.

83
00:10:18,480 --> 00:10:30,720
사람들은 첫 인쇄본을 받고 있는데요, URL은 smltar.com입니다.

84
00:10:28,560 --> 00:10:36,188
만일 여러분이 텍스트 데이터를 처음
다뤄보신다면, 이러한 기본적인 텍스트 전처리

85
00:10:36,212 --> 00:10:43,839
접근법에 대한 이해가 여러분이 효과적인
모델을 학습시키는데에 도움이 될 것입니다.

86
00:10:41,760 --> 00:10:50,469
만일 여러분이 텍스트 데이터 경험이 많은 사람이라면,

87
00:10:50,493 --> 00:10:58,000
아마 우리가 그랬던 것처럼 느끼셨을 겁니다.

88
00:10:54,480 --> 00:11:05,348
책이든 튜토리얼이든 블로그 글이든 전처리
과정이 어떻게 되고 그 과정에서 내리는 선택들이 모델

89
00:11:05,372 --> 00:11:17,519
결과에 어떤 영향을 주는지에 대해 자세하고 깊게
다루는 자료들이 생각보다 많이 없다는 것을 말이죠.

90
00:11:15,600 --> 00:11:21,355
이제 전처리에 대해 좀 더 자세히
이야기 해봅시다. 기본적인 변수 가공(fature

91
00:11:21,379 --> 00:11:28,800
engineering) 접근법들을 살펴보며, 그들이
어떻게 동작하는지, 또 무엇을 하는지 알아봅시다.

92
00:11:27,040 --> 00:11:31,920
토큰화(tokenization)에서 시작해봅시다.

93
00:11:28,800 --> 00:11:40,548
토큰화는 우리가 하려고 텍스트 분석의 종류와 상관 없이,

94
00:11:40,572 --> 00:11:52,320
그 분석이 탐색적 데이터 분석이든 모델을 만드는 것이든,

95
00:11:49,040 --> 00:11:55,360
일반적으로 자연어를 머신러닝의 변수로 바꾸기 위한 첫번째 단계입니다.

96
00:11:52,320 --> 00:12:02,079
토큰화에서는 우리는 문자열이나 문자 벡터를 입력으로 넣어주고, 또 토큰의 유형도 입력으로 넣어줍니다.

97
00:11:59,440 --> 00:12:05,323
토큰은 우리가 관심이 있는
글의 단위로, 예를 들면 단어 같은

98
00:12:05,347 --> 00:12:12,480
것입니다. 토큰화는 글을 우리가
관심있는 유형의 토큰으로 분해합니다.

99
00:12:09,519 --> 00:12:19,040
우리가 일반적으로 텍스트를 토큰이라는 어떤 단위로 쪼갤 때 가장 많이 사용하는 토큰의 유형은 바로 '단어'입니다.

100
00:12:17,839 --> 00:12:24,421
단어를 기준으로 글을 쪼개는 것이
간단하거나 명백해 보일 수 있지만, 많은 언어들,

101
00:12:24,445 --> 00:12:30,880
심지어는 대부분의 언어들에서 단어가
무엇인지 명확하게 정의하기는 어렵습니다.

102
00:12:29,519 --> 00:12:40,959
단어 사이에 띄어쓰기를 하지 않는 많은 언어들에서 단어를 기준으로 토큰화 하는 것은 어렵고,

103
00:12:38,240 --> 00:12:51,148
또 영어나 한국어처럼 띄어쓰기를 하는 언어라도
영어의 축약형처럼 단어를 구분해내기 모호한 경우,

104
00:12:51,172 --> 00:13:04,079
한국어의 보조사(particle)처럼 붙어있지만
사실은  두 단어로 간주하는 것이 더 정확한 경우,

105
00:13:01,519 --> 00:13:13,839
이탈리아어, 프랑스어와 같은 로망스어군 언어(Romance Language)의 대명사나 부정어처럼 붙어있지만 두 개의 다른 단어로 간주되어야 하는 경우와 같은 특정한 경우들 때문에 단어를 기준으로 토큰화 하는 것은 어려운 일입니다.

106
00:13:12,399 --> 00:13:22,548
만일 여러분이 무엇을 할 것인지 정했고, 몇 가지 선택을 거쳐
결국 텍스트를 토큰화하는데 성공했다면, 이제 여러분은 이것을 탐색적

107
00:13:22,572 --> 00:13:32,720
데이터 분석이나 비지도 학습 알고리즘에서 사용할 수 있으며
지금부터 얘기할 내용인 예측 모델의 변수로도 사용할 수 있습니다.

108
00:13:30,320 --> 00:13:37,748
여기에 표시되는 내용은 영국의
Tate 컬렉션에 있는 예술 작품의

109
00:13:37,772 --> 00:13:45,199
소재에 대해 설명하는 글들을
회귀 모형으로 훈련한 결과입니다.

110
00:13:43,279 --> 00:13:52,667
우리가 예측하고자 하는 것은
작품이 몇 년도에 창작되었는지와

111
00:13:52,691 --> 00:14:02,079
작품의 소재입니다. 작품의
소재는 짧은 글로 묘사되어 있고요.

112
00:14:00,079 --> 00:14:14,848
보시다시피 연필로 그려진 그림, 수채화, 판화는
비교적 예전에 창작되었을 가능성이 높았고, 사진,

113
00:14:14,872 --> 00:14:29,920
스크린 프린팅(screen printing),
코끼리 똥(dung), 반짝이를 사용한 예술은

114
00:14:27,760 --> 00:14:37,519
비교적 최근에 창작된 현대 미술일 가능성이 높았습니다.

115
00:14:33,440 --> 00:14:43,113
텍스트를 토큰화하는 방법은 우리는
예술 작품의 소재에 대한  자연어 텍스트에서

116
00:14:43,137 --> 00:14:54,655
시작합니다. 자연어를 토큰화하는 방식은
예측 모형 학습 결과에 큰 영향을 미칩니다. 만약

117
00:14:54,679 --> 00:15:05,967
다른 방식으로 토큰화했다면 연도예측의
정확도, 우리가 어떻게 이 모델을 해석할 것인지,

118
00:15:05,991 --> 00:15:17,279
그리고 어떤 것들을 배울 수 있는지와 같은
성능 측면에서 다른 결과가 나올 것입니다.

119
00:15:15,279 --> 00:15:22,169
앞선 사례는하나의 단어를 기준으로
토큰화하는 것의 한 방법입니다. 단일 단어 또는

120
00:15:22,193 --> 00:15:31,279
유니그램(unigram)으로 분해하는 대신,
n-그램이라는 다른 토큰화 방식을 사용할 수도 있습니다.

121
00:15:27,440 --> 00:15:37,920
n-그램은 주어진 텍스트 시퀀스 속 연속된 n개의 시퀀스를 말합니다.

122
00:15:35,199 --> 00:15:44,450
다음은 동물 설명문 텍스트가
바이그램(Bi-gram), 즉 2개의 토큰으로 이루어진

123
00:15:44,474 --> 00:15:54,959
n-gram으로 쪼개진 모습입니다. 바이그램
간의 단어들이 어떻게 겹치는지 주목해보세요.

124
00:15:51,519 --> 00:16:01,308
collard'라는 단어는 첫 번째 바이그램인 'the collared'와 두 번째
바이그램인 'collared peccary'에 모두 나타납니다. 그리고 'peccary' 또한

125
00:16:01,332 --> 00:16:11,120
두 번째 바이그램과 세 번째 바이그램 두 곳에서 나타납니다. 이와 같이, n-gram
토큰화는 텍스트를 따라 미끄러짐 이동(slide)을 하면서 겹치는 토큰 집합을 만듭니다.

126
00:16:07,040 --> 00:16:14,480
다음은 같은 텍스트에 대한 트라이그램(tri-gram) 결과를 보여줍니다.

127
00:16:11,120 --> 00:16:23,360
유니그램(uni-gram)을 사용하면 더 빠르고 효율적이지만 단어 순서에 대한 정보는 잡아내지 못합니다.

128
00:16:20,560 --> 00:16:30,988
n-gram에서 n에 2, 3 혹은 그보다 더
높은 값을 사용하면, 여러 개의 단어로 구성된

129
00:16:31,012 --> 00:16:41,440
구(phrase)가 지닌 단어 순서나 개념과
같이 보다 복잡한 정보를 보존할 수 있습니다.

130
00:16:37,680 --> 00:16:47,842
하지만, 이 경우 토큰 벡터 공간이
극적으로 증가하는 반면, 각 토큰(n-gram)이

131
00:16:47,866 --> 00:16:56,480
텍스트 내에서 발견되는 횟수는
감소하게 됩니다. 이는 데이터에 따라

132
00:16:55,120 --> 00:17:00,399
좋은 예측 결과를 얻지 못할 수 있음을 의미합니다.

133
00:16:58,160 --> 00:17:06,108
다양한 n값을 이용한 n-gram들을 함께
이용함으로써 다양한 수준의 세부 정보를 텍스트로부터

134
00:17:06,132 --> 00:17:14,079
추출할 수 있습니다. 유니그램은 어떤 개별
단어가 많이 사용되었는지 알려줄 수 있습니다.

135
00:17:11,439 --> 00:17:23,360
이러한 단어 중 일부는, 다른 특정 단어들과 함께 나타나는 경우가 그리 많지 않은 경우, 바이그램이나 트라이그램에서는 간과될 수 있습니다.

136
00:17:23,520 --> 00:17:31,588
다음 그림은 미국 대법원 의견의 연도를
예측하는 Lasso 회귀 모형들의 성능을 보여주고

137
00:17:31,612 --> 00:17:39,679
있는데요, 모형 세 가지 다른 수준의
n-gram에 기반해 모형의 성능을 비교합니다.

138
00:17:37,840 --> 00:17:44,254
여기서 우리가 수행하는 작업은  미국 대법원 문서를

139
00:17:44,278 --> 00:17:51,840
가져온 뒤 그 문서가 언제 작성되었는지 예측하는 것입니다.

140
00:17:49,760 --> 00:17:58,799
즉, 텍스트의 내용을 통해, 텍스트가 얼마나 오래된 것인지 예측할 수 있을까 하는 문제입니다

141
00:17:55,360 --> 00:18:04,468
사용할 토큰의 개수를 1,000개로
제한했을 때, 유니그램만 사용한 모형이

142
00:18:04,492 --> 00:18:13,600
이 미국 대법원 문서 연도 예측
문제에서는 가장 좋은 성능을 보였습니다

143
00:18:11,280 --> 00:18:18,508
하지만 유니그램을 사용하는 것이 항상 가장 좋은 성능을
보이는 것은 아닙니다. 데이터 세트 자체, 그리고 사용하는 모형에

144
00:18:18,532 --> 00:18:25,760
따라, 유니그램과 바이그램을 결합한 경우나, 혹은 또 다른
옵션을 선택한 경우가  더 나은 예측 성능을 보일 수 있습니다.

145
00:18:23,679 --> 00:18:33,347
이 때 바이그램과 트라이그램에
담겨있는 좀 더 복잡한 정보를 모형에

146
00:18:33,371 --> 00:18:43,039
반영하려면, 아마도 모형의 토큰
수를 상당히 늘려야 할 것입니다.

147
00:18:40,160 --> 00:18:51,039
이와 같은 결과를 볼 때, n-그램을 식별하는데에는 비용이 많이 든다는 점을 명심하세요.

148
00:18:48,960 --> 00:19:00,640
우리는 비용과 모델의 성능 향상도를 비교해야 합니다.

149
00:18:58,720 --> 00:19:06,833
바이그램(Bi-gram)을 추가했을 때 모델의 성능이
아주 조금만 개선된다면 어떨지와 같이, 바이그램을

150
00:19:06,857 --> 00:19:15,840
식별하고 학습시키는 데에 드는 시간 대비 바이그램을
사용한 모델의 성능이 얼마나 좋아졌는지를 고려해야합니다.

151
00:19:13,200 --> 00:19:20,148
예를 들어, 토큰수를 일정하게
유지한 대법원 의견 데이터셋의 경우,

152
00:19:20,172 --> 00:19:27,120
모델 학습에는 동일한 수의 토큰이 포함되었습니다.

153
00:19:25,919 --> 00:19:35,651
하지만 바이그램과 유니그램을 함께 사용했을 때
변수 가공을 하는데 걸리는 시간은 유니그램만 사용했을 때

154
00:19:35,675 --> 00:19:46,799
걸리는 시간의 두배입니다. 또한 트라이그램까지 사용하면
유니그램만 학습하는 것보다 거의 5배 시간이 걸립니다.

155
00:19:44,320 --> 00:19:56,559
즉, 컴퓨터 계산 비용이 많이 드는 작업인 것입니다. 또 다른 방향으로 접근해보자면,

156
00:19:53,760 --> 00:20:05,440
단어보다 작은 단위로도 토큰화를 할 수 있습니다. 화면에 보이는 것과 같은 것들을 'character shingles'이라고 합니다.

157
00:20:03,280 --> 00:20:13,120
"collared peccary"라는 단어를 뽑고, 단어를 보는 대신 내려가서 하위 단어 정보를 살펴볼 수 있습니다.

158
00:20:10,880 --> 00:20:20,880
단어를 기계 학습에 적합한 하위 단어로 나누는 방법에는 여러 가지가 있는데요,

159
00:20:18,480 --> 00:20:31,360
종종 이런 접근 방식이나 알고리즘은 예측 시 알 수 없는 단어나 새로운 단어를 인코딩할 수 있다는 이점이 있습니다.

160
00:20:29,039 --> 00:20:41,200
새로운 데이터에 대한 예측을 해야 할 때, 새로운 단어들이 종종 등장하곤 하는데요,

161
00:20:39,200 --> 00:20:49,360
만일 그 단어가 훈련 데이터에 없었다면, 우리는 어떻게 이 단어를 처리해야할까요?

162
00:20:45,600 --> 00:20:51,988
하위 단어(subword) 정보를 사용하여
모델을 학습시켰고, 하위 단어에 그 새로운

163
00:20:52,012 --> 00:20:58,400
단어의 하위단어와 비슷한 것이 있다면,
우리는 이새로운 단어를 처리할 수 있습니다.

164
00:20:55,760 --> 00:21:03,886
이렇게 하위 단어(subword) 정보를
사용해 형태학적 시퀀스를 다양한 종류의

165
00:21:03,910 --> 00:21:13,360
모형에 사용할 수 있습니다. 이는 영어뿐만
아니라 다양한 언어에도 적용되는 방식입니다.

166
00:21:11,919 --> 00:21:22,960
지금 보시는 것은 매우 짧은 텍스트 데이터 셋을 분석한 분류 모형의 결과입니다.

167
00:21:20,000 --> 00:21:27,120
이들은 미국에 있는 우체국 이름인데요, 매우 짧습니다.

168
00:21:24,240 --> 00:21:32,788
모형의 목적은 우체국이 태평양
한가운데 하와이에 있는지, 아니면 하와이가

169
00:21:32,812 --> 00:21:41,360
아닌 미국의 나머지 지역에 위치하고
있는지를 예측하는 것이였습니다.

170
00:21:38,159 --> 00:21:49,200
저는 우체국 이름의 하위 단어(subword)를 변수로 만들었는데요,

171
00:21:46,480 --> 00:21:53,348
이름에 'h'와 'p'로 시작하거나
'ale' 라는 하위 단어(subword)를

172
00:21:53,372 --> 00:22:00,240
포함하는 우체국은 하와이에 있을
가능성이 더 높다는 것을 알게 되었습니다.

173
00:21:57,440 --> 00:22:11,440
또한 이름에 하위단어  'and', 'ri' 및 'ing'를 포함하고 있는 경우는 하와이가 아닌 미국 지역에 위치한 우체국의 이름일 가능성이 높았습니다 .

174
00:22:09,360 --> 00:22:19,600
방금 살펴본 사례는 토큰화 방식에 따라 우리가 배울 수 있는 것이 달라진다는 것을 보여주는 예시였습니다.

175
00:22:18,559 --> 00:22:29,919
`Tidymodels`에서는 토큰화를 둘러싼 우리의 모든 결정들이 기록됩니다. 코드는 다음과 같습니다.

176
00:22:26,880 --> 00:22:38,640
사용할 변수 즉 재료를 지정하는  레시피(recipe)로 시작한 다음, 이러한 전처리 단계를 정의합니다.

177
00:22:35,679 --> 00:22:50,400
첫번째 단계에서, 그리고  정말 간단하고 기본적인 단계에서도 데이터 과학자가 내리는 결정은 모델의 결과에 큰 영향을 미칩니다.

178
00:22:47,120 --> 00:22:56,240
다음으로 이야기하고자 하는 전처리 단계는 불용어(stopwords)에 관한 것입니다.

179
00:22:53,600 --> 00:23:11,840
텍스트를 토큰으로 분할시키면, 모든 단어가 기계 학습 작업에 대해 동일한 양의 정보를 전달하지 않는다는 사실을 종종 발견합니다.

180
00:23:09,520 --> 00:23:18,880
흔히 쓰이는 단어들 중 의미 있는 정보가 거의 또는 전혀 전달되지 않는 단어를 "불용어(stopwords)"라고 합니다.

181
00:23:16,080 --> 00:23:22,720
다음은 한국어의 불용어 목록 중 하나입니다.

182
00:23:20,400 --> 00:23:36,640
많은 자연어 처리 작업에서 이런 불용어를 제거하는 것은 일반적인 과정입니다.

183
00:23:35,039 --> 00:23:44,320
제가 여기서 보여드리는 것은 정말 널리 사용되는 영어의 짧은 불용어 목록 중 하나입니다.

184
00:23:41,919 --> 00:23:50,418
"I", "me", "my" 같은
대명사, "and", "of",

185
00:23:50,442 --> 00:24:00,400
"the"와 같은 접속사는 그다지
중요하지 않은 매우 흔한 단어입니다.

186
00:23:57,679 --> 00:24:15,840
불용어를 단순히 제거하기로 한 것은 흔히 찾아볼 수 있는 많은 자료들에 설명된 것보다 더 복잡하고 어려울 수 있습니다.

187
00:24:12,000 --> 00:24:22,400
거의 대부분의 자연어처리 실무자들은 미리 만들어진 불용어 목록을 사용합니다.

188
00:24:20,000 --> 00:24:32,960
이 그림은 "Upset Plot"이라고 불리며,  일반적으로 사용되는 영어 불용어 목록에 대한 교집합을 시각화합니다.

189
00:24:31,200 --> 00:24:40,159
세 가지 목록의 이름은 "snowball", "smart",  "iso" 인데요,

190
00:24:37,440 --> 00:24:44,001
막대 길이를 통해 목록의 길이가 표시되었고,  수직

191
00:24:44,025 --> 00:24:51,760
막대들로부터 목록들 간 공통된 단어들을 나타내고 있는데요,

192
00:24:48,799 --> 00:25:00,720
목록들의 길이가 모두 다르고, 모두 동일한 단어 집합을 포함하지 않는다는 것을 알 수 있습니다.

193
00:24:58,320 --> 00:25:20,480
불용어 말뭉치에 대해 기억해야 할 중요한 사항은 불용어 말뭉치는 완벽히 중립적인 상태에서 만들어진 것이 아니라, 특정 문맥 속에서 형성되었다는 것입니다.

194
00:25:16,960 --> 00:25:28,559
그래서 이러한 불용어 말뭉치는 편향되었을 수도 있습니다. 이 말뭉치들 역시 이들이 생성된 자연어 데이터 셋의

195
00:25:27,039 --> 00:25:37,440
특성을 반영하고 있습니다.

196
00:25:34,000 --> 00:25:45,600
다음은 smart말뭉치에는 있지만 snowball 말뭉치에는 없는 10개의 단어입니다.

197
00:25:43,039 --> 00:25:52,240
이 단어들은 모두 축약형임에 주목하세요. 하지만, 이 차이는  snowball에 축약형이 포함되어 있지 않기 때문은 아닙니다.

198
00:25:50,000 --> 00:25:57,329
snowball은 많은 축약형을 포함하고 있습니다.
또한, she's가 목록에 있는 것에 주목해보세요.

199
00:25:57,353 --> 00:26:05,600
이는, snowball 말뭉치에는 he's는 포함되어
있지만 she's는 포함되어 있지 않다는 것을 의미합니다

200
00:26:03,360 --> 00:26:07,679
이것은 편향의 한 예입니다.

201
00:26:05,600 --> 00:26:13,520
이러한 편향은 해당 목록이 매우 큰 텍스트 데이터 세트로부터 생성되기 때문에 발생합니다.

202
00:26:11,440 --> 00:26:22,480
말뭉치 생산자(lexicon creator)는 큰 말뭉치(corpus) 내에서 가장 자주 사용되는 단어를 살펴보고,

203
00:26:19,760 --> 00:26:33,840
사용 빈도의 기준치를 정한 뒤, 어떤 단어들을 포함시키고 제외시킬지를 결정하여 말뭉치를 만듭니다.

204
00:26:29,919 --> 00:26:42,347
많은 대규모 언어 데이터 셋에서
남성을 더 많이 표현하기 때문에 불용어

205
00:26:42,371 --> 00:26:54,799
목록에 "he's"가 있지만
"she's"는 없는 상황이 발생합니다.

206
00:26:51,840 --> 00:26:59,748
우리는 실무자로서 특정 영역에 적합한 것을 기준으로

207
00:26:59,772 --> 00:27:07,679
모델링과 분석에서의 많은 것들을 결정해야 합니다.

208
00:27:05,279 --> 00:27:12,960
불용어 목록을 선택하는 경우에도 마찬가지입니다.

209
00:27:10,720 --> 00:27:21,760
Tidymodels에서는 레시피(recipe)에 추가적으로 단계를 더하여 불용어 제거와 같은 전처리 단계를 구현할 수 있습니다.

210
00:27:19,200 --> 00:27:26,588
우선 우리가 사용할 변수들을 정의한 뒤,
텍스트를 토큰화하고, 불용어들을 제거합니다. 다른

211
00:27:26,612 --> 00:27:35,360
인수(argument)를 주지 않았기 때문에 기본으로
설정된 과정을 따라 불용어들이 제거될 것입니다.

212
00:27:32,640 --> 00:27:42,399
물론 기본으로 설정된 것과 다른 과정으로 불용어들을 제거할 수도 있으며, 우리의 특수한 상황에 적합하도록 불용어 리스트를 커스터마이징 할 수도 있습니다.

213
00:27:42,880 --> 00:27:51,782
이 표는 앞서 봤던 고등 법원 데이터를
사용해 작성년도를 예측하는 과제를

214
00:27:51,806 --> 00:28:02,080
위해 각기 다른 길이의 불용어 사전을
이용한 모델의 성능을 비교하고 있습니다.

215
00:27:59,279 --> 00:28:06,960
스노우볼 말뭉치가 가장 적은 수의 단어를 포함하고 있었는데

216
00:28:04,480 --> 00:28:10,320
가장 좋은 성능을 보였습니다.

217
00:28:06,960 --> 00:28:16,399
이 과제에서는 불용어를 적게 없앨수록 모델의 성능이 더 좋게 나왔습니다.

218
00:28:12,799 --> 00:28:21,424
불용어를 적게 없앨수록 모델의 성능이 더 좋아진다는
것은 모든 데이터나 문맥에 적용되는 것은 아니지만,

219
00:28:21,448 --> 00:28:29,919
어떤 불용어를 줄이느냐에 따라 모델의 성능이
눈에 띄게 달라진다는 점 자체는 명백한 사실입니다.

220
00:28:26,799 --> 00:28:34,068
뭐가 더 모델의 성능을 좋게 하는지 아는 방법은

221
00:28:34,092 --> 00:28:41,360
여러 시도를 직접 해보는 방법 밖에 없습니다.

222
00:28:39,279 --> 00:28:49,919
머신러닝은 사실 경험적 영역입니다. 뭐가 가장 좋은 방법인지, 그리고 그 이유는 무엇인지에 대해 우리는 알 수 없습니다.

223
00:28:47,440 --> 00:29:01,039
여러 다른 옵션들을 시도해보아야 무엇이 가장 나은 옵션인지 알 수 있습니다.

224
00:28:58,799 --> 00:29:07,760
이제부터 세번째 전처리 과정인 어간추출(stemming) 에 대해 이야기하고자 합니다.

225
00:29:05,279 --> 00:29:11,907
우리가 텍스트를 분석할 때는 한 어간(stem)에 대한

226
00:29:11,931 --> 00:29:18,559
여러 버전의 단어를 포함하고 있는 있는 문서들이 많습니다.

227
00:29:15,440 --> 00:29:23,348
영어로 예를 들어보자면, 우리가
'동물들(animals)'이라는 복수형과

228
00:29:23,372 --> 00:29:31,279
'동물(animal)'이라는 단수형을
똑같이 묶어 취급하고 싶다고 해봅시다.

229
00:29:27,919 --> 00:29:33,200
이런게 바로 어간추출의 핵심입니다.

230
00:29:31,279 --> 00:29:40,320
어간추출에 있어서 딱 정해진 옳은 방식은 없습니다.

231
00:29:37,279 --> 00:29:44,880
이 그래프는 세 개의 다른 영어 어간 추출 접근법을 보여주고 있는데요,

232
00:29:42,480 --> 00:29:49,988
단순히 복수형의 마지막 's'를 지워버리는
방법부터 좀 더 복잡하게 복수형 말미를

233
00:29:50,012 --> 00:29:57,520
처리하는 접근법까지 있습니다. 가운데는
s어간추출 (s stemmer)이라고 불리는데

234
00:29:54,480 --> 00:30:01,360
몇 가지의 규칙들을 가지고 있습니다.

235
00:29:58,640 --> 00:30:09,919
마지막은 어간추출 구현을 가장 잘했다고 알려진 모델 중 하나인 포터 알고리즘(Porter algorithm)입니다.

236
00:30:07,279 --> 00:30:19,840
앞서 보여드렸던 동물 묘사 텍스트 예시의 상위 20단어들에 대해 포터 어간추출방식이 나머지 두개의 어간추출 방식과 가장 다르다는 것을 볼 수 있습니다.

237
00:30:16,799 --> 00:30:25,600
species(종)이라는 단어가 어떻게 다르게 추출되었는지도 볼 수 있고요,

238
00:30:24,240 --> 00:30:31,934
animal(동물)과 predator(포식자)라는
단어도 다르게 취급되었고, live(살다),

239
00:30:31,958 --> 00:30:40,559
living(살다), life(삶), lives(살다)와 같은
단어의 묶음도 다르게 취급되었습니다.

240
00:30:38,960 --> 00:30:46,148
어간추출을 통해 단어들을 묶을 수 있고,
그렇게 같이 묶인 단어들을 인간들이 언어를

241
00:30:46,172 --> 00:30:53,360
이해하는 방식으로 이해할 수 있다고 믿기
때문에 실무자들은 어간추출에 관심이 많습니다.

242
00:30:51,600 --> 00:31:00,588
우선 이거 해, 그 다음에 이거
해, 이런 알고리즘적인 단계별

243
00:31:00,612 --> 00:31:09,600
규칙에 기반한 접근법을 어간
추출(stemming)이라고 하는데

244
00:31:07,039 --> 00:31:14,720
여러분은 어간 추출을 사용하실 수도 있고,

245
00:31:12,080 --> 00:31:21,588
사전을 이용해 어떤 단어들이 같이 묶일
수 있는지를 언어학적인 이해에 기반해

246
00:31:21,612 --> 00:31:31,120
접근하는 표제어 추출(lemmatization)을
사용하실 수도 있습니다.

247
00:31:28,559 --> 00:31:38,068
한국어에 대해 이런 과제를 수행하기 위한 대부분의 접근법은

248
00:31:38,092 --> 00:31:47,600
사전과 많은 양의 단어 학습에
기반한 표제어 추출 접근법입니다.

249
00:31:45,679 --> 00:31:54,868
이런 어간추출이나 표제어 추출이 아주 도움되는
좋은 과정이라는 생각이 드실 겁니다. 왜냐하면

250
00:31:54,892 --> 00:32:04,080
텍스트 데이터는 일반적으로 토큰의 갯수에
따라 엄청나게 많은 변수를 다뤄야 하기 때문이죠.

251
00:32:02,320 --> 00:32:09,440
이건 텍스트 데이터를 다루는 가장 전형적인 상황입니다.

252
00:32:05,840 --> 00:32:12,249
저는 동물 묘사 데이터로 행렬을 만들었습니다. 행렬은

253
00:32:12,273 --> 00:32:18,240
전형적으로 머신러닝 알고리즘에서 많이 이용하죠.

254
00:32:16,080 --> 00:32:23,293
얼마나 많은 변수(feature)들이 여기 있는지

255
00:32:23,317 --> 00:32:31,600
보세요. 거의 17,000개에 가까운 피처가 있습니다.

256
00:32:29,679 --> 00:32:36,640
저게 모델로 들어가는 변수의 갯수입니다.

257
00:32:33,519 --> 00:32:42,995
희소성(sparsity) 좀 보세요. 98%입니다.
이것은 엄청난 희소 행렬입니다.

258
00:32:43,019 --> 00:32:53,760
이게 저희 머신 러닝 알고리즘에 들어가서
지도 학습 머신 러닝 모델을 만들 데이터라니요.

259
00:32:51,600 --> 00:33:02,240
만일 제가 여기서 단어의 어간을 이용한다면, 어간 추출 접근법을 사용한다면, 우리는 단어 변수의 갯수를 수 천개 줄일 수 있습니다.

260
00:33:00,159 --> 00:33:08,508
어간추출을 통해 여전히 희소성은 그리
많이 낮출 수 없어 안타깝지만, 어간추출

261
00:33:08,532 --> 00:33:16,880
알고리즘이 단어를 묶는 방식을 통해
변수의 갯수를 아주 많이 줄일 수 있습니다.

262
00:33:15,200 --> 00:33:29,039
일반적으로 단어를 상징하는 변수의 갯수를 줄이는 것은 머신러닝 모델의 성능을 좋게한다고 하는데요,

263
00:33:26,640 --> 00:33:38,040
이는 어간 추출을 통해 어떠한 중요한 정보를 잃지
않았음을 가정합니다. 어간 추출이나 표제어 추출은 어떠한

264
00:33:38,064 --> 00:33:51,039
맥락에서는 많은 도움이 될 수 있지만, 이를 수행하는
전형적인 알고리즘들은 과하게 추출을 수행하는 경향이 있습니다.

265
00:33:48,320 --> 00:33:59,279
이 모델들은 민감도(sensitivity, recall, true positive rate) 개선에 집중하고 있는데,

266
00:33:56,880 --> 00:34:07,200
이는 특이도(specificity, true negative rate)나 정밀도(precision)를 악화시키죠.

267
00:34:04,640 --> 00:34:14,242
지도학습 머신러닝에서 어간 추출은 모델의
양성예측값 (PPV; Positive Predictive Value),

268
00:34:14,266 --> 00:34:25,760
정밀도(precision), 혹은 음성을 양성이라고 판단하지 않는
능력 (제가 맞게 말했으면 좋겠네요!) 등에 영향을 미칩니다.

269
00:34:22,800 --> 00:34:32,188
좀 더 구체적으로 말해보자면, 어간 추출은
모델이 양성을 찾아내는 능력을 향상 시킬 수

270
00:34:32,212 --> 00:34:43,200
있습니다. 예를 들어 동물 설명과 특정 섭취
질환이 관련있다면 이를 잘 찾아낼 수 있게 됩니다.

271
00:34:40,720 --> 00:34:52,240
그러나 만일 텍스트가 과하게 어간 추출되면, 모델은 음성을 찾아내는 능력을 잃습니다. 즉 그 특정 섭취 질환과 관련이 없는 동물 설명문을 찾아내는 능력을 잃습니다.

272
00:34:49,760 --> 00:34:58,112
어간 추출 알고리즘을 미세하게
조정할 방법이 없기 때문에, 텍스트

273
00:34:58,136 --> 00:35:08,400
데이터로 모델을 학습 시킬 때, 이러한
균형을 찾는 것은 아주 어렵습니다.

274
00:35:05,119 --> 00:35:17,440
지금 보여드리는 것과 같이 아주 간단한 텍스트의 전처리도 방식도 내부적으로는 굉장히 많은 계산을 필요로 할 수 있습니다.

275
00:35:15,280 --> 00:35:27,782
이렇게 불용어를 제거할 것인지, 또 어간
추출을 할 것인지에 대해 우리가 내리는 결정은 모든

276
00:35:27,806 --> 00:35:42,560
모델들에게, 그 모델이 아주 간단하고 전통적인
모델이든 딥러닝이든, 아주 큰 영향을 미칠 수 있습니다.

277
00:35:39,040 --> 00:35:48,788
이것은 우리가 이런 텍스트 변수 가공(feature
engineering)에 대해 배우고 가르치고

278
00:35:48,812 --> 00:35:58,560
쓰는 것이 실제로 더  강건한(robust) 통계
분석들에 기여할 수 있다는 것을 시사하기도 합니다.

279
00:35:56,320 --> 00:36:04,734
제가 아까 언급했던 텍스트 데이터의
희소성(sparsity)에 대해 좀 더 이야기

280
00:36:04,758 --> 00:36:14,320
하고 싶은데요, 희소성(sparsity)는
텍스트 데이터 만의 정말 고유한 특징입니다.

281
00:36:10,400 --> 00:36:17,748
정말 소수의 단어는 아주 많이 언급되고, 정말 많은

282
00:36:17,772 --> 00:36:25,119
단어들은 몇 번 사용되지 않는 말의 특성 때문에

283
00:36:22,640 --> 00:36:27,520
더 많은 문서를 추가하거나 새로운 단어들을 말뭉치(corpus)에 추가하면

284
00:36:25,119 --> 00:36:38,320
희소성(sparsity)은 지금 보이는 표에 나타나는 것과 같은 관계를 보입니다.

285
00:36:35,680 --> 00:36:44,588
새로운 단어들을 넣을수록
희소성은 빠르게 증가합니다. 그리고

286
00:36:44,612 --> 00:36:53,520
이 문서를 처리하는 데 필요한
메모리 용량도 빠르게 증가합니다.

287
00:36:51,920 --> 00:37:02,508
희소행렬(sparse matrix)처럼 높은 데이터를
위한 데이터 구조를 선택한다고 하더라도,

288
00:37:02,532 --> 00:37:13,119
이 데이터를 처리하기 위해 필요한 메모리의
용량은 여전히 비선형적으로 매우 빠르게 증가합니다.

289
00:37:09,920 --> 00:37:16,868
그리고 이렇게 큰 용량의 데이터를
사용하면 모델을 학습시키는 데 많은

290
00:37:16,892 --> 00:37:23,839
시간이 소요될 수도 있고, 아니면
여러분의 컴퓨터 용량보다도 더 커서

291
00:37:21,760 --> 00:37:31,200
비싼 돈을 주고 클라우드 서비스의 메모리를 사용해야 하는 등 많은 어려움을 야기할 것입니다.

292
00:37:27,680 --> 00:37:43,520
이러한 어려움들은 모델에 벡터를 쓰는 방식에 대해 생각해보게 했습니다.

293
00:37:40,240 --> 00:37:49,188
언어학자들은 사람들이 어떻게 언어를
구사하는지를 바탕으로 오랫동안 모델을 위한

294
00:37:49,212 --> 00:37:58,160
벡터를 연구하며 어떻게 하면 텍스트
데이터의 차원을 낮출 수 있는지 고민했습니다.

295
00:37:55,280 --> 00:38:04,960
이 인용문은 1957년으로 거슬러 올라갑니다.

296
00:38:02,960 --> 00:38:15,440
여기에서 제시하는 아이디어는, 데이터에 나타나는 것처럼 우리는 단어들을 희소하게 사용하지만, 단어들을 무작위로 사용하지는 않는다는 것입니다.

297
00:38:12,800 --> 00:38:17,359
우리가 사용하는 단어들은 독립적이지 않습니다.

298
00:38:15,440 --> 00:38:25,920
단어들은 독립적으로 사용되지 않으며 같이 사용된 단어들은 관계가 있습니다.

299
00:38:23,359 --> 00:38:40,160
우리는 이 관계를 사용해 고차원의 희소(sparse)한 데이터를 낮은 차원의 조밀한 데이터로 변환할 수 있습니다.

300
00:38:37,359 --> 00:38:49,839
여전히 몇 백 차원 정도일 수는 있지만 몇 천, 몇 만 차원에 비하면 아주 낮은 차원입니다.

301
00:38:48,160 --> 00:39:05,839
우리는  단어 갯수와 행렬분해 혹은 좀 더 인공 신경망을 필요로 하는 좀 더 복잡한 수학을 다룰 수 있는 통계 모델에 정말 고차원의 벡터 공간(space)을 넣고,

302
00:39:03,680 --> 00:39:21,839
낮은 차원의 벡터 공간을만들 수 있습니다. 이 벡터 공간은 어떤 단어들이 같이 쓰이는지에 대한 정보를 담고 있는 벡터를 사용해서 생성되었다는 점에서 특별합니다.

303
00:39:18,560 --> 00:39:26,480
우리는 단어가 곁에 두는 친구를 보면 그 단어를 알 수 있습니다.

304
00:39:24,079 --> 00:39:35,119
이런 단어 벡터(word vector)나 단어 임베딩(word embedding)을 생성하거나 학습하기 위해서는 큰 텍스트 데이터가 필요합니다.

305
00:39:32,000 --> 00:39:53,839
제가 지금 보여드리는 이 표는 미국의 소비자금융보호국(consumer financial protection bureau)을 향한 컴플레인이 담긴 말뭉치로 부터 생성된 임베딩입니다.

306
00:39:51,680 --> 00:40:13,119
소비자금융보호국은 미국 정부 부처로, 사람들이 신용 카드, 모기지, 학생 대출과 같은 금융 상품에 대해  컴플레인하고 신고할 수 있는 기관입니다.

307
00:40:11,119 --> 00:40:24,000
예를 들어 신용카드에 뭔가 문제가 생겼다든지, 모기지에 문제가 생겼다든지, 어떤 회사가 공정하지 않다든지와 같은 컴플레인들이죠.

308
00:40:20,000 --> 00:40:27,839
저는 이 모든 컴플레인을 이용해 모델을 만들었습니다.

309
00:40:25,440 --> 00:40:32,400
고차원 벡터 공간을 저차원 벡터 공간으로 만들죠.

310
00:40:30,160 --> 00:40:44,079
그리고 그 공간을 들여다보면 우리는 어떤 단어들이 이 공간 내의 어떤 다른 단어들과 연관이 있는지 이해할 수 있습니다.

311
00:40:40,240 --> 00:41:09,680
새로운 임베딩 벡터 공간은 월(month)이라는 단어는 년도(year), 월의 복수형(months), 월간(montly), 할부(installment payment) 같은 단어들과 가장 가깝다고 정의되었고, 이는 신용카드나 모기지 등 금융 상품 맥락에서 일리가 있습니다.

312
00:41:06,720 --> 00:41:26,240
에러(error)는 실수(mistake),  사무적 오류가 있다 할 때 사무적(clerical), 문제(problem), 모기지에 일시적인 오류가 있었다 할 때 일시적 오류 (glitch),

313
00:41:23,440 --> 00:41:30,880
오해(miscommunication, misunderstanding) 같은 단어들과 가깝습니다.

314
00:41:28,319 --> 00:41:33,599
이런 단어들은 비슷하게 사용됩니다.

315
00:41:31,599 --> 00:41:42,240
이런 임베딩을 만들기 위해서는 상당한 크기의 데이터가 필요하기 때문에 여러분들은 임베딩을 직접 만드실 필요는 없습니다.

316
00:41:39,520 --> 00:41:54,079
여러분은 접근 권한이 없는 큰 말뭉지 데이터에 대해 접근 권한을 가지고 있는 사람이 이를 이용해 만든 임베딩을 사용하면 됩니다.

317
00:41:51,359 --> 00:41:55,599
누군가가 미리 만들어둔 임베딩도 살펴봅시다.

318
00:41:54,079 --> 00:42:04,560
이 표는 에러(error)와 비슷한 단어를 글로브 임베딩(glove embedding)에서 찾은 결과입니다.

319
00:42:02,800 --> 00:42:27,280
글로브 임베딩은 모든 위키피디아 데이터, 구글의 모든 뉴스 데이터와 같이 엄청나게 큰 데이터를 사용해 만들어진 임베딩입니다.

320
00:42:24,800 --> 00:42:40,839
어떤 단어들은 앞서 봤던 리스트에도 있었지만, 이 리스트에는 이제 사무적(clerical)이나 오해(miscommunication), 불일치(discrepancy)와 같이 금융 분야에서 주로 사용되는 단어들은 없습니다.

321
00:42:38,640 --> 00:42:54,319
그리고 계산(calculation), 확률(probability)과 같이 사람들이 금융 상품에 대한 컴플레인에서는 자주 말하지는 않는 단어들이 표에 있습니다.

322
00:42:51,599 --> 00:43:10,400
여기서 우리는 이런 것들이 어떻게 동작하는지 알 수 있습니다. 직접 말뭉치를 가지고 임베딩을 만든 앞 예시에서는 아주 구체적인 맥락 속에서 단어들의 관계를 알 수있었고,

323
00:43:06,800 --> 00:43:16,720
뒤에 나온 예시에서는 일반적인 데이터 셋으로 누군가가 학습시킨 것을 사용했습니다.

324
00:43:13,119 --> 00:43:27,359
임베딩은 텍스트 데이터의 아주 큰 말뭉치를 이용해 학습되고, 이 때 이 말뭉치의 특성이 임베딩의 일부가 됩니다.

325
00:43:24,240 --> 00:43:35,839
일반적으로 기계 학습은 훈련 데이터에 많은 영향을 받습니다.

326
00:43:32,240 --> 00:43:40,319
그리고 이러한 민감도는 텍스트 데이터를 다룰 때 보다 명확하게 드러납니다.

327
00:43:38,160 --> 00:44:01,440
아마도 단어 임베딩은 그런 것의 아주 고전적인 예시일 것입니다. 말뭉치 속에 담긴 사람들이 가지고 있는 편견이나 편향들이 임베딩에도 반영된다는 것이 밝혀졌습니다.

328
00:43:59,040 --> 00:44:14,640
우리가 쉽게 접할 수 있는 임베딩에서도 그 편향을 찾을 수 있습니다.

329
00:44:12,200 --> 00:44:23,280
우리는 임베딩에서 아프리카계 미국인들이 주로 쓰는 이름이

330
00:44:19,680 --> 00:44:30,240
유럽계 미국인이 쓰는 이름들보다 부정적인 감정과의 연관성이 더 높았음을 볼 수 있습니다.

331
00:44:27,680 --> 00:44:38,640
여성의 이름은 가족과 더 밀접하게 연관되어 있고, 남성의 이름은 경력과 더 밀접한 연관을 가지고 있습니다.

332
00:44:36,079 --> 00:44:44,079
여성이라는 단어는 예술과 더 관련성이 높았고, 남성이라는 단어는 과학과의 관련성이 더 높았습니다.

333
00:44:42,640 --> 00:44:53,828
실제로 단어 임베딩에는 우리가
가지고 있는 편향이 너무 잘 드러나서,

334
00:44:53,852 --> 00:45:05,040
사회적 태도들의 변화를 수치화하는
데 임베딩을 사용할 수도 있습니다.

335
00:45:01,440 --> 00:45:11,119
단어 임베딩은 어쩌면 좀 과장되거나 극단적인 예시일 수도 있지만

336
00:45:08,160 --> 00:45:19,868
어쨌든 텍스트 데이터에 대한 변수 가공
단계에서 내리는 모든 결정들이 모델의 성능이든,

337
00:45:19,892 --> 00:45:31,599
이 모델의 적절성과 공평성이든, 모델의
결과에 아주 큰 영향을 미친다는 것은 사실입니다.

338
00:45:29,520 --> 00:45:36,088
이 점을 고려했을 때, 텍스트 데이터를 통해
필요한 변수들을 만들어내는 전처리 단계에서

339
00:45:36,112 --> 00:45:43,520
우리에게는 아주 많은 선택지와 또 그에 따른
상당한 책임이 있다는 점을 아셨으면 좋겠습니다.

340
00:45:39,680 --> 00:45:48,391
제 조언은 여러분이 꽤 깊이 이해할 수
있는 간단한 모델에서부터 시작하시라는

341
00:45:48,415 --> 00:45:59,040
것입니다. 모델을 학습시키거나 튜닝할 때에
반드시 통계적으로 타당한 것들을 선택하세요.

342
00:45:56,720 --> 00:46:10,400
그래서 다양한 시도에 따른 모델의 성능 향상에 대해 잘 이해하고 있을 수 있도록 해야 합니다.

343
00:46:07,119 --> 00:46:20,240
모델 설명 가능성 (model explainability)도구나 프레임워크를 사용해서 좀 명확히 이해되지 않는 모델들에 대해 이해해보세요.

344
00:46:18,000 --> 00:46:24,148
제 동료와 저는 이러한 것들을 주제로 글을
썼고, 어떻게 Tidymodels로 이런 과정들을

345
00:46:24,172 --> 00:46:30,319
거칠 수 있는지에 대해서도 썼으며, 앞으로도
계속해서 쓸 것이니 참고하실 분들은 참고해주세요.

346
00:46:28,720 --> 00:46:38,960
그럼 제 이야기는 여기서 이만 마무리 하겠습니다. 들어주셔서 정말 감사하고,

347
00:46:36,560 --> 00:46:45,119
또 한국 R 사용자회 주최측 분들께 다시 한 번 감사드립니다.

348
00:46:42,560 --> 00:46:51,240
끝으로 RStudio의 Tidymodels 팀 동료들에게도, 또 공동저자인 Emil Hvitfeldt에게도 고맙다는 말을 전하고 싶습니다.

