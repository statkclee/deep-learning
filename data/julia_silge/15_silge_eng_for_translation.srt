1
00:00:01,120 --> 00:00:08,639
Hi my name is julia silge.
I'm a data scientist and software engineer at RStudio.

2
00:00:06,080 --> 00:00:18,080
I'd like to thank the the organizers of the R User Group in Korea. And thank so much for having me.

3
00:00:15,519 --> 00:00:29,359
I am so happy to be speaking specifically today about creating features for machine learning from text data for a couple of reasons.

4
00:00:27,199 --> 00:00:42,879
Having a better understanding of what we do to take text data and then to make it appropriate as an input for machine learning algorithms has many benefits

5
00:00:39,680 --> 00:00:50,719
both if you are directly getting ready to train a model or if you're at the beginning of some text analysis project

6
00:00:48,399 --> 00:00:55,440
or if you are trying to understand the behavior of a model that

7
00:00:52,800 --> 00:01:06,640
you're interacting with some way which is something that we do in our work as data scientists or in our in our daily lives more and more.

8
00:01:03,359 --> 00:01:15,759
so when we build models for text either supervised or unsupervised we start with something like this.

9
00:01:11,600 --> 00:01:24,560
this is some example text data that I'll use a couple of times during this talk that describes some animals.

10
00:01:21,040 --> 00:01:34,400
I'm using some text data so to me as an english speaker looks familiar.

11
00:01:30,960 --> 00:01:43,920
I am as someone who uses a human language so I look at this and I can read it I could speak it aloud and I understand.

12
00:01:41,280 --> 00:01:45,759
I can interpret it what it means

13
00:01:43,920 --> 00:01:59,840
so this kind of data this sort of natural language data is being generated all the time in all kinds of languages in all kinds of contexts.

14
00:01:56,880 --> 00:02:26,720
whether you work in healthcare in tech in finance basically any kind of organization this sort of text data is being generated by customers by clients by internal stakeholders inside of a business by people taking surveys via social media via business processes.

15
00:02:24,319 --> 00:02:35,360
in all this natural language there's information latent in that text data that can be used to make better decisions.

16
00:02:33,519 --> 00:02:47,440
However, computers are not great at looking at this and doing math on language as it's represented like this.

17
00:02:45,440 --> 00:02:55,440
instead language has to be dramatically transformed to some kind of machine readable numeric representation.

18
00:02:53,680 --> 00:03:01,840
that looks more like this what I'm showing here on the screen to be ready for almost any kind of model.

19
00:02:59,760 --> 00:03:22,159
so I spent a fair amount of time working on software for people to be able to do exploratory data analysis, visualization, summarization tasks like that with text data in a tidy format where we have one observation per row.

20
00:03:19,040 --> 00:03:34,879
I love using tidy data principles for text analysis especially during those exploratory phases of an analysis when it comes time to build a model.

21
00:03:32,640 --> 00:03:52,319
often what the underlying mathematical implementation really needs is typically something like this which is a way to this particular representation is called the document term matrix.

22
00:03:49,760 --> 00:03:55,760
so the exact representation may differ from what I've shown here.

23
00:03:53,840 --> 00:04:04,720
what I have here is we're weighting things by counts so each row in this matrix is a document.

24
00:04:02,640 --> 00:04:07,439
each column is a is a word.

25
00:04:04,720 --> 00:04:19,359
A token and the numbers represent counts how many times does each document, use
each word you could weight it in a different way, or use TF-IDF instead of counts.

26
00:04:16,720 --> 00:04:41,120
or you might keep sequence information if you're interested in building a deep learning model but basically for all kinds of text modeling from simpler models like Naive Bayes models which work well for text to word embeddings to really the most state-of-the-art kind of work that's happening today like transformers for text data.

27
00:04:39,520 --> 00:04:53,680
we have to heavily feature engineer and process language to get it to some kind of representation. that's suitable for machine learning algorithms.

28
00:04:50,960 --> 00:05:04,800
so I work on an open source framework in R for modeling and machine learning that's called Tidymodels and the examples that I'll be showing today use Tidymodels code.

29
00:05:02,320 --> 00:05:21,360
some of the specific goals of the Tidymodels project are to provide a consistent flexible framework for real world modeling practitioners people who are you know doing that are dealing with real world data.

30
00:05:19,199 --> 00:05:37,360
those who are just starting out to those who are very experienced in modeling and the goal is to harmonize the heterogeneous interfaces that exist within R and to encourage good statistical practice.

31
00:05:35,360 --> 00:05:52,800
I'm glad to get to show you some of what I work on and build and how we apply it to text modeling but a lot of what I will talk about today isn't very specific to Tidymodels or even to R.

32
00:05:50,880 --> 00:06:00,479
I know this is an R user group but what we're going to talk
about and focus on is a little more conceptual and basic thing.

33
00:05:57,440 --> 00:06:06,800
how do we transform text into predictors for machine learning?

34
00:06:04,560 --> 00:06:19,720
I am excited though to talk about Tidymodels and Tidymodels if you have not used it before is a meta package in a similar way that the Tidyverse is a meta package.

35
00:06:16,319 --> 00:06:33,039
so if you've ever typed library Tidyverse and then you've used ggplot2 for visualization, dplyr for data manipulation, Tidymodels works in a similar way.

36
00:06:30,560 --> 00:06:39,840
there are different packages inside of it that are used for different purposes.

37
00:06:37,280 --> 00:06:48,000
so the pre-processing or the feature engineering is part of a broader model process.

38
00:06:45,280 --> 00:06:56,240
that process starts really with with exploratory data analysis that helps us decide what kind of model.

39
00:06:54,080 --> 00:07:03,120
we will build and then it comes to completion.

40
00:06:59,280 --> 00:07:09,280
I think I would argue with model evaluation when you measure how well your model performed.

41
00:07:06,479 --> 00:07:38,560
Tidymodels as a piece of software is made up of our packages each of which has a specific focus like our sample is for re-sampling data to be able to create bootstrap resamples cross-validation resamples all different kinds of resamples you might want to use to train and evaluate models the tune package is for hyper parameter tuning.

42
00:07:34,800 --> 00:07:52,720
As you might guess from the name one of these packages is for feature engineering for a data preprocessing feature engineering and it is the one that is called recipes.

43
00:07:49,360 --> 00:08:25,759
In Tidymodels we capture this idea of data pre-processing and feature engineering in the concept of a pre-processing recipe that has steps so you choose ingredients or variables that you're going to use, then you define the steps that go into your recipe, then you prepare them using training data, and then you can apply that to any data set like testing data or new data at prediction time.

44
00:08:23,039 --> 00:08:34,560
The variables or ingredients that we use in modeling come in all kinds of shapes and sizes including text data.

45
00:08:32,000 --> 00:09:05,279
Some of the techniques and approaches that we use for pre-processing text data are the same as for any other kind of data that you might use like non-text data, numeric data,  categorical data.
some of it is the same but some of what you need to know to be able to do a good job in this process for text is different and is specific to the nature of what language data is like.

46
00:09:03,680 --> 00:09:14,720
I've written a book with my co-author Emile Hvitfeldt
on "supervised machine learning for text analysis and R".

47
00:09:11,360 --> 00:09:25,920
Fully the first third of the book focuses on how we transform the natural language that we have in text data into features for modeling.

48
00:09:23,600 --> 00:09:40,240
The middle section is about how we use these features in simpler or more traditional
machine learning models like regularized regression or support vector machines.

49
00:09:37,360 --> 00:10:15,279
Then the last third of the book talks about how we use deep learning models with text data so deep learning models still require these kinds of transformations from natural language into features as input for these kinds of models but deep learning models are often able to inherently learn structure of features from text in ways that those more traditional or simpler machine learning models are not.

50
00:10:12,800 --> 00:10:20,560
This book is now complete and available as of this month as of november.

51
00:10:18,480 --> 00:10:30,720
Folks are getting their first paper copies and also this book is available in its entirety at smalltar.com.

52
00:10:28,560 --> 00:10:43,839
If you're new to dealing with text data understanding these fundamental pre-processing
approaches for text will set you up for being able to train effective models.

53
00:10:41,760 --> 00:10:58,000
if you're really experienced with text data and if you've dealt with it a lot, already you've probably noticed the existing resources or literature.

54
00:10:54,480 --> 00:11:17,519
Whether that's books or tutorials or blog posts is quite sparse when it comes to detailed thoughtful explorations of
how these pre-processing steps work and how choices made in these feature engineering steps impact our model output.

55
00:11:15,600 --> 00:11:28,800
so let's walk through several of some of these like basic feature engineering approaches and how they work and what they do.

56
00:11:27,040 --> 00:11:31,920
Let's start out with tokenization.

57
00:11:28,800 --> 00:11:52,320
Typically one of the first steps in transfer information from natural language to machine learning
feature for really any kind of text analysis including exploratory data analysis or building a model.

58
00:11:49,040 --> 00:11:55,360
Anything is tokenization.

59
00:11:52,320 --> 00:12:02,079
In tokenization we take an input some string, some character vector and some kind of token type some meaningful unit of text.

60
00:11:59,440 --> 00:12:12,480
We're interested in a word and we split the input pieces into tokens that correspond to the type we're interested in.

61
00:12:09,519 --> 00:12:19,040
Most commonly the meaningful unit or type of token
that we want to split text into units of is a word.

62
00:12:17,839 --> 00:12:30,880
This might seem straightforward or obvious but it turns out it's difficult to clearly define what a word is for many or even most languages.

63
00:12:29,519 --> 00:12:40,959
Many languages do not use white space between words at all which presents a challenge for tokenization.

64
00:12:38,240 --> 00:13:04,079
Even languages that do use white space like English and Korean often have particular examples that are ambiguous
like contractions in English or more accurately considered two words the way particles are used in Korean.

65
00:13:01,519 --> 00:13:13,839
How pronouns and negation words are written in romance languages like italian and french where they're stuck together and really maybe they should be considered two words.

66
00:13:12,399 --> 00:13:32,720
Once you have figured out what you're going to do and you make some choices and you tokenize your text, then it's on its way to being able to be used in exploratory data analysis or unsupervised algorithms or as features for predictive modeling which is what we're talking about here.

67
00:13:30,320 --> 00:13:45,199
What these results show here, so these results are from a regression model trained on descriptions of media from artwork in the Tate collection in the UK.

68
00:13:43,279 --> 00:14:02,079
What we're predicting in what we are predicting is when what year was a piece of art created based on the the medium that the artwork was created with and the medium is described with a little bit of text.

69
00:14:00,079 --> 00:14:29,920
We see here that artwork created using graphite watercolor and engraving was more likely to be created earlier that though. That is more likely to come from older art and artwork that is created using photography screen point or sorry screen print screen printing and and dung and glitter are more likely to be created later.

70
00:14:27,760 --> 00:14:37,519
There this is more likely to come from contemporary art modern art.

71
00:14:33,440 --> 00:15:17,279
The the way that we tokenize this text we started with natural human generated texts of people writing out the descriptions of the the media that these art pieces of art were created with and the way we tokenized that natural human generated text that we started with has a big impact on what we learned from it. If we tokenized in a different way we would have gotten different results in terms of performance like how accurately we were able to predict predict the year and also in terms of how we interpret the model like what is it that we're able to learn from it.

72
00:15:15,279 --> 00:15:31,279
This is one kind of tokenization to the single word, but also all another way to tokenize instead of breaking up into single words or unigrams we can tokenize to n-grams.

73
00:15:27,440 --> 00:15:37,920
An n-gram is a continuous sequence of N items from a given sequence of texts.

74
00:15:35,199 --> 00:15:54,959
This shows that same piece of little bit of text i'm describing this animal divided up into
bi-grams or n-grams of two tokens so notice how the words in the bi-grams overlap.

75
00:15:51,519 --> 00:16:11,120
The word 'collard' appears in both of the first bigrams the 'collared', 'collared peccary' also
referred to so n-gram tokenization slides along the text to create overlapping sets of tokens.

76
00:16:07,040 --> 00:16:14,480
This shows tri-grams for the same thing.

77
00:16:11,120 --> 00:16:23,360
Using uni-grams one word is faster and more efficient but we don't capture information about word order.

78
00:16:20,560 --> 00:16:41,440
I'm using a higher value two or three or even more keeps more complex information about word order and concepts that are described in multi-word phrases.

79
00:16:37,680 --> 00:16:56,480
But the vector space of tokens increases dramatically that corresponds to a reduction in token counts.
We don't count each token as very many times and that means depending on your particular data set.

80
00:16:55,120 --> 00:17:00,399
You might not be able to get good results.

81
00:16:58,160 --> 00:17:14,079
Combining different degrees of n-grams can allow you to extract different levels of detail from text so uni-grams can tell you which individual words have been used a lot of times.

82
00:17:11,439 --> 00:17:23,360
Some of those words might be overlooked in bi-gram or tri-gram 'crowns' if they don't co-appear with other words as often.

83
00:17:23,520 --> 00:17:39,679
This plot compares model performance for a Lasso regression model predicting the year of supreme court opinions the United States supreme court opinions with three different degrees of n-grams.

84
00:17:37,840 --> 00:17:51,840
What we're doing here is we are taking the text of the writings of the United
States supreme court and we're predicting when did it when was that text written.

85
00:17:49,760 --> 00:17:58,799
Can we predict how old a piece of text is from the contents of the text?

86
00:17:55,360 --> 00:18:13,600
Holding the number of tokens constant at a thousand using uni-grams alone performs best for this corpus of opinions from the United States supreme court.

87
00:18:11,280 --> 00:18:25,760
This is not always the case depending on the kind of model you use the data set itself we might see the best performance combining uni-grams and bi-grams or maybe some other option.

88
00:18:23,679 --> 00:18:43,039
In this case if we wanted to incorporate some of that more complex information that we have in the
bi-grams and the tri-grams, we probably would need to increase the number of tokens in the model quite a bit.

89
00:18:40,160 --> 00:18:51,039
Keep in mind when you look at results like these that identifying n-grams is computationally expensive.

90
00:18:48,960 --> 00:19:00,640
This is especially compared to the amount of like a model the
improvement in model performance that we often see.

91
00:18:58,720 --> 00:19:03,600
if we see some you know modest

92
00:19:00,640 --> 00:19:05,760
improvement by adding in bigrams it's

93
00:19:03,600 --> 00:19:08,640
important to keep in mind how much

94
00:19:05,760 --> 00:19:11,039
improvement we see relative to

95
00:19:08,640 --> 00:19:13,200
how long it takes to

96
00:19:11,039 --> 00:19:15,840
identify bi-grams and then train that

97
00:19:13,200 --> 00:19:18,480
model so for example for this data set

98
00:19:15,840 --> 00:19:20,799
of supreme court opinions where we held

99
00:19:18,480 --> 00:19:24,480
the number of tokens constant so the

100
00:19:20,799 --> 00:19:27,120
model training had the same number of tokens in it

101
00:19:25,919 --> 00:19:35,200
using bi-grams plus uni-grams takes twice as long to train

102
00:19:33,280 --> 00:19:38,240
to do the feature engineering and the

103
00:19:35,200 --> 00:19:42,000
training than only uni-grams and adding

104
00:19:38,240 --> 00:19:44,320
in tri-grams as well takes almost five

105
00:19:42,000 --> 00:19:46,799
times as long as training on uni-grams

106
00:19:44,320 --> 00:19:50,640
alone so this is a computationally

107
00:19:46,799 --> 00:19:50,640
expensive thing to do

108
00:19:51,760 --> 00:19:56,559
going in the other direction

109
00:19:53,760 --> 00:19:58,960
we can tokenize to units smaller than

110
00:19:56,559 --> 00:20:00,880
words so like these are what are called

111
00:19:58,960 --> 00:20:05,440
character shingles so we take words

112
00:20:03,280 --> 00:20:08,480
the collared peccary

113
00:20:05,440 --> 00:20:10,880
and we can instead of looking at words

114
00:20:08,480 --> 00:20:13,120
we can go down and look at sub word

115
00:20:10,880 --> 00:20:16,000
information there's multiple different

116
00:20:13,120 --> 00:20:17,440
ways to break words up into sub words

117
00:20:16,000 --> 00:20:20,880
that are appropriate for machine learning

118
00:20:18,480 --> 00:20:23,440
and often these kinds of approaches

119
00:20:20,880 --> 00:20:25,200
or algorithms have the benefit of being

120
00:20:23,440 --> 00:20:31,360
able to encode unknown or new words

121
00:20:29,039 --> 00:20:33,760
at prediction time so when it when it's

122
00:20:31,360 --> 00:20:36,400
time to make a prediction on new data

123
00:20:33,760 --> 00:20:39,200
it's not it's not uncommon for there to

124
00:20:36,400 --> 00:20:41,200
be new vocabulary words at that time and

125
00:20:39,200 --> 00:20:42,640
if we didn't see them in the training

126
00:20:41,200 --> 00:20:45,600
data you know what are we going to do

127
00:20:42,640 --> 00:20:49,360
about those new words when we train

128
00:20:45,600 --> 00:20:52,000
using subword information often we can

129
00:20:49,360 --> 00:20:54,960
handle those new words if we saw the subword

130
00:20:52,880 --> 00:20:58,400
in our training data set so

131
00:20:55,760 --> 00:21:01,440
using this kind of subword information

132
00:20:58,400 --> 00:21:04,720
is a way to incorporate morphological

133
00:21:01,440 --> 00:21:06,240
sequences into our models of you know

134
00:21:04,720 --> 00:21:09,280
various kinds of this is something that applies to

135
00:21:08,080 --> 00:21:13,360
various languages not just english

136
00:21:11,919 --> 00:21:16,240
so these results are for a

137
00:21:13,360 --> 00:21:20,000
classification model with a data set of

138
00:21:16,240 --> 00:21:22,960
very short texts it's just the names of

139
00:21:20,000 --> 00:21:27,120
post offices in the United States so super short

140
00:21:24,240 --> 00:21:28,799
and the goal of the model was to predict

141
00:21:27,120 --> 00:21:35,280
the post office located in hawaii

142
00:21:32,880 --> 00:21:37,280
in the middle of the pacific ocean or

143
00:21:35,280 --> 00:21:41,360
it located in the rest of the united states

144
00:21:38,159 --> 00:21:46,480
so I created features for the model that are

145
00:21:42,159 --> 00:21:49,200
subwords of these post office names

146
00:21:46,480 --> 00:21:53,120
and we end up learning that the names

147
00:21:49,200 --> 00:21:57,440
that start with h and p or contain that ale

148
00:21:54,240 --> 00:22:00,240
sub word are more likely to be in hawaii

149
00:21:57,440 --> 00:22:06,480
and the sub words a and d and ri and ing are are

150
00:22:04,559 --> 00:22:09,360
more likely to come from the post office

151
00:22:06,480 --> 00:22:11,440
that are outside of hawaii

152
00:22:09,360 --> 00:22:14,480
so this is an example of how we

153
00:22:11,440 --> 00:22:17,280
tokenized differently and we're able to

154
00:22:14,480 --> 00:22:19,600
learn something new we're able to learn something

155
00:22:18,559 --> 00:22:24,320
different so in Tidymodels we collect all these

156
00:22:21,840 --> 00:22:26,880
kinds of decisions about tokenization

157
00:22:24,320 --> 00:22:29,919
and code that looks like this

158
00:22:26,880 --> 00:22:33,760
so we start with a recipe that specifies what

159
00:22:30,799 --> 00:22:35,679
variables or ingredients that we'll use

160
00:22:33,760 --> 00:22:38,640
and then we define these preprocessing

161
00:22:35,679 --> 00:22:41,600
steps so even at this first

162
00:22:38,640 --> 00:22:44,159
and arguably you know simple and basic

163
00:22:41,600 --> 00:22:47,120
step the choices that we make affect our

164
00:22:44,159 --> 00:22:50,400
modeling results in a big way

165
00:22:47,120 --> 00:22:52,080
the next pre-processing um step that

166
00:22:50,400 --> 00:22:56,240
I want to talk about is stop words

167
00:22:53,600 --> 00:23:00,320
so once we have split text

168
00:22:56,240 --> 00:23:03,360
into tokens we often find that not

169
00:23:00,320 --> 00:23:06,720
all words carry the same amount of

170
00:23:03,360 --> 00:23:09,520
information if maybe any information at

171
00:23:06,720 --> 00:23:11,840
all actually for a machine learning task

172
00:23:09,520 --> 00:23:13,919
so common words that carry little or

173
00:23:11,840 --> 00:23:16,080
perhaps no meaningful information are

174
00:23:13,919 --> 00:23:18,880
called stopwords

175
00:23:16,080 --> 00:23:20,400
so this is one of the stopword lists

176
00:23:18,880 --> 00:23:22,720
that's available for

177
00:23:20,400 --> 00:23:28,000
Korean so it's common advice and practice

178
00:23:24,960 --> 00:23:31,200
to say hey just remove just remove

179
00:23:28,000 --> 00:23:35,039
remove these stopwords for a lot of

180
00:23:31,919 --> 00:23:36,640
natural language processing tasks

181
00:23:35,039 --> 00:23:39,600
what I'm showing here

182
00:23:36,640 --> 00:23:41,919
is the entirety of one of the shorter

183
00:23:39,600 --> 00:23:44,320
english stopword lists that's used

184
00:23:41,919 --> 00:23:52,080
really broadly so you know it's words like I

185
00:23:47,200 --> 00:23:54,400
me my pronouns conjunctions and of the

186
00:23:52,080 --> 00:23:57,679
and these are very common words

187
00:23:54,400 --> 00:24:00,400
that are not considered super important

188
00:23:57,679 --> 00:24:04,400
the decision though to just remove

189
00:24:00,400 --> 00:24:07,279
stopwords is often more involved and

190
00:24:04,400 --> 00:24:08,960
perhaps more fraught than what you'll

191
00:24:07,279 --> 00:24:12,000
than what you'll find reflected in a lot

192
00:24:08,960 --> 00:24:15,840
of resources that are out there

193
00:24:12,000 --> 00:24:18,880
so almost all the time real world NLP

194
00:24:15,840 --> 00:24:22,400
practitioners use pre-made stopword lists

195
00:24:20,000 --> 00:24:26,400
so this plot visualizes

196
00:24:22,400 --> 00:24:28,720
set intersections for three common

197
00:24:26,400 --> 00:24:31,200
stopword lists in english

198
00:24:28,720 --> 00:24:32,960
in what is called an upset plot

199
00:24:31,200 --> 00:24:37,440
so the three lists are called the

200
00:24:32,960 --> 00:24:40,159
snowball list smart and the iso list

201
00:24:37,440 --> 00:24:41,840
so you can see the the lengths of the

202
00:24:40,159 --> 00:24:43,679
list are represented by the length of

203
00:24:41,840 --> 00:24:46,400
the bars and then we see the

204
00:24:43,679 --> 00:24:48,799
intersections which words are in common

205
00:24:46,400 --> 00:24:51,760
on these lists by the by the vertical

206
00:24:48,799 --> 00:24:55,760
bars so the lengths of the list are quite different

207
00:24:52,960 --> 00:24:58,320
and also notice they don't all contain

208
00:24:55,760 --> 00:25:00,720
the same sets of words

209
00:24:58,320 --> 00:25:03,120
the important thing to remember about

210
00:25:00,720 --> 00:25:06,880
stopword lexicons is that

211
00:25:04,240 --> 00:25:10,000
they are not created in some

212
00:25:06,880 --> 00:25:16,080
neutral perfect setting but instead they are

213
00:25:12,799 --> 00:25:20,480
they are context specific

214
00:25:16,960 --> 00:25:23,600
they they can be biased both of these

215
00:25:20,480 --> 00:25:27,039
things are true because they are lists

216
00:25:23,600 --> 00:25:28,559
created from large data sets of language

217
00:25:27,039 --> 00:25:32,159
so they reflect the characteristics of the data used in

218
00:25:30,640 --> 00:25:37,440
their creation so this is

219
00:25:34,000 --> 00:25:40,559
the ten words that are in the english

220
00:25:37,440 --> 00:25:43,039
language smart lexicon but not in the

221
00:25:40,559 --> 00:25:45,600
English snowball lexicon

222
00:25:43,039 --> 00:25:47,520
so notice that they're all contractions

223
00:25:45,600 --> 00:25:50,000
but that's not because the snowball

224
00:25:47,520 --> 00:25:52,240
exchange doesn't include contractions

225
00:25:50,000 --> 00:25:56,960
it has a lot of them also notice that it has

226
00:25:55,440 --> 00:26:01,360
that she's is on this list and so that means that

227
00:25:58,960 --> 00:26:03,360
that list has he's but it does not have

228
00:26:01,360 --> 00:26:05,600
the list she's

229
00:26:03,360 --> 00:26:07,679
so this is an example of that

230
00:26:05,600 --> 00:26:11,440
The bias I mentioned that occurs because

231
00:26:07,679 --> 00:26:13,520
these lists are created from large data

232
00:26:11,440 --> 00:26:19,760
sets of text lexicon creators look at the most

233
00:26:16,400 --> 00:26:22,480
frequent words in some big corpus of

234
00:26:19,760 --> 00:26:24,720
language they make a cut off

235
00:26:22,480 --> 00:26:28,400
and then some decisions about what to include or

236
00:26:26,080 --> 00:26:29,919
exclude you know

237
00:26:28,400 --> 00:26:33,840
based on the list that they

238
00:26:29,919 --> 00:26:36,799
have and you end up here so because

239
00:26:33,840 --> 00:26:39,760
in many large data sets of language

240
00:26:36,799 --> 00:26:45,679
you have more representation of

241
00:26:42,720 --> 00:26:47,440
men you end up with a

242
00:26:45,679 --> 00:26:51,840
situation like this where a stopword

243
00:26:47,440 --> 00:26:54,799
list will have he's but not she's

244
00:26:51,840 --> 00:26:57,840
so many decisions when it comes to

245
00:26:54,799 --> 00:27:01,120
modeling or analysis with language

246
00:26:57,840 --> 00:27:03,919
we as practitioners have to decide

247
00:27:01,120 --> 00:27:07,679
what is appropriate for our particular domain

248
00:27:05,279 --> 00:27:10,720
it turns out this is even true when it

249
00:27:07,679 --> 00:27:12,960
comes to picking a stopword list

250
00:27:10,720 --> 00:27:14,720
so in Tidymodels we can implement a

251
00:27:12,960 --> 00:27:19,200
pre-processing step like removing stopwords

252
00:27:17,120 --> 00:27:21,760
by adding an additional step to our

253
00:27:19,200 --> 00:27:24,720
recipe so first we specified what

254
00:27:21,760 --> 00:27:28,000
variables we would use then we tokenized

255
00:27:24,720 --> 00:27:30,720
the text and now we are removing

256
00:27:28,000 --> 00:27:32,640
stopwords here using just the default

257
00:27:30,720 --> 00:27:35,360
step since we are not passing in any

258
00:27:32,640 --> 00:27:37,600
other arguments we could though

259
00:27:35,360 --> 00:27:39,919
use a non-default step or even a custom

260
00:27:37,600 --> 00:27:42,399
list if that was most appropriate to our domain

261
00:27:42,880 --> 00:27:48,960
this plot compares the model performance

262
00:27:46,799 --> 00:27:52,399
for predicting the year of

263
00:27:50,159 --> 00:27:55,039
that same data set of

264
00:27:52,399 --> 00:27:58,240
supreme court opinions with three

265
00:27:55,039 --> 00:28:02,080
different stopword lexicons of different lengths

266
00:27:59,279 --> 00:28:04,480
so the snowball lexicon contains the

267
00:28:02,080 --> 00:28:06,960
smallest number of words and in this

268
00:28:04,480 --> 00:28:10,320
case it results in the best performance

269
00:28:06,960 --> 00:28:12,799
so removing fewer stopwords results in

270
00:28:10,320 --> 00:28:16,399
the best performance here so this

271
00:28:12,799 --> 00:28:19,440
specific result is not generalizable to

272
00:28:16,399 --> 00:28:22,080
all data sets and contexts but the fact

273
00:28:19,440 --> 00:28:23,919
that removing different sets of

274
00:28:22,080 --> 00:28:26,799
stopwords can have noticeably different

275
00:28:23,919 --> 00:28:29,919
effects on your model that is quite

276
00:28:26,799 --> 00:28:33,200
transferable so the only way to know

277
00:28:29,919 --> 00:28:35,360
what is the best thing to do is to try

278
00:28:33,200 --> 00:28:37,600
several options and see so machine

279
00:28:35,360 --> 00:28:41,360
learning in general.

280
00:28:39,279 --> 00:28:42,480
this is an empirical field right like we

281
00:28:41,360 --> 00:28:47,440
don't know we don't often have reasons a priori to

282
00:28:45,679 --> 00:28:49,919
know what will be the best thing to do

283
00:28:47,440 --> 00:28:52,240
and so typically we have to

284
00:28:49,919 --> 00:28:54,960
try a different option to see what will

285
00:28:52,240 --> 00:29:01,039
be the best thing all right. then the the third

286
00:28:58,799 --> 00:29:04,000
pre-processing step that I want to talk about

287
00:29:02,080 --> 00:29:07,760
for text is stemming

288
00:29:05,279 --> 00:29:10,640
so when we deal with text often

289
00:29:07,760 --> 00:29:14,399
documents contain different versions of

290
00:29:10,640 --> 00:29:18,559
one base word often called a stem so what

291
00:29:15,440 --> 00:29:20,000
if say for an english example

292
00:29:18,559 --> 00:29:22,000
if we aren't interested in the difference

293
00:29:20,000 --> 00:29:26,480
between animals plural and animal

294
00:29:24,480 --> 00:29:31,279
singular and we want to treat them both together

295
00:29:27,919 --> 00:29:33,200
so that idea is at the heart of stemming

296
00:29:31,279 --> 00:29:37,279
so there's no one

297
00:29:33,200 --> 00:29:40,320
right way or correct way to stem text so

298
00:29:37,279 --> 00:29:42,480
this plot shows three approaches for

299
00:29:40,320 --> 00:29:44,880
stemming in English

300
00:29:42,480 --> 00:29:49,039
starting from hey let's just remove a final s

301
00:29:46,240 --> 00:29:51,760
to more complex rules about plural

302
00:29:49,039 --> 00:29:54,480
handling plural endings that middle

303
00:29:51,760 --> 00:29:57,520
one it is called the s stemmer

304
00:29:54,480 --> 00:30:01,360
it's a set of it's like a little set of rules

305
00:29:58,640 --> 00:30:02,640
and that last one is the best known

306
00:30:01,360 --> 00:30:04,960
one probably the best-known

307
00:30:02,640 --> 00:30:07,279
implementation of stemming in English

308
00:30:04,960 --> 00:30:09,919
called the Porter algorithm

309
00:30:07,279 --> 00:30:12,559
so you can see here that Porter stemming

310
00:30:09,919 --> 00:30:14,880
is the most different from the other two

311
00:30:12,559 --> 00:30:16,799
in the top 20 words here from the data

312
00:30:14,880 --> 00:30:19,840
set of animal descriptions that I've

313
00:30:16,799 --> 00:30:21,520
been using we see how the word species

314
00:30:19,840 --> 00:30:25,600
was treated differently animal predator

315
00:30:24,240 --> 00:30:31,440
this sort of collection of words

316
00:30:28,240 --> 00:30:33,200
live living life lives that was treated

317
00:30:31,440 --> 00:30:38,960
differently so practitioners are typically

318
00:30:36,240 --> 00:30:40,559
interested in stemming text data because

319
00:30:38,960 --> 00:30:46,960
it buckets tokens together that we believe

320
00:30:43,679 --> 00:30:51,600
belong together in in a way that

321
00:30:46,960 --> 00:30:53,360
we understand that as human users

322
00:30:51,600 --> 00:30:58,320
of language so we can use approaches like this

323
00:30:56,799 --> 00:31:04,159
which are pretty like step-by-step

324
00:31:01,519 --> 00:31:07,039
rules based this is typically called

325
00:31:04,159 --> 00:31:09,600
stemming or and it's fairly

326
00:31:07,039 --> 00:31:12,080
algorithmic in nature like

327
00:31:09,600 --> 00:31:14,720
first do this then do this then do this

328
00:31:12,080 --> 00:31:17,840
or you can use lemmatization

329
00:31:15,519 --> 00:31:23,519
which is usually based on large dictionaries

330
00:31:19,440 --> 00:31:26,559
of words and it incorporates like a

331
00:31:23,519 --> 00:31:31,120
linguistic understanding of what words belong together

332
00:31:28,559 --> 00:31:34,960
so most of the existing approaches for

333
00:31:31,120 --> 00:31:38,559
this kind of task in Korean are

334
00:31:35,919 --> 00:31:41,279
are limited lemmatizers based on these

335
00:31:38,559 --> 00:31:45,679
dictionaries and that are trained

336
00:31:42,480 --> 00:31:47,600
using large data sets of language

337
00:31:45,679 --> 00:31:50,559
so this seems like it's going to be a helpful thing to do

338
00:31:49,039 --> 00:31:53,360
when you hear about this you're like

339
00:31:50,559 --> 00:31:54,880
oh yeah sounds good,  sounds smart

340
00:31:53,360 --> 00:32:00,480
especially because with text data we are typically

341
00:31:56,919 --> 00:32:04,080
overwhelmed with features with numbers of tokens

342
00:32:02,320 --> 00:32:05,840
this is typically the situation

343
00:32:04,080 --> 00:32:09,440
when we're dealing with text data

344
00:32:05,840 --> 00:32:16,080
so here we have these animal description data

345
00:32:12,080 --> 00:32:18,240
and I made a matrix representation of it

346
00:32:16,080 --> 00:32:20,159
like we would typically use in some

347
00:32:18,240 --> 00:32:24,000
machine learning algorithm

348
00:32:20,159 --> 00:32:28,880
and look how many features there are

349
00:32:25,200 --> 00:32:31,600
16,000 almost 17,000 features

350
00:32:29,679 --> 00:32:33,519
that's the number of features that

351
00:32:31,600 --> 00:32:36,640
would be going into the model

352
00:32:33,519 --> 00:32:39,919
look at the sparsity

353
00:32:36,640 --> 00:32:43,440
98 percent sparse that's high very

354
00:32:39,919 --> 00:32:45,360
sparse data so this is the sparsity of

355
00:32:43,440 --> 00:32:47,760
the data that will go into the machine

356
00:32:45,360 --> 00:32:51,600
learning algorithm to build our

357
00:32:47,760 --> 00:32:53,760
supervised machine learning model

358
00:32:51,600 --> 00:32:56,480
if we stem the words

359
00:32:53,760 --> 00:33:00,159
if I use here an approach for stemming

360
00:32:56,480 --> 00:33:02,240
we reduce the number of word features by

361
00:33:00,159 --> 00:33:06,480
many thousands the sparsity unfortunately did not

362
00:33:04,640 --> 00:33:09,519
change as much but we reduced the number

363
00:33:06,480 --> 00:33:11,600
of features by a lot by bucketing those

364
00:33:09,519 --> 00:33:15,200
words together that our stemming algorithm

365
00:33:12,720 --> 00:33:16,880
belong together so you know

366
00:33:15,200 --> 00:33:19,519
common sense says

367
00:33:16,880 --> 00:33:21,679
reducing the number of words features

368
00:33:19,519 --> 00:33:24,880
so dramatically is going to perform

369
00:33:22,720 --> 00:33:29,039
improve the performance of our machine learning model

370
00:33:26,640 --> 00:33:32,559
but that is that does assume that we

371
00:33:29,039 --> 00:33:35,039
have not lost any important information

372
00:33:32,559 --> 00:33:39,279
by by stemming and it turns out that stemming or

373
00:33:37,120 --> 00:33:44,399
lemmatization can often be very helpful in some

374
00:33:41,440 --> 00:33:48,320
contexts but the typical algorithms used for these

375
00:33:45,519 --> 00:33:51,039
are somewhat aggressive

376
00:33:48,320 --> 00:33:53,840
and they have been built to favor sensitivity

377
00:33:52,399 --> 00:33:59,279
or recall or the true positive rate and

378
00:33:56,880 --> 00:34:01,760
this is at the expense of the

379
00:33:59,279 --> 00:34:04,640
specificity or the precision or the true

380
00:34:01,760 --> 00:34:07,200
negative rate so in a supervised machine

381
00:34:04,640 --> 00:34:09,679
learning context what this does is this

382
00:34:07,200 --> 00:34:13,839
affects a model's positive predictive

383
00:34:11,359 --> 00:34:19,200
value the precision or its ability to

384
00:34:16,399 --> 00:34:20,560
to not incorrectly label true negatives

385
00:34:19,200 --> 00:34:25,760
as positive I hope I got that right

386
00:34:22,800 --> 00:34:28,960
so you know to make this more concrete

387
00:34:25,760 --> 00:34:31,359
stemming can increase a model's ability

388
00:34:28,960 --> 00:34:33,760
to find the positive examples

389
00:34:31,359 --> 00:34:36,320
of say the animal descriptions that are

390
00:34:33,760 --> 00:34:38,399
associated with say a certain diet if

391
00:34:36,320 --> 00:34:40,720
that's what we're modeling however if

392
00:34:38,399 --> 00:34:43,200
text is over stemmed

393
00:34:40,720 --> 00:34:45,520
the resulting model loses its ability to

394
00:34:43,200 --> 00:34:47,200
label the negative examples

395
00:34:45,520 --> 00:34:49,760
say the descriptions that are not about

396
00:34:47,200 --> 00:34:52,240
that diet that's what we're looking for

397
00:34:49,760 --> 00:34:54,960
and this can be a real challenge when

398
00:34:52,240 --> 00:34:57,200
training models with text data kind of

399
00:34:54,960 --> 00:34:59,359
finding that that balance there because

400
00:34:57,200 --> 00:35:01,280
often we don't have a

401
00:34:59,359 --> 00:35:05,119
dial that we can change on these

402
00:35:01,280 --> 00:35:08,400
stemming on these stemming algorithms

403
00:35:05,119 --> 00:35:10,800
so even just very basic pre-processing

404
00:35:08,400 --> 00:35:13,200
for text like what I'm showing here in

405
00:35:10,800 --> 00:35:15,280
this feature engineering recipe can be

406
00:35:13,200 --> 00:35:17,440
computationally expensive

407
00:35:15,280 --> 00:35:20,079
and the choices that a practitioner

408
00:35:17,440 --> 00:35:24,400
makes like whether or not to remove

409
00:35:20,079 --> 00:35:27,599
stopwords or to stem text can have dramatic

410
00:35:24,400 --> 00:35:30,320
impact on how machine learning models

411
00:35:27,599 --> 00:35:32,320
of all kinds perform whether those are

412
00:35:30,320 --> 00:35:34,000
simpler models

413
00:35:32,320 --> 00:35:36,000
more traditional machine learning models

414
00:35:34,000 --> 00:35:42,560
or deep learning models what this means is that

415
00:35:39,040 --> 00:35:45,520
the price the prioritization that we

416
00:35:42,560 --> 00:35:47,920
as practitioners give to like learning

417
00:35:45,520 --> 00:35:50,079
teaching and writing about feature

418
00:35:47,920 --> 00:35:53,040
engineering steps for text really

419
00:35:50,079 --> 00:35:56,320
contributes to better more robust

420
00:35:53,040 --> 00:35:58,560
statistical practice in our field

421
00:35:56,320 --> 00:36:01,359
I mentioned before the sparsity of text

422
00:35:58,560 --> 00:36:04,079
data and I want to come back to that

423
00:36:01,359 --> 00:36:06,720
because it is one of text data's really

424
00:36:04,079 --> 00:36:14,320
defining characteristics because of just how language works

425
00:36:10,400 --> 00:36:16,640
we use a few words a lot of times

426
00:36:14,320 --> 00:36:19,599
and then a lot of words only just a

427
00:36:16,640 --> 00:36:22,640
couple of times only a few a few times

428
00:36:19,599 --> 00:36:25,119
and with a real set of natural language

429
00:36:22,640 --> 00:36:27,520
you end up with relationships that look

430
00:36:25,119 --> 00:36:30,640
like this that look like these plots in

431
00:36:27,520 --> 00:36:32,400
terms of how the sparsity changes as you

432
00:36:30,640 --> 00:36:35,680
add more documents

433
00:36:32,400 --> 00:36:38,320
and more unique words to a corpus

434
00:36:35,680 --> 00:36:43,040
so the sparsity goes up real fast as you

435
00:36:38,320 --> 00:36:45,760
add more unique words and the memory

436
00:36:43,040 --> 00:36:51,920
that is required to handle

437
00:36:48,480 --> 00:36:53,520
this set of documents goes up very fast

438
00:36:51,920 --> 00:36:57,520
so even if you use specialized data

439
00:36:56,320 --> 00:37:03,280
structures meant to store sparse data like sparse

440
00:37:00,079 --> 00:37:05,920
matrices you still end up growing the

441
00:37:03,280 --> 00:37:07,599
memory required to handle these data

442
00:37:05,920 --> 00:37:13,119
sets in a very non-linear way it still grows up very

443
00:37:09,920 --> 00:37:15,520
fast so this means it can take a very

444
00:37:13,119 --> 00:37:19,839
long time to train your model or even that

445
00:37:16,960 --> 00:37:21,760
you outgrow the memory

446
00:37:19,839 --> 00:37:23,839
available on your machine you have to go

447
00:37:21,760 --> 00:37:26,079
to the cloud to an expensive

448
00:37:23,839 --> 00:37:31,200
big memory situation this can be a real challenge

449
00:37:27,680 --> 00:37:34,320
and this challenge

450
00:37:31,200 --> 00:37:39,359
it is what has behind the motivating of vector

451
00:37:37,200 --> 00:37:43,520
languages for models so

452
00:37:40,240 --> 00:37:46,480
linguists have worked for a long time

453
00:37:43,520 --> 00:37:49,680
on vector languages for models that can

454
00:37:46,480 --> 00:37:55,280
reduce the number of dimensions representing text data

455
00:37:51,680 --> 00:37:58,160
based on how people use language

456
00:37:55,280 --> 00:38:04,960
so this quote here goes all the way back to 1957.

457
00:38:02,960 --> 00:38:09,280
so the idea here is that we use

458
00:38:07,119 --> 00:38:12,800
like the data is very sparse

459
00:38:09,280 --> 00:38:15,440
but we don't use words

460
00:38:12,800 --> 00:38:17,359
randomly it's not independent the words

461
00:38:15,440 --> 00:38:19,599
are not used independently of each other

462
00:38:17,359 --> 00:38:22,240
but rather there's relationships that

463
00:38:19,599 --> 00:38:25,920
exist between how words are used together

464
00:38:23,359 --> 00:38:31,040
and we can use those relationships to create

465
00:38:27,599 --> 00:38:33,599
to transform our sparse high dimensional

466
00:38:31,040 --> 00:38:37,359
space into a special dense

467
00:38:34,560 --> 00:38:40,160
low dimensional space lower

468
00:38:37,359 --> 00:38:42,480
we still has like 100

469
00:38:40,160 --> 00:38:46,160
dimensions but much lower than the many thousands

470
00:38:44,000 --> 00:38:48,160
hundreds tens hundreds of thousands

471
00:38:46,160 --> 00:38:49,839
of space so the idea here we use

472
00:38:48,160 --> 00:38:55,599
statistical modeling maybe just

473
00:38:51,839 --> 00:38:58,240
word counts plus matrix factorization

474
00:38:55,599 --> 00:39:00,480
maybe fancier math that involves neural

475
00:38:58,240 --> 00:39:03,680
networks to take this really high

476
00:39:00,480 --> 00:39:05,839
dimensional space and we create a new

477
00:39:03,680 --> 00:39:08,480
lower dimensional lower dimensional

478
00:39:05,839 --> 00:39:12,000
space that is special because the new

479
00:39:08,480 --> 00:39:15,040
space is created based on vectors that

480
00:39:12,000 --> 00:39:18,560
incorporate information

481
00:39:15,040 --> 00:39:21,839
about which words are used together so

482
00:39:18,560 --> 00:39:26,480
you shall know a word by the company it keeps

483
00:39:24,079 --> 00:39:29,359
so you need a big data set of text to

484
00:39:26,480 --> 00:39:32,000
create or learn these kinds of word

485
00:39:29,359 --> 00:39:35,119
vectors or word embeddings

486
00:39:32,000 --> 00:39:37,040
so this table that I'm showing right now

487
00:39:35,119 --> 00:39:40,560
it's from a set of embeddings that

488
00:39:37,040 --> 00:39:48,000
I created using a data set or a corpus of complaints

489
00:39:44,640 --> 00:39:51,680
complaints to the United States consumer

490
00:39:49,200 --> 00:39:53,839
financial protection bureau

491
00:39:51,680 --> 00:39:56,640
so this is a government body in the

492
00:39:53,839 --> 00:40:00,720
United States where people can complain and say

493
00:39:57,920 --> 00:40:03,680
what is wrong with something to do with

494
00:40:00,720 --> 00:40:07,599
a financial product like a credit card

495
00:40:04,720 --> 00:40:09,760
a mortgage a student loan

496
00:40:07,599 --> 00:40:11,119
something to do with like a financial

497
00:40:09,760 --> 00:40:13,119
product they're like something went

498
00:40:11,119 --> 00:40:15,359
wrong with my credit card something went

499
00:40:13,119 --> 00:40:17,520
wrong with my mortgage that company is

500
00:40:15,359 --> 00:40:24,000
not being fair so you come and you complain to it

501
00:40:20,000 --> 00:40:27,839
so I took all those complaints and built

502
00:40:25,440 --> 00:40:30,160
it's our high dimensional space and

503
00:40:27,839 --> 00:40:32,400
build a low dimensional space

504
00:40:30,160 --> 00:40:37,680
and we can look in that space and understand

505
00:40:33,920 --> 00:40:40,240
what words are related to each other

506
00:40:37,680 --> 00:40:44,079
in this space so in the new space

507
00:40:40,240 --> 00:40:48,000
defined by the embeddings the word month

508
00:40:44,079 --> 00:40:50,880
is closest to words like year months plural

509
00:40:49,680 --> 00:40:56,480
monthly installments payment so these are words

510
00:40:54,560 --> 00:41:01,280
that are that makes sense in the context of

511
00:40:57,839 --> 00:41:05,119
financial products like credit cards or mortgages

512
00:41:03,359 --> 00:41:09,680
in the new space defined by these embeddings

513
00:41:06,720 --> 00:41:11,760
the word error is closest to the words

514
00:41:09,680 --> 00:41:17,680
like mistake clerical like a clerical mistake

515
00:41:14,960 --> 00:41:20,880
problem glitch or there was a glitch on my

516
00:41:19,040 --> 00:41:26,240
mortgage statement so we see these kinds of

517
00:41:23,440 --> 00:41:28,319
or miscommunication misunderstanding you

518
00:41:26,240 --> 00:41:30,880
know like these are these are words that

519
00:41:28,319 --> 00:41:33,599
are used in similar ways so

520
00:41:31,599 --> 00:41:37,200
you don't have to create embeddings yourself

521
00:41:34,800 --> 00:41:39,520
because it requires quite a lot of data

522
00:41:37,200 --> 00:41:42,240
to make them so you can use word

523
00:41:39,520 --> 00:41:44,880
embeddings that are pre-trained

524
00:41:42,240 --> 00:41:47,520
i.e created by someone else

525
00:41:44,880 --> 00:41:49,839
based on some huge corpus of data that

526
00:41:47,520 --> 00:41:54,079
they have access to and you probably don't

527
00:41:51,359 --> 00:41:55,599
so let's look at one of those data sets

528
00:41:54,079 --> 00:42:02,800
let's look at this table shows the results for the same word error

529
00:42:00,160 --> 00:42:04,560
but for the glove embeddings so the

530
00:42:02,800 --> 00:42:07,359
glove embeddings are a set of

531
00:42:04,560 --> 00:42:09,920
pre-trained embeddings that are created

532
00:42:07,359 --> 00:42:12,800
based on a very large data set that's like

533
00:42:11,040 --> 00:42:16,640
all of wikipedia all of the google news data set

534
00:42:15,280 --> 00:42:24,800
just like huge swaths of the internet have been

535
00:42:21,440 --> 00:42:27,280
fed in to create these embeddings

536
00:42:24,800 --> 00:42:31,680
so some of the closest words here are similar

537
00:42:29,520 --> 00:42:34,800
to those that are before but we no

538
00:42:31,680 --> 00:42:38,640
longer have some of that domain specific

539
00:42:34,800 --> 00:42:40,839
flavor like clerical discrepancy

540
00:42:38,640 --> 00:42:43,280
and now we have like

541
00:42:40,839 --> 00:42:46,720
miscommunication you know but and now we

542
00:42:43,280 --> 00:42:48,960
have calculation and probability

543
00:42:46,720 --> 00:42:51,599
which people were not talking about with

544
00:42:48,960 --> 00:42:54,319
their financial product complaints

545
00:42:51,599 --> 00:42:58,240
so this really highlights

546
00:42:54,319 --> 00:43:00,800
how these how these work here before

547
00:42:58,240 --> 00:43:02,640
we we created our own and we were able

548
00:43:00,800 --> 00:43:06,800
to learn relationships that were

549
00:43:02,640 --> 00:43:10,400
specific to this context and here we go

550
00:43:06,800 --> 00:43:13,119
to a more general set that that

551
00:43:10,400 --> 00:43:16,720
was learned somewhere else

552
00:43:13,119 --> 00:43:19,599
so embeddings are trained or learned

553
00:43:16,720 --> 00:43:22,079
from a large corpus of text data and the

554
00:43:19,599 --> 00:43:24,240
characteristics of that corpus become

555
00:43:22,079 --> 00:43:27,359
part of the embeddings

556
00:43:24,240 --> 00:43:30,160
so machine learning in general you know

557
00:43:27,359 --> 00:43:32,240
is exquisitely sensitive to whatever it

558
00:43:30,160 --> 00:43:35,839
is that's in your training data and this

559
00:43:32,240 --> 00:43:38,160
is never more obvious than when

560
00:43:35,839 --> 00:43:40,319
dealing with text data

561
00:43:38,160 --> 00:43:42,400
and perhaps with word embeddings is

562
00:43:40,319 --> 00:43:43,760
just like one of these classic examples

563
00:43:42,400 --> 00:43:48,160
where this is true it turns out that

564
00:43:46,079 --> 00:43:54,400
this shows up in how any human

565
00:43:51,040 --> 00:43:56,480
prejudice or bias in the corpus

566
00:43:54,400 --> 00:44:01,440
becomes imprinted into the embeddings

567
00:43:59,040 --> 00:44:04,720
so in fact when we look at some of these

568
00:44:01,440 --> 00:44:07,040
most commonly available embeddings that

569
00:44:04,720 --> 00:44:14,640
are out there bias is we we see that

570
00:44:12,200 --> 00:44:17,520
african-american first names that are

571
00:44:14,640 --> 00:44:19,680
more common for african americans in the

572
00:44:17,520 --> 00:44:23,280
United States they're associated with

573
00:44:19,680 --> 00:44:25,520
more unpleasant feelings than European

574
00:44:23,280 --> 00:44:30,240
American first names in these embedding spaces

575
00:44:27,680 --> 00:44:31,599
women's first names are more associated

576
00:44:30,240 --> 00:44:38,640
with family and men's first names are more associated with career

577
00:44:36,079 --> 00:44:40,560
and terms associated with women are more

578
00:44:38,640 --> 00:44:42,640
associated with the arts and terms

579
00:44:40,560 --> 00:44:44,079
associated with men are more associated

580
00:44:42,640 --> 00:44:51,440
with science so it turns out actually bias is so

581
00:44:48,319 --> 00:44:54,160
ingrained in word embeddings that the

582
00:44:51,440 --> 00:44:56,800
word embeddings themselves can be used

583
00:44:54,160 --> 00:45:01,440
to quantify change

584
00:44:58,480 --> 00:45:05,040
in social attitudes over time

585
00:45:01,440 --> 00:45:08,160
so word embeddings are

586
00:45:05,040 --> 00:45:11,119
maybe an exaggerated or extreme example

587
00:45:08,160 --> 00:45:13,599
but it turns out that all the feature

588
00:45:11,119 --> 00:45:16,720
engineering decisions that we make when

589
00:45:13,599 --> 00:45:18,480
it comes to text data have a significant

590
00:45:16,720 --> 00:45:21,440
effect on our results

591
00:45:18,480 --> 00:45:26,000
both in terms of the model performance that we see

592
00:45:22,560 --> 00:45:29,520
and also in terms of how appropriate or

593
00:45:26,000 --> 00:45:31,599
fair our models are

594
00:45:29,520 --> 00:45:34,560
so given all that when it comes to

595
00:45:31,599 --> 00:45:37,040
pre-processing your text data

596
00:45:34,560 --> 00:45:39,680
creating these features that you need

597
00:45:37,040 --> 00:45:43,520
you have a lot of options and quite a

598
00:45:39,680 --> 00:45:46,560
bit of responsibility so my advice is

599
00:45:43,520 --> 00:45:50,480
always start with simpler models that

600
00:45:46,560 --> 00:45:50,480
you can understand quite deeply

601
00:45:50,640 --> 00:45:56,720
be sure to adopt good statistical

602
00:45:53,520 --> 00:45:59,040
practices as you train and tune your

603
00:45:56,720 --> 00:46:04,720
models so you aren't fooled about model

604
00:46:01,839 --> 00:46:07,119
performance improvements

605
00:46:04,720 --> 00:46:10,400
when you try different approaches

606
00:46:07,119 --> 00:46:12,839
and also to use model explainability

607
00:46:10,400 --> 00:46:16,079
tools and frameworks so you can

608
00:46:12,839 --> 00:46:18,000
understand any less straightforward

609
00:46:16,079 --> 00:46:20,240
models that you try

610
00:46:18,000 --> 00:46:22,640
so my co-workers and I have written

611
00:46:20,240 --> 00:46:24,720
about all of these topics and how to

612
00:46:22,640 --> 00:46:26,880
use them with Tidymodels if that's what

613
00:46:24,720 --> 00:46:30,319
you like to use and we will continue to do so

614
00:46:28,720 --> 00:46:33,040
with that i will say

615
00:46:30,319 --> 00:46:34,880
thank you so very much and I want to be

616
00:46:33,040 --> 00:46:38,960
sure to again

617
00:46:36,560 --> 00:46:42,560
thank the organizers of the R user group

618
00:46:38,960 --> 00:46:45,119
in Korea I want to thank my teammates on

619
00:46:42,560 --> 00:46:47,440
the Tidymodels team at Rstudio as

620
00:46:45,119 --> 00:46:51,240
well as my co-author EMIL HVITFELDT.

