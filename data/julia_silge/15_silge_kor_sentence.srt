1
00:00:01,120 --> 00:00:08,639
안녕하세요 제 이름은 줄리아 실지입니다. 저는 RStudio의 데이터 과학자이자 소프트웨어 엔지니어입니다.

2
00:00:06,080 --> 00:00:18,080
한국의 R User Group 주최측에 감사드립니다. 그리고 저를 가져주셔서 정말 감사합니다.

3
00:00:15,519 --> 00:00:29,359
저는 오늘 몇 가지 이유로 텍스트 데이터에서 기계 학습을 위한 기능을 만드는 것에 대해 구체적으로 이야기하게 되어 매우 기쁩니다.

4
00:00:27,199 --> 00:00:50,719
텍스트 데이터를 가져와 기계 학습 알고리즘에 대한 입력으로 적절하게 만들기 위해 우리가 하는 일을 더 잘 이해하면 모델을 직접 훈련할 준비를 하거나 일부 텍스트의 시작 부분에 있는 경우 모두 많은 이점이 있습니다. 분석 프로젝트

5
00:00:48,399 --> 00:01:06,640
또는 어떤 방식으로든 상호 작용하는 모델의 동작을 이해하려고 하는 경우 데이터 과학자로서의 작업이나 일상 생활에서 점점 더 많이 하는 것입니다.

6
00:01:03,359 --> 00:01:15,759
감독되거나 감독되지 않는 텍스트에 대한 모델을 만들 때 우리는 이와 같은 것으로 시작합니다.

7
00:01:11,600 --> 00:01:24,560
이것은 일부 동물을 설명하는 이 강연에서 몇 번 사용할 텍스트 데이터의 예입니다.

8
00:01:21,040 --> 00:01:34,400
영어 사용자가 친숙하게 보일 수 있도록 일부 텍스트 데이터를 사용하고 있습니다.

9
00:01:30,960 --> 00:01:43,920
나는 인간의 언어를 사용하는 사람이기 때문에 이것을보고 읽을 수 있고 큰 소리로 말할 수 있고 이해할 수 있습니다.

10
00:01:41,280 --> 00:01:45,759
나는 그것이 무엇을 의미하는지 해석할 수 있다

11
00:01:43,920 --> 00:01:59,840
이러한 종류의 자연어 데이터는 모든 종류의 컨텍스트에서 모든 종류의 언어로 항상 생성되고 있습니다.

12
00:01:56,880 --> 00:02:26,720
금융 기술 분야의 의료 분야에서 일하든 기본적으로 어떤 조직이든 이러한 종류의 텍스트 데이터는 비즈니스 프로세스를 통해 소셜 미디어를 통해 설문 조사에 참여하는 사람들이 비즈니스 내부 이해 관계자에 의해 고객에 의해 생성됩니다.

13
00:02:24,319 --> 00:02:35,360
이 모든 자연어에는 더 나은 결정을 내리는 데 사용할 수 있는 해당 텍스트 데이터에 숨겨진 정보가 있습니다.

14
00:02:33,519 --> 00:02:47,440
그러나 컴퓨터는 이것을 보고 이렇게 표현되는 언어로 수학을 잘 하지 못합니다.

15
00:02:45,440 --> 00:02:55,440
대신 언어는 일종의 기계 판독 가능한 숫자 표현으로 극적으로 변환되어야 합니다.

16
00:02:53,680 --> 00:03:01,840
거의 모든 종류의 모델에 사용할 수 있도록 여기 화면에서 보여주고 있는 것과 더 비슷합니다.

17
00:02:59,760 --> 00:03:22,159
나는 행당 하나의 관찰이 있는 깔끔한 형식의 텍스트 데이터로 탐색적 데이터 분석, 시각화, 요약 작업을 수행할 수 있는 소프트웨어 작업에 상당한 시간을 할애했습니다.

18
00:03:19,040 --> 00:03:34,879
저는 특히 모델을 구축할 때 분석의 탐색 단계에서 텍스트 분석을 위해 깔끔한 데이터 원칙을 사용하는 것을 좋아합니다.

19
00:03:32,640 --> 00:03:52,319
종종 기본 수학적 구현이 실제로 필요로 하는 것은 일반적으로 이와 같은 것인데, 이 특정 표현을 문서 용어 행렬이라고 합니다.

20
00:03:49,760 --> 00:03:55,760
정확한 표현은 내가 여기에 표시한 것과 다를 수 있습니다.

21
00:03:53,840 --> 00:04:04,720
여기서 제가 가지고 있는 것은 이 행렬의 각 행이 문서가 되도록 카운트로 가중치를 부여한다는 것입니다.

22
00:04:02,640 --> 00:04:07,439
각 열은 단어입니다.

23
00:04:04,720 --> 00:04:19,359
토큰과 숫자는 각 문서가 몇 번이나 문서를 작성하는지, 다른 방식으로 가중치를 부여할 수 있는 각 단어를 사용하거나, 카운트 대신 TF-IDF를 사용하는 것을 나타냅니다.

24
00:04:16,720 --> 00:04:41,120
딥 러닝 모델을 구축하는 데 관심이 있지만 기본적으로 모든 종류의 텍스트 모델링에 대해 시퀀스 정보를 유지할 수 있습니다. 텍스트 데이터를 위한 변환기처럼 오늘날 일어나고 있는 일종의 작업입니다.

25
00:04:39,520 --> 00:04:53,680
우리는 어떤 종류의 표현을 얻기 위해 엔지니어링 및 프로세스 언어를 많이 기능화해야 합니다. 머신 러닝 알고리즘에 적합합니다.

26
00:04:50,960 --> 00:05:04,800
저는 Tidymodels라고 하는 모델링 및 기계 학습을 위해 R에서 오픈 소스 프레임워크를 작업하고 있으며 오늘 보여드릴 예제에서는 Tidymodels 코드를 사용합니다.

27
00:05:02,320 --> 00:05:21,360
Tidymodels 프로젝트의 특정 목표 중 일부는 실제 데이터를 다루는 실제 모델링 실무자에게 일관되고 유연한 프레임워크를 제공하는 것입니다.

28
00:05:19,199 --> 00:05:37,360
모델링에 매우 경험이 있는 사람들로 시작하는 사람들과 목표는 R 내에 존재하는 이질적인 인터페이스를 조화시키고 좋은 통계 관행을 장려하는 것입니다.

29
00:05:35,360 --> 00:05:52,800
제가 작업하고 구축한 것과 이를 텍스트 모델링에 적용하는 방법을 보여드리게 되어 기쁩니다. 하지만 오늘 이야기할 많은 내용은 Tidymodels나 R에만 국한되지 않습니다.

30
00:05:50,880 --> 00:06:00,479
나는 이것이 R 사용자 그룹이라는 것을 알고 있지만 우리가 이야기하고 집중할 것은 조금 더 개념적이고 기본적인 것입니다.

31
00:05:57,440 --> 00:06:06,800
기계 학습을 위해 텍스트를 예측자로 어떻게 변환합니까?

32
00:06:04,560 --> 00:06:19,720
Tidyverse가 메타 패키지인 것과 유사한 방식으로 이전에 사용한 적이 없다면 Tidymodels와 Tidymodels에 대해 이야기하게 되어 기쁩니다.

33
00:06:16,319 --> 00:06:33,039
라이브러리 Tidyverse를 입력한 다음 시각화에 ggplot2를 사용하고 데이터 조작에 dplyr을 사용했다면 Tidymodels도 비슷한 방식으로 작동합니다.

34
00:06:30,560 --> 00:06:39,840
내부에는 다양한 용도로 사용되는 다양한 패키지가 있습니다.

35
00:06:37,280 --> 00:06:48,000
전처리 또는 피쳐 엔지니어링은 보다 광범위한 모델 프로세스의 일부입니다.

36
00:06:45,280 --> 00:06:56,240
그 프로세스는 실제로 어떤 종류의 모델을 결정하는 데 도움이 되는 탐색적 데이터 분석으로 시작됩니다.

37
00:06:54,080 --> 00:07:03,120
우리는 구축하고 완료됩니다.

38
00:06:59,280 --> 00:07:09,280
귀하의 모델이 얼마나 잘 수행되었는지 측정할 때 모델 평가와 논쟁을 벌일 것입니다.

39
00:07:06,479 --> 00:07:38,560
소프트웨어의 한 부분인 Tidymodels는 부트스트랩 리샘플을 생성할 수 있도록 데이터를 리샘플링하는 것과 같이 각각의 특정 초점이 있는 패키지로 구성됩니다. 튜닝 패키지가 하이퍼 매개변수 튜닝을 위한 모델을 훈련하고 평가합니다.

40
00:07:34,800 --> 00:07:52,720
이름에서 짐작할 수 있듯이 이 패키지 중 하나는 데이터 전처리 기능 엔지니어링을 위한 기능 엔지니어링이며 레시피라고 하는 패키지입니다.

41
00:07:49,360 --> 00:08:25,759
Tidymodels에서 우리는 단계가 있는 사전 처리 레시피의 개념에서 데이터 사전 처리 및 기능 엔지니어링에 대한 아이디어를 캡처하므로 사용할 재료 또는 변수를 선택한 다음 레시피에 들어갈 단계를 정의합니다. , 그런 다음 훈련 데이터를 사용하여 준비한 다음 예측 시간에 테스트 데이터 또는 새 데이터와 같은 모든 데이터 세트에 적용할 수 있습니다.

42
00:08:23,039 --> 00:08:34,560
모델링에 사용하는 변수나 성분은 텍스트 데이터를 포함하여 모든 종류의 모양과 크기로 제공됩니다.

43
00:08:32,000 --> 00:09:05,279
텍스트 데이터를 사전 처리하는 데 사용하는 기술 및 접근 방식 중 일부는 텍스트가 아닌 데이터, 숫자 데이터, 범주형 데이터와 같이 사용할 수 있는 다른 종류의 데이터와 동일합니다. 일부는 동일하지만 텍스트에 대한 이 프로세스에서 좋은 작업을 수행하기 위해 알아야 할 일부는 다르며 언어 데이터의 특성에 따라 다릅니다.

44
00:09:03,680 --> 00:09:14,720
저는 공동 저자인 Emile Hvitfeldt와 함께 "텍스트 분석 및 R을 위한 감독된 기계 학습"에 관한 책을 저술했습니다.

45
00:09:11,360 --> 00:09:25,920
이 책의 첫 번째 3분의 1은 텍스트 데이터에 있는 자연어를 모델링을 위한 기능으로 변환하는 방법에 중점을 둡니다.

46
00:09:23,600 --> 00:09:40,240
중간 섹션에서는 정규화 회귀 또는 지원 벡터 머신과 같은 더 단순하거나 더 전통적인 머신 러닝 모델에서 이러한 기능을 사용하는 방법에 대해 설명합니다.

47
00:09:37,360 --> 00:10:15,279
그런 다음 이 책의 마지막 3분의 1은 텍스트 데이터와 함께 딥 러닝 모델을 사용하는 방법에 대해 이야기합니다. 더 전통적이거나 단순한 기계 학습 모델이 아닌 방식으로 텍스트의 기능 구조.

48
00:10:12,800 --> 00:10:20,560
이 책은 현재 완성되어 11월 현재 이달에 사용할 수 있습니다.

49
00:10:18,480 --> 00:10:30,720
사람들은 첫 번째 종이 사본을 받고 있으며 이 책의 전체 내용은 smalltar.com에서 볼 수 있습니다.

50
00:10:28,560 --> 00:10:43,839
텍스트 데이터를 처음 다루는 경우 텍스트에 대한 이러한 기본적인 사전 처리 접근 방식을 이해하면 효과적인 모델을 훈련할 수 있습니다.

51
00:10:41,760 --> 00:10:58,000
텍스트 데이터에 대해 실제로 경험이 있고 많이 다루었다면 이미 기존 리소스나 문헌을 보았을 것입니다.

52
00:10:54,480 --> 00:11:17,519
그것이 책이든 튜토리얼이든 블로그 게시물이든 이러한 사전 처리 단계가 작동하는 방식과 이러한 기능 엔지니어링 단계에서 선택한 사항이 모델 출력에 미치는 영향에 대한 자세한 사려 깊은 탐구와 관련하여 매우 희소합니다.

53
00:11:15,600 --> 00:11:28,800
기본적인 기능 엔지니어링 접근 방식과 작동 방식 및 수행 작업과 같은 몇 가지를 살펴보겠습니다.

54
00:11:27,040 --> 00:11:31,920
토큰화부터 시작하겠습니다.

55
00:11:28,800 --> 00:11:52,320
일반적으로 탐색 데이터 분석 또는 모델 구축을 포함한 모든 종류의 텍스트 분석을 위해 자연어에서 기계 학습 기능으로 정보를 전송하는 첫 번째 단계 중 하나입니다.

56
00:11:49,040 --> 00:11:55,360
무엇이든 토큰화입니다.

57
00:11:52,320 --> 00:12:02,079
토큰화에서는 문자열, 문자 벡터 및 토큰 유형을 입력으로 받아 의미 있는 텍스트 단위를 입력합니다.

58
00:11:59,440 --> 00:12:12,480
우리는 단어에 관심이 있고 입력 조각을 관심 있는 유형에 해당하는 토큰으로 분할합니다.

59
00:12:09,519 --> 00:12:19,040
텍스트를 단위로 분할하려는 가장 일반적으로 의미 있는 단위 또는 토큰 유형은 단어입니다.

60
00:12:17,839 --> 00:12:30,880
이것은 간단하거나 명백해 보일 수 있지만 많은 또는 대부분의 언어에서 단어가 무엇인지 명확하게 정의하기는 어렵습니다.

61
00:12:29,519 --> 00:12:40,959
많은 언어는 토큰화에 대한 도전을 제시하는 단어 사이에 공백을 전혀 사용하지 않습니다.

62
00:12:38,240 --> 00:13:04,079
영어나 한국어처럼 공백을 사용하는 언어라도 영어의 축약형처럼 모호하거나 한국어에서 입자가 사용되는 방식으로 두 단어로 더 정확하게 간주되는 특정 예가 있는 경우가 많습니다.

63
00:13:01,519 --> 00:13:13,839
대명사와 부정 단어가 서로 붙어 있고 실제로는 두 단어로 간주되어야 하는 이탈리아어 및 프랑스어와 같은 로맨스 언어로 작성되는 방법.

64
00:13:12,399 --> 00:13:32,720
수행할 작업을 파악하고 몇 가지 선택을 하고 텍스트를 토큰화하면 탐색적 데이터 분석이나 비지도 알고리즘 또는 예측 모델링을 위한 기능으로 사용할 수 있게 됩니다. 우리는 여기에 대해 이야기하고 있습니다.

65
00:13:30,320 --> 00:13:45,199
이 결과가 여기에 표시되는 내용은 영국의 Tate 컬렉션에 있는 예술 작품의 미디어 설명에 대해 훈련된 회귀 모델에서 나온 것입니다.

66
00:13:43,279 --> 00:14:02,079
우리가 예측하고 있는 것은 작품이 만들어진 매체와 매체가 약간의 텍스트로 설명되어 있는 작품이 몇 년도에 만들어졌는지입니다.

67
00:14:00,079 --> 00:14:29,920
흑연 수채화와 조각을 사용하여 만든 작품은 그보다 더 일찍 만들어졌을 가능성이 더 큽니다. 그것은 사진 스크린 포인트 또는 유감 스크린 인쇄 스크린 인쇄를 사용하여 만든 오래된 예술 및 예술 작품에서 나올 가능성이 더 높고 똥과 반짝이는 나중에 생성 될 가능성이 더 큽니다.

68
00:14:27,760 --> 00:14:37,519
이것은 현대 미술 현대 미술에서 나올 가능성이 더 큽니다.

69
00:14:33,440 --> 00:15:17,279
우리가 이 텍스트를 토큰화하는 방법 우리는 이 예술 작품이 생성된 미디어에 대한 설명을 작성하는 사람들의 자연적인 인간 생성 텍스트로 시작했으며 우리가 시작한 자연 인간 생성 텍스트를 토큰화하는 방법은 다음과 같습니다. 우리가 그로부터 배운 것에 큰 영향을 미칩니다. 우리가 다른 방식으로 토큰화했다면 연도를 얼마나 정확하게 예측할 수 있었는지와 같은 성능 측면에서 그리고 우리가 배울 수 있는 것과 같이 모델을 해석하는 방식 측면에서 다른 결과를 얻었을 것입니다. 그것.

70
00:15:15,279 --> 00:15:31,279
이것은 단일 단어에 대한 토큰화의 한 종류이지만 단일 단어 또는 유니그램으로 분해하는 대신 n-그램으로 토큰화할 수 있는 토큰화하는 다른 모든 방법입니다.

71
00:15:27,440 --> 00:15:37,920
n-gram은 주어진 텍스트 시퀀스에서 N개 항목의 연속 시퀀스입니다.

72
00:15:35,199 --> 00:15:54,959
이것은 내가 이 동물을 설명하고 있는 동일한 텍스트 조각이 두 개의 토큰으로 구성된 바이그램 또는 n그램으로 나누어져 있음을 보여줍니다. 따라서 바이그램의 단어가 어떻게 겹치는지 주목하세요.

73
00:15:51,519 --> 00:16:11,120
'collard'라는 단어는 'collared', 'collared peccary'라고도 하는 두 첫 번째 bigrams에 모두 나타나므로 n-gram 토큰화가 텍스트를 따라 슬라이드하여 겹치는 토큰 세트를 만듭니다.

74
00:16:07,040 --> 00:16:14,480
이것은 같은 것에 대한 트라이그램을 보여줍니다.

75
00:16:11,120 --> 00:16:23,360
유니그램을 사용하면 한 단어가 더 빠르고 효율적이지만 단어 순서에 대한 정보는 캡처하지 않습니다.

76
00:16:20,560 --> 00:16:41,440
나는 더 높은 값을 사용하고 2, 3 또는 그 이상은 여러 단어로 된 구로 설명되는 단어 순서와 개념에 대한 더 복잡한 정보를 유지합니다.

77
00:16:37,680 --> 00:16:56,480
그러나 토큰의 벡터 공간은 토큰 수의 감소에 해당하는 극적으로 증가합니다. 우리는 각 토큰을 여러 번 계산하지 않으며 이는 특정 데이터 세트에 따라 다르다는 것을 의미합니다.

78
00:16:55,120 --> 00:17:00,399
좋은 결과를 얻지 못할 수도 있습니다.

79
00:16:58,160 --> 00:17:14,079
다양한 수준의 n-gram을 결합하면 텍스트에서 다양한 수준의 세부 정보를 추출할 수 있으므로 유니그램은 어떤 개별 단어가 많이 사용되었는지 알려줄 수 있습니다.

80
00:17:11,439 --> 00:17:23,360
이러한 단어 중 일부는 다른 단어와 자주 함께 나타나지 않는 경우 bi-gram 또는 tri-gram 'crowns'에서 간과될 수 있습니다.

81
00:17:23,520 --> 00:17:39,679
이 그림은 미국 대법원 의견의 연도를 예측하는 올가미 회귀 모델의 모델 성능을 3가지 다른 등급의 n-gram과 비교합니다.

82
00:17:37,840 --> 00:17:51,840
우리가 여기서 하고 있는 것은 미국 대법원의 문서를 가져오는 것입니다. 그리고 우리는 그 문서가 언제 작성되었는지 예측하고 있습니다.

83
00:17:49,760 --> 00:17:58,799
텍스트의 내용에서 텍스트 조각이 얼마나 오래된 것인지 예측할 수 있습니까?

84
00:17:55,360 --> 00:18:13,600
유니그램만 사용하여 토큰 수를 천 단위로 일정하게 유지하는 것이 미국 대법원의 이 의견 모음에 가장 적합합니다.

85
00:18:11,280 --> 00:18:25,760
데이터 세트 자체를 사용하는 모델의 종류에 따라 항상 그런 것은 아닙니다. 유니그램과 바이그램 또는 다른 옵션을 결합한 최상의 성능을 볼 수 있습니다.

86
00:18:23,679 --> 00:18:43,039
이 경우 바이그램과 트라이그램에 있는 좀 더 복잡한 정보를 통합하려면 모델의 토큰 수를 상당히 늘려야 할 것입니다.

87
00:18:40,160 --> 00:18:51,039
이와 같은 결과를 볼 때 n-그램을 식별하는 것은 계산 비용이 많이 든다는 점을 명심하십시오.

88
00:18:48,960 --> 00:19:00,640
이것은 특히 우리가 흔히 볼 수 있는 모델의 성능 향상 정도와 유사합니다.

89
00:18:58,720 --> 00:19:15,840
빅그램을 추가하여 약간의 개선이 보인다면 바이그램을 식별한 다음 해당 모델을 훈련하는 데 걸리는 시간에 비해 얼마나 개선되었는지를 염두에 두는 것이 중요합니다.

90
00:19:13,200 --> 00:19:27,120
예를 들어, 우리가 토큰의 수를 일정하게 유지한 이 대법원 의견 데이터 세트의 경우 모델 훈련에는 동일한 수의 토큰이 포함되었습니다.

91
00:19:25,919 --> 00:19:46,799
바이그램과 유니그램을 함께 사용하면 기능 엔지니어링과 훈련을 수행하는 데 유니그램만 사용하는 것보다 훈련 시간이 2배, 트라이그램을 추가하는 것은 유니그램만 훈련하는 것보다 거의 5배의 시간이 걸립니다.

92
00:19:44,320 --> 00:19:56,559
이것은 다른 방향으로 가는 데 계산적으로 많은 비용이 드는 일입니다.

93
00:19:53,760 --> 00:20:05,440
우리는 단어보다 작은 단위로 토큰화할 수 있으므로 "문자 대상 포진"이라고 합니다.

94
00:20:03,280 --> 00:20:13,120
우리는 칼라 페커리라는 단어를 사용하고 단어를 보는 대신 하위 단어 정보를 볼 수 있습니다.

95
00:20:10,880 --> 00:20:20,880
단어를 기계 학습에 적합한 하위 단어로 나누는 방법에는 여러 가지가 있습니다.

96
00:20:18,480 --> 00:20:31,360
종종 이러한 종류의 접근 방식이나 알고리즘은 예측 시 알 수 없거나 새로운 단어를 인코딩할 수 있다는 이점이 있습니다.

97
00:20:29,039 --> 00:20:41,200
새로운 데이터에 대한 예측을 해야 할 때, 그 당시에 새로운 어휘가 있는 것은 드문 일이 아닙니다.

98
00:20:39,200 --> 00:20:49,360
훈련 데이터에서 그것들을 보지 못했다면 우리는 그 새로운 단어에 대해 무엇을 할 것입니까?

99
00:20:45,600 --> 00:20:58,400
하위 단어 정보를 사용하여 훈련할 때 훈련 데이터 세트에서 하위 단어를 본다면 종종 새로운 단어를 처리할 수 있습니다.

100
00:20:55,760 --> 00:21:13,360
이러한 종류의 하위 단어 정보를 사용하는 것은 형태학적 시퀀스를 다양한 종류의 모델에 통합하는 방법입니다. 이는 영어뿐만 아니라 다양한 언어에 적용되는 것입니다.

101
00:21:11,919 --> 00:21:22,960
이 결과는 매우 짧은 텍스트의 데이터 세트가 있는 분류 모델에 대한 것입니다.

102
00:21:20,000 --> 00:21:27,120
그것은 단지 미국에 있는 우체국의 이름일 뿐입니다. 매우 짧습니다.

103
00:21:24,240 --> 00:21:41,360
모델의 목표는 태평양 한가운데의 하와이 또는 미국의 나머지 지역에 위치한 우체국을 예측하는 것이었습니다.

104
00:21:38,159 --> 00:21:49,200
이 우체국 이름의 하위 단어인 모델에 대한 기능을 만들었습니다.

105
00:21:46,480 --> 00:22:00,240
우리는 h와 p로 시작하거나 ale 하위 단어를 포함하는 이름이 하와이에 있을 가능성이 더 높다는 것을 알게 됩니다.

106
00:21:57,440 --> 00:22:11,440
하위 단어 a 및 d 및 ri 및 ing은 하와이 외부의 우체국에서 올 가능성이 더 큽니다.

107
00:22:09,360 --> 00:22:19,600
이것은 우리가 어떻게 다르게 토큰화하고 새로운 것을 배울 수 있는지에 대한 예입니다. 우리는 다른 것을 배울 수 있습니다.

108
00:22:18,559 --> 00:22:29,919
Tidymodels에서 우리는 토큰화 및 이와 같은 코드에 대한 모든 종류의 결정을 수집합니다.

109
00:22:26,880 --> 00:22:38,640
사용할 변수나 재료를 지정하는 레시피로 시작한 다음 이러한 전처리 단계를 정의합니다.

110
00:22:35,679 --> 00:22:50,400
이 첫 번째이자 틀림없이 간단하고 기본적인 단계에서도 우리가 내리는 선택은 모델링 결과에 큰 영향을 미칩니다.

111
00:22:47,120 --> 00:22:56,240
제가 이야기하고 싶은 다음 전처리 단계는 stopwords입니다.

112
00:22:53,600 --> 00:23:11,840
텍스트를 토큰으로 분할하면 모든 단어가 기계 학습 작업에 대해 동일한 양의 정보를 전달하지 않는다는 것을 종종 발견합니다.

113
00:23:09,520 --> 00:23:18,880
의미 있는 정보가 거의 또는 전혀 전달되지 않는 일반적인 단어를 "중단어"라고 합니다.

114
00:23:16,080 --> 00:23:22,720
이것은 한국어로 사용할 수 있는 불용어 목록 중 하나입니다.

115
00:23:20,400 --> 00:23:36,640
많은 자연어 처리 작업에 대해 이러한 불용어를 제거하는 것이 일반적입니다.

116
00:23:35,039 --> 00:23:44,320
제가 여기서 보여드리는 것은 정말 광범위하게 사용되는 짧은 영어 불용어 목록 중 하나의 전체입니다.

117
00:23:41,919 --> 00:24:00,400
"나", "나", "나의" 대명사, 접속사 "그리고", "~의"," 및 "이것들"과 같은 단어는 그다지 중요하지 않은 매우 일반적인 단어입니다.

118
00:23:57,679 --> 00:24:15,840
불용어를 제거하기로 한 결정은 종종 거기에 있는 많은 리소스에 반영된 것보다 더 복잡하고 어려울 수 있습니다.

119
00:24:12,000 --> 00:24:22,400
거의 항상 실제 NLP 실무자는 미리 만들어진 불용어 목록을 사용합니다.

120
00:24:20,000 --> 00:24:32,960
이 플롯은 "업셋 플롯"이라고 하는 영어로 된 세 가지 일반적인 불용어 목록에 대한 교차 집합을 시각화합니다.

121
00:24:31,200 --> 00:24:40,159
세 가지 목록을 "snowball", "smart" 및 "iso" 목록이라고 합니다.

122
00:24:37,440 --> 00:24:51,760
목록의 길이가 막대의 길이로 표시되는 것을 볼 수 있으며 이러한 목록에서 공통되는 단어가 수직 막대로 교차하는 것을 볼 수 있습니다.

123
00:24:48,799 --> 00:25:00,720
목록의 길이는 상당히 다르며 모두 동일한 단어 집합을 포함하지 않는다는 점에 유의하십시오.

124
00:24:58,320 --> 00:25:20,480
불용어 사전에 대해 기억해야 할 중요한 점은 중립적인 완벽한 설정에서 생성되지 않고 대신 문맥에 따라 다르다는 것입니다.

125
00:25:16,960 --> 00:25:28,559
그들은 편향될 수 있습니다. 이 두 가지는 모두 큰 언어 데이터 세트에서 생성된 목록이기 때문에 사실입니다.

126
00:25:27,039 --> 00:25:37,440
생성에 사용된 데이터의 특성을 반영합니다.

127
00:25:34,000 --> 00:25:45,600
이것은 영어 스마트 사전에는 있지만 영어 눈덩이 사전에는 없는 10개의 단어입니다.

128
00:25:43,039 --> 00:25:52,240
그것들은 모두 수축이지만 눈덩이 교환에 수축이 포함되어 있지 않기 때문이 아닙니다.

129
00:25:50,000 --> 00:26:05,600
많은 사람들이 "그녀"가 이 목록에 있다는 사실을 알아차렸으므로 해당 목록에는 "그가"가 있지만 "그녀는" 목록이 없다는 것을 의미합니다.

130
00:26:03,360 --> 00:26:07,679
이것이 그 예입니다.

131
00:26:05,600 --> 00:26:13,520
내가 언급한 편향은 이러한 목록이 텍스트 어휘의 큰 데이터 세트에서 생성되기 때문에 발생합니다.

132
00:26:11,440 --> 00:26:22,480
제작자는 대규모 언어 모음에서 가장 자주 사용되는 단어를 찾습니다.

133
00:26:19,760 --> 00:26:33,840
그들은 잘라낸 다음 그들이 가지고 있는 목록을 기반으로 무엇을 포함하거나 제외할지에 대한 몇 가지 결정을 내립니다. 그러면 여기에서 끝납니다.

134
00:26:29,919 --> 00:26:54,799
많은 대규모 언어 데이터 세트에서 남성을 더 많이 표현하기 때문에 불용어 목록에 "he's"가 있지만 "she's"가 아닌 이와 같은 상황이 발생합니다.

135
00:26:51,840 --> 00:27:07,679
언어로 모델링하거나 분석할 때 많은 결정을 내립니다. 실무자로서 우리는 우리의 특정 영역에 적합한 것이 무엇인지 결정해야 합니다.

136
00:27:05,279 --> 00:27:12,960
불용어 목록을 선택하는 경우에도 마찬가지입니다.

137
00:27:10,720 --> 00:27:21,760
Tidymodels에서는 레시피에 추가 단계를 추가하여 불용어를 제거하는 것과 같은 전처리 단계를 구현할 수 있습니다.

138
00:27:19,200 --> 00:27:35,360
먼저 사용할 변수를 지정한 다음 텍스트를 토큰화했습니다. 이제 다른 인수를 전달하지 않기 때문에 기본 단계만 사용하여 여기서 불용어를 제거합니다.

139
00:27:32,640 --> 00:27:42,399
기본이 아닌 단계를 사용하거나 도메인에 가장 적합한 경우 사용자 지정 목록을 사용할 수도 있습니다.

140
00:27:42,880 --> 00:28:02,080
이 도표는 대법원 의견의 동일한 데이터 세트의 연도를 서로 다른 길이의 3가지 다른 불용어 사전을 사용하여 예측하기 위한 모델 성능을 비교합니다.

141
00:27:59,279 --> 00:28:06,960
눈덩이 사전에는 가장 적은 수의 단어가 포함되어 있습니다.

142
00:28:04,480 --> 00:28:10,320
이 경우 최상의 성능을 제공합니다.

143
00:28:06,960 --> 00:28:16,399
여기서 더 적은 수의 불용어를 제거하면 최상의 성능을 얻을 수 있습니다.

144
00:28:12,799 --> 00:28:29,919
이 특정 결과를 모든 데이터 세트와 컨텍스트에 일반화할 수는 없지만 다른 불용어 세트를 제거하면 모델에 상당히 다른 영향을 미칠 수 있으며 이는 상당히 이전할 수 있습니다.

145
00:28:26,799 --> 00:28:41,360
최선의 방법이 무엇인지 아는 유일한 방법은 여러 옵션을 시도하고 일반적으로 기계 학습을 확인하는 것입니다.

146
00:28:39,279 --> 00:28:49,919
이것은 경험적 분야입니다. 우리는 무엇을 하는 것이 가장 좋을지 사전에 알 수 없고 이유가 없는 경우가 많습니다.

147
00:28:47,440 --> 00:29:01,039
일반적으로 우리는 가장 좋은 것이 무엇인지 확인하기 위해 다른 옵션을 시도해야 합니다.

148
00:28:58,799 --> 00:29:07,760
텍스트에 대해 이야기하고 싶은 세 번째 전처리 단계는 "어간 추출"입니다.

149
00:29:05,279 --> 00:29:18,559
텍스트를 다룰 때 종종 문서에는 "줄기"라고 하는 하나의 기본 단어의 다른 버전이 포함되어 있습니다.

150
00:29:15,440 --> 00:29:31,279
동물의 복수형과 동물의 단수형의 차이에 관심이 없고 둘 다 함께 취급하고 싶다면.

151
00:29:27,919 --> 00:29:33,200
그 아이디어가 스테밍의 핵심입니다.

152
00:29:31,279 --> 00:29:40,320
텍스트의 줄기를 만드는 올바른 방법이나 올바른 방법은 없습니다.

153
00:29:37,279 --> 00:29:44,880
이 플롯은 영어 형태소 분석에 대한 세 가지 접근 방식을 보여줍니다.

154
00:29:42,480 --> 00:29:57,520
복수형 처리에 대한 더 복잡한 규칙에 대해 마지막 "s"를 제거하는 것부터 시작하겠습니다. 그 중간에 있는 것을 "스템머(stemmer)"라고 합니다.

155
00:29:54,480 --> 00:30:01,360
그것은 일종의 규칙의 집합과 같습니다.

156
00:29:58,640 --> 00:30:09,919
마지막 것은 Porter 알고리즘이라고 하는 영어 형태소 분석의 가장 잘 알려진 구현일 것입니다.

157
00:30:07,279 --> 00:30:19,840
여기에서 Porter 형태소 분석이 제가 사용한 동물 설명 데이터 세트에서 상위 20개 단어의 다른 두 단어와 가장 다르다는 것을 알 수 있습니다.

158
00:30:16,799 --> 00:30:25,600
우리는 종이라는 단어가 어떻게 다르게 취급되었는지 봅니다.

159
00:30:24,240 --> 00:30:40,559
동물 육식 동물은 "살다", "살아있는", "생명", "생명"이라는 단어 모음을 다르게 취급하므로 실무자는 일반적으로 텍스트 데이터의 형태소 분석에 관심이 있습니다.

160
00:30:38,960 --> 00:30:53,360
우리가 언어의 인간 사용자로서 이해하는 방식으로 함께 속한다고 믿는 토큰을 함께 버킷화하기 때문입니다.

161
00:30:51,600 --> 00:31:09,600
우리는 이와 같은 접근 방식을 사용할 수 있습니다. 이는 일반적으로 형태소 분석(stemming)이라고 하는 단계별 규칙과 매우 유사합니다.

162
00:31:07,039 --> 00:31:14,720
아니면 먼저 이것을 하고 나서 이것을 하고 나서 이것을 하는 것과 같이 본질적으로 상당히 알고리즘적입니다.

163
00:31:12,080 --> 00:31:31,120
또는 일반적으로 단어의 큰 사전을 기반으로 하는 표제어 분류를 사용할 수 있으며 이는 함께 속하는 단어에 대한 언어적 이해를 통합합니다.

164
00:31:28,559 --> 00:31:47,600
이러한 종류의 한국어 작업에 대한 기존 접근 방식의 대부분은 이러한 사전을 기반으로 하고 대규모 언어 데이터 세트를 사용하여 훈련된 제한된 표제어입니다.

165
00:31:45,679 --> 00:32:04,080
이것은 텍스트 데이터의 경우 일반적으로 많은 토큰이 있는 기능에 압도되기 때문에 이 말을 들을 때 도움이 될 것 같습니다.

166
00:32:02,320 --> 00:32:09,440
이것은 일반적으로 텍스트 데이터를 다룰 때의 상황입니다.

167
00:32:05,840 --> 00:32:18,240
여기에 우리는 이러한 동물 설명 데이터를 가지고 있고 나는 그것을 행렬로 표현했습니다.

168
00:32:16,080 --> 00:32:31,600
일부 기계 학습 알고리즘에서 일반적으로 사용하는 것처럼 기능이 얼마나 많은지 보십시오. 16,000개 거의 17,000개의 기능이 있습니다.

169
00:32:29,679 --> 00:32:36,640
그것은 모델에 들어갈 기능의 수입니다.

170
00:32:33,519 --> 00:32:53,760
98% 희소성(sparse)을 보세요. 매우 희박한 데이터입니다. 이것은 지도 머신 러닝 모델을 구축하기 위해 머신 러닝 알고리즘에 들어갈 데이터의 희소성입니다.

171
00:32:51,600 --> 00:33:02,240
단어를 어간으로 하고 여기서 어간법을 사용하면 단어 특징의 수를 수천 배로 줄일 수 있습니다.

172
00:33:00,159 --> 00:33:16,880
희소성은 불행히도 많이 변경되지 않았지만, 우리의 형태소 분석 알고리즘이 함께 속하는 단어를 함께 버킷화하여 기능의 수를 많이 줄였습니다.

173
00:33:15,200 --> 00:33:29,039
상식에 따르면 단어 기능의 수를 줄이는 것이 기계 학습 모델의 성능을 향상시킬 것입니다.

174
00:33:26,640 --> 00:33:51,039
그러나 그것은 우리가 형태소 분석을 통해 중요한 정보를 잃지 않았다고 가정하고 형태소 분석이나 표제어형 분류가 일부 상황에서는 종종 매우 도움이 될 수 있지만 여기에 사용되는 일반적인 알고리즘은 다소 공격적이라는 것이 밝혀졌습니다.

175
00:33:48,320 --> 00:33:59,279
그들은 민감도, 회상 또는 참 양성률을 선호하도록 제작되었습니다.

176
00:33:56,880 --> 00:34:07,200
이것은 특이성 또는 정밀도 또는 진음성 비율을 희생합니다.

177
00:34:04,640 --> 00:34:25,760
지도 머신 러닝 컨텍스트에서 이것이 하는 일은 모델의 양의 예측 값에 영향을 미치거나 참 부정을 긍정으로 잘못 지정하지 않는 능력에 영향을 미칩니다. 내가 맞았으면 좋겠어!

178
00:34:22,800 --> 00:34:43,200
이것을 보다 구체적으로 만들기 위해, 형태소 분석은 우리가 모델링하는 경우 특정 식단과 관련된 동물 설명과 같은 긍정적인 예를 찾는 모델의 능력을 증가시킬 수 있습니다.

179
00:34:40,720 --> 00:34:52,240
그러나 텍스트가 지나치게 요약되면 결과 모델은 부정적인 예에 레이블을 지정할 수 있는 능력을 잃게 됩니다.

180
00:34:49,760 --> 00:35:08,400
이러한 형태소 분석 알고리즘에서 변경할 수 있는 다이얼이 없는 경우가 많기 때문에 텍스트 데이터를 사용하여 모델을 훈련할 때 균형이 맞는지 확인하는 것이 매우 어려울 수 있습니다.

181
00:35:05,119 --> 00:35:17,440
이 기능 엔지니어링 레시피에서 보여 주는 것과 같은 텍스트에 대한 아주 기본적인 사전 처리조차도 계산 비용이 많이 들 수 있습니다.

182
00:35:15,280 --> 00:35:42,560
불용어를 제거할지 또는 텍스트를 줄기할지 여부와 같이 실무자가 내리는 선택은 모든 종류의 기계 학습 모델이 수행하는 방식에 극적인 영향을 미칠 수 있습니다. 더 단순한 모델이든 더 전통적인 기계 학습 모델이든 딥 러닝 모델이든.

183
00:35:39,040 --> 00:35:58,560
이것이 의미하는 바는 우리가 실무자로서 텍스트에 대한 피쳐 엔지니어링 단계에 대한 학습 교육 및 쓰기에 부여하는 가격 우선 순위가 실제로 우리 분야에서 더 강력한 통계 관행에 기여한다는 것입니다.

184
00:35:56,320 --> 00:36:14,320
앞에서 텍스트 데이터의 희소성을 언급했는데 텍스트 데이터의 진정한 정의 특성 중 하나이기 때문에 다시 그 점으로 돌아가고 싶습니다.

185
00:36:10,400 --> 00:36:25,119
언어가 작동하는 방식 때문에 우리는 몇 단어를 여러 번 사용하고 많은 단어를 몇 번만 사용합니다.

186
00:36:22,640 --> 00:36:27,520
실제 자연어 세트를 사용하면 다음과 같은 관계가 생깁니다.

187
00:36:25,119 --> 00:36:38,320
말뭉치에 더 많은 문서와 더 많은 고유한 단어를 추가함에 따라 희소성이 어떻게 변하는지에 대한 이러한 플롯과 같습니다.

188
00:36:35,680 --> 00:36:53,520
고유한 단어를 더 추가하면 희소성이 정말 빠르게 높아집니다. 이 문서 세트를 처리하는 데 필요한 메모리가 매우 빠르게 증가합니다.

189
00:36:51,920 --> 00:37:13,119
희소 행렬과 같은 희소 데이터를 저장하기 위한 특수 데이터 구조를 사용하더라도 이러한 데이터 세트를 매우 비선형적인 방식으로 처리하는 데 필요한 메모리가 계속 증가하게 됩니다. 여전히 매우 빠르게 자랍니다.

190
00:37:09,920 --> 00:37:23,839
즉, 모델을 훈련하는 데 매우 오랜 시간이 걸리거나 컴퓨터에서 사용 가능한 메모리가 초과될 수도 있습니다.

191
00:37:21,760 --> 00:37:31,200
비용이 많이 드는 대용량 메모리 상황에서 클라우드로 이동해야 하는 상황은 이것이 진짜 도전이 될 수 있습니다.

192
00:37:27,680 --> 00:37:43,520
이 도전은 모델에 대한 벡터 언어의 동기 부여 뒤에 있는 것입니다.

193
00:37:40,240 --> 00:37:58,160
언어학자들은 사람들이 언어를 사용하는 방식에 따라 텍스트 데이터를 나타내는 차원 수를 줄일 수 있는 모델용 벡터 언어에 대해 오랫동안 연구해 왔습니다.

194
00:37:55,280 --> 00:38:04,960
이 인용문은 1957년으로 거슬러 올라갑니다.

195
00:38:02,960 --> 00:38:15,440
여기서 아이디어는 데이터가 매우 희박한 것처럼 사용하지만 단어를 무작위로 사용하지 않는다는 것입니다.

196
00:38:12,800 --> 00:38:17,359
독립적이지 않습니다.

197
00:38:15,440 --> 00:38:25,920
단어는 서로 독립적으로 사용되는 것이 아니라 단어가 함께 사용되는 방식 사이에 존재하는 관계가 있습니다.

198
00:38:23,359 --> 00:38:40,160
우리는 이러한 관계를 사용하여 희소한 고차원 공간을 특수한 조밀한 저차원 공간으로 변환하기 위해 생성할 수 있습니다.

199
00:38:37,359 --> 00:38:49,839
우리는 여전히 100개의 차원을 가지고 있지만 수천, 수백, 수만, 수십만 개의 공간보다 훨씬 낮습니다.

200
00:38:48,160 --> 00:39:05,839
여기서 우리는 통계적 모델링을 사용합니다. 단어 수와 행렬 분해를 더한 것인데, 이 정말 고차원 공간을 차지하기 위해 신경망을 포함하는 더 멋진 수학일 수도 있습니다.

201
00:39:03,680 --> 00:39:21,839
어떤 단어가 함께 사용되는지에 대한 정보를 포함하는 벡터를 기반으로 새로운 공간이 생성되기 때문에 특별한 새로운 저차원 저차원 공간을 생성합니다.

202
00:39:18,560 --> 00:39:26,480
당신은 그것이 유지하는 회사에 의해 단어를 알 것입니다.

203
00:39:24,079 --> 00:39:35,119
이러한 종류의 단어 벡터 또는 단어 임베딩을 만들거나 배우려면 큰 데이터 세트의 텍스트가 필요합니다.

204
00:39:32,000 --> 00:39:53,839
지금 보여드리는 이 표는 데이터 세트나 미국 소비자 금융 보호국에 대한 불만 사항을 사용하여 만든 임베딩 세트에서 가져온 것입니다.

205
00:39:51,680 --> 00:40:13,119
이것은 사람들이 불만을 제기하고 신용 카드 모기지 학자금 대출과 같은 금융 상품, 금융 상품과 같은 금융 상품과 관련된 문제를 말할 수 있는 미국의 정부 기관입니다.

206
00:40:11,119 --> 00:40:24,000
그들은 내 신용 카드에 문제가 발생한 것과 같습니다. 회사가 공정하지 않아 내 담보 대출에 문제가 발생하여 불만을 제기하는 것입니다.

207
00:40:20,000 --> 00:40:27,839
나는 그 모든 불만을 받아들이고 모델을 만들었습니다.

208
00:40:25,440 --> 00:40:32,400
그것은 우리의 고차원 공간이고 저차원 공간을 구축합니다.

209
00:40:30,160 --> 00:40:44,079
우리는 그 공간을 보고 이 공간에서 어떤 단어들이 서로 관련되어 있는지 이해할 수 있습니다.

210
00:40:40,240 --> 00:41:09,680
임베딩으로 정의된 새로운 공간에서 "월"이라는 단어는 년 월 복수 월부금과 같은 단어에 가장 가깝기 때문에 다음으로 정의된 새로운 공간에서 신용 카드 또는 모기지와 같은 금융 상품의 맥락에서 의미가 있는 단어입니다. 이러한 임베딩.

211
00:41:06,720 --> 00:41:26,240
"오류"라는 단어는 사무 오류 문제 오류 또는 내 모기지 명세서에 오류가 있었던 것과 같은 사무 오류와 같은 단어에 가장 가깝습니다.

212
00:41:23,440 --> 00:41:30,880
우리는 이러한 종류의 오해나 오해를 봅니다.

213
00:41:28,319 --> 00:41:33,599
이 단어는 비슷한 방식으로 사용됩니다.

214
00:41:31,599 --> 00:41:42,240
임베딩을 만드는 데 많은 데이터가 필요하기 때문에 임베딩을 직접 만들 필요가 없습니다.

215
00:41:39,520 --> 00:41:54,079
사전 훈련된 단어 임베딩을 사용할 수 있습니다. 즉, 다른 사람이 액세스할 수 있지만 귀하는 액세스할 수 없는 방대한 데이터를 기반으로 다른 사람이 생성한 것입니다.

216
00:41:51,359 --> 00:41:55,599
데이터 세트 중 하나를 살펴보겠습니다.

217
00:41:54,079 --> 00:42:04,560
이 표는 동일한 단어 오류이지만 장갑 임베딩에 대한 결과를 보여줍니다.

218
00:42:02,800 --> 00:42:27,280
글러브 임베딩은 모든 wikipedia, 모든 Google 뉴스 데이터 세트와 같은 매우 큰 데이터 세트를 기반으로 생성되는 사전 훈련된 임베딩 세트입니다. .

219
00:42:24,800 --> 00:42:40,839
여기에서 가장 가까운 단어 중 일부는 이전 단어와 유사하지만 더 이상 사무 불일치와 같은 해당 도메인 특유의 풍미가 없습니다.

220
00:42:38,640 --> 00:42:54,319
이제 우리는 당신이 알고 있는 것과 같은 잘못된 의사 소통을 가지고 있지만 이제 우리는 사람들이 금융 상품 불만에 대해 이야기하지 않은 계산과 확률을 가지고 있습니다.

221
00:42:51,599 --> 00:43:10,400
이것은 우리가 우리 자신을 만들고 이 컨텍스트에 특정한 관계를 배울 수 있기 전에 여기에서 어떻게 작동하는지 강조합니다.

222
00:43:06,800 --> 00:43:16,720
여기서 우리는 다른 곳에서 배운 보다 일반적인 집합으로 이동합니다.

223
00:43:13,119 --> 00:43:27,359
임베딩은 방대한 텍스트 데이터 코퍼스에서 훈련되거나 학습되며 해당 코퍼스의 특성은 임베딩의 일부가 됩니다.

224
00:43:24,240 --> 00:43:35,839
일반적으로 기계 학습은 훈련 데이터에 있는 내용에 매우 민감합니다.

225
00:43:32,240 --> 00:43:40,319
이것은 텍스트 데이터를 다룰 때보다 더 명확하지 않습니다.

226
00:43:38,160 --> 00:44:01,440
아마도 단어 임베딩은 이러한 고전적인 예 중 하나와 같을 것입니다. 이것이 사실인 경우 이는 말뭉치에 있는 인간의 편견이나 편견이 임베딩에 각인되는 방식에서 나타납니다.

227
00:43:59,040 --> 00:44:14,640
실제로 가장 일반적으로 사용 가능한 임베딩 중 일부를 볼 때 편향이 있습니다.

228
00:44:12,200 --> 00:44:23,280
우리는 미국에서 아프리카계 미국인에게 더 흔한 아프리카계 미국인 이름을 봅니다.

229
00:44:19,680 --> 00:44:30,240
그들은 이러한 임베딩 공간에서 유럽계 미국인 이름보다 더 불쾌한 감정과 관련이 있습니다.

230
00:44:27,680 --> 00:44:38,640
여성의 이름은 가족과 더 관련이 있고 남성의 이름은 경력과 더 관련이 있습니다.

231
00:44:36,079 --> 00:44:44,079
여성과 관련된 용어는 예술과 더 관련되고 남성과 관련된 용어는 과학과 더 관련이 있습니다.

232
00:44:42,640 --> 00:45:05,040
실제로 편향은 단어 임베딩에 너무 깊이 뿌리박혀 있어서 단어 임베딩 자체를 사용하여 시간이 지남에 따라 사회적 태도의 변화를 정량화할 수 있습니다.

233
00:45:01,440 --> 00:45:11,119
단어 임베딩은 과장되거나 극단적인 예일 수 있습니다.

234
00:45:08,160 --> 00:45:31,599
그러나 텍스트 데이터와 관련하여 우리가 내리는 모든 기능 엔지니어링 결정은 우리가 보는 모델 성능과 모델이 얼마나 적절하거나 공정한지 모두에서 결과에 상당한 영향을 미친다는 것이 밝혀졌습니다.

235
00:45:29,520 --> 00:45:43,520
필요한 이러한 기능을 생성하는 텍스트 데이터를 사전 처리하는 것과 관련하여 많은 옵션과 상당한 책임이 있습니다.

236
00:45:39,680 --> 00:45:59,040
내 조언은 항상 매우 깊이 이해할 수 있는 더 간단한 모델로 시작하고 모델을 훈련하고 조정할 때 좋은 통계 방법을 채택해야 한다는 것입니다.

237
00:45:56,720 --> 00:46:10,400
다른 접근 방식을 시도할 때 모델 성능 향상에 대해 속지 않습니다.

238
00:46:07,119 --> 00:46:20,240
모델 설명 도구 및 프레임워크를 사용하여 시도하는 덜 간단한 모델을 이해할 수 있습니다.

239
00:46:18,000 --> 00:46:30,319
제 동료와 저는 이 모든 주제와 Tidymodels와 함께 사용하는 방법에 대해 썼습니다.

240
00:46:28,720 --> 00:46:38,960
그것으로 나는 너무너무 감사하다는 말을 하고 다시 확신하고 싶습니다.

241
00:46:36,560 --> 00:46:45,119
한국의 R 유저 그룹 주최측에 감사드립니다.

242
00:46:42,560 --> 00:46:51,240
Rstudio의 Tidymodels 팀 동료와 공동 저자인 EMIL HVITFELDT에게 감사드립니다.
