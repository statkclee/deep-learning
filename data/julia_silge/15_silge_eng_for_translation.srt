1
00:00:01,120 --> 00:00:08,639
Hi my name is julia silge.
I'm a data scientist and software engineer at RStudio.

2
00:00:06,080 --> 00:00:18,080
I'd like to thank the the organizers of the R User Group in Korea. And thank so much for having me.

3
00:00:15,519 --> 00:00:29,359
I am so happy to be speaking specifically today about creating features for machine learning from text data for a couple of reasons.

4
00:00:27,199 --> 00:00:42,879
Having a better understanding of what we do to take text data and then to make it appropriate as an input for machine learning algorithms has many benefits

5
00:00:39,680 --> 00:00:50,719
both if you are directly getting ready to train a model or if you're at the beginning of some text analysis project

6
00:00:48,399 --> 00:00:55,440
or if you are trying to understand the behavior of a model that

7
00:00:52,800 --> 00:01:06,640
you're interacting with some way which is something that we do in our work as data scientists or in our in our daily lives more and more.

8
00:01:03,359 --> 00:01:15,759
so when we build models for text either supervised or unsupervised we start with something like this.

9
00:01:11,600 --> 00:01:24,560
this is some example text data that I'll use a couple of times during this talk that describes some animals.

10
00:01:21,040 --> 00:01:34,400
I'm using some text data so to me as an english speaker looks familiar.

11
00:01:30,960 --> 00:01:43,920
I am as someone who uses a human language so I look at this and I can read it I could speak it aloud and I understand.

12
00:01:41,280 --> 00:01:45,759
I can interpret it what it means

13
00:01:43,920 --> 00:01:59,840
so this kind of data this sort of natural language data is being generated all the time in all kinds of languages in all kinds of contexts.

14
00:01:56,880 --> 00:02:26,720
whether you work in healthcare in tech in finance basically any kind of organization this sort of text data is being generated by customers by clients by internal stakeholders inside of a business by people taking surveys via social media via business processes.

15
00:02:24,319 --> 00:02:35,360
in all this natural language there's information latent in that text data that can be used to make better decisions.

16
00:02:33,519 --> 00:02:47,440
However, computers are not great at looking at this and doing math on language as it's represented like this.

17
00:02:45,440 --> 00:02:55,440
instead language has to be dramatically transformed to some kind of machine readable numeric representation.

18
00:02:53,680 --> 00:03:01,840
that looks more like this what I'm showing here on the screen to be ready for almost any kind of model.

19
00:02:59,760 --> 00:03:22,159
so I spent a fair amount of time working on software for people to be able to do exploratory data analysis, visualization, summarization tasks like that with text data in a tidy format where we have one observation per row.

20
00:03:19,040 --> 00:03:34,879
I love using tidy data principles for text analysis especially during those exploratory phases of an analysis when it comes time to build a model.

21
00:03:32,640 --> 00:03:52,319
often what the underlying mathematical implementation really needs is typically something like this which is a way to this particular representation is called the document term matrix.

22
00:03:49,760 --> 00:03:55,760
so the exact representation may differ from what I've shown here.

23
00:03:53,840 --> 00:04:04,720
what I have here is we're weighting things by counts so each row in this matrix is a document.

24
00:04:02,640 --> 00:04:07,439
each column is a is a word.

25
00:04:04,720 --> 00:04:19,359
A token and the numbers represent counts how many times does each document, use
each word you could weight it in a different way, or use TF-IDF instead of counts.

26
00:04:16,720 --> 00:04:41,120
or you might keep sequence information if you're interested in building a deep learning model but basically for all kinds of text modeling from simpler models like Naive Bayes models which work well for text to word embeddings to really the most state-of-the-art kind of work that's happening today like transformers for text data.

27
00:04:39,520 --> 00:04:53,680
we have to heavily feature engineer and process language to get it to some kind of representation. that's suitable for machine learning algorithms.

28
00:04:50,960 --> 00:05:04,800
so I work on an open source framework in R for modeling and machine learning that's called Tidymodels and the examples that I'll be showing today use Tidymodels code.

29
00:05:02,320 --> 00:05:21,360
some of the specific goals of the Tidymodels project are to provide a consistent flexible framework for real world modeling practitioners people who are you know doing that are dealing with real world data.

30
00:05:19,199 --> 00:05:37,360
those who are just starting out to those who are very experienced in modeling and the goal is to harmonize the heterogeneous interfaces that exist within R and to encourage good statistical practice.

31
00:05:35,360 --> 00:05:52,800
I'm glad to get to show you some of what I work on and build and how we apply it to text modeling but a lot of what I will talk about today isn't very specific to Tidymodels or even to R.

32
00:05:50,880 --> 00:06:00,479
I know this is an R user group but what we're going to talk
about and focus on is a little more conceptual and basic thing.

33
00:05:57,440 --> 00:06:06,800
how do we transform text into predictors for machine learning?

34
00:06:04,560 --> 00:06:19,720
I am excited though to talk about Tidymodels and Tidymodels if you have not used it before is a meta package in a similar way that the Tidyverse is a meta package.

35
00:06:16,319 --> 00:06:33,039
so if you've ever typed library Tidyverse and then you've used ggplot2 for visualization, dplyr for data manipulation, Tidymodels works in a similar way.

36
00:06:30,560 --> 00:06:39,840
there are different packages inside of it that are used for different purposes.

37
00:06:37,280 --> 00:06:48,000
so the pre-processing or the feature engineering is part of a broader model process.

38
00:06:45,280 --> 00:06:56,240
that process starts really with with exploratory data analysis that helps us decide what kind of model.

39
00:06:54,080 --> 00:07:03,120
we will build and then it comes to completion.

40
00:06:59,280 --> 00:07:09,280
I think I would argue with model evaluation when you measure how well your model performed.

41
00:07:06,479 --> 00:07:38,560
Tidymodels as a piece of software is made up of our packages each of which has a specific focus like our sample is for re-sampling data to be able to create bootstrap resamples cross-validation resamples all different kinds of resamples you might want to use to train and evaluate models the tune package is for hyper parameter tuning.

42
00:07:34,800 --> 00:07:52,720
As you might guess from the name one of these packages is for feature engineering for a data preprocessing feature engineering and it is the one that is called recipes.

43
00:07:49,360 --> 00:08:25,759
In Tidymodels we capture this idea of data pre-processing and feature engineering in the concept of a pre-processing recipe that has steps so you choose ingredients or variables that you're going to use, then you define the steps that go into your recipe, then you prepare them using training data, and then you can apply that to any data set like testing data or new data at prediction time.

44
00:08:23,039 --> 00:08:34,560
The variables or ingredients that we use in modeling come in all kinds of shapes and sizes including text data.

45
00:08:32,000 --> 00:09:05,279
Some of the techniques and approaches that we use for pre-processing text data are the same as for any other kind of data that you might use like non-text data, numeric data,  categorical data.
some of it is the same but some of what you need to know to be able to do a good job in this process for text is different and is specific to the nature of what language data is like.

46
00:09:03,680 --> 00:09:14,720
I've written a book with my co-author Emile Hvitfeldt
on "supervised machine learning for text analysis and R".

47
00:09:11,360 --> 00:09:25,920
Fully the first third of the book focuses on how we transform the natural language that we have in text data into features for modeling.

48
00:09:23,600 --> 00:09:40,240
The middle section is about how we use these features in simpler or more traditional
machine learning models like regularized regression or support vector machines.

49
00:09:37,360 --> 00:10:15,279
Then the last third of the book talks about how we use deep learning models with text data so deep learning models still require these kinds of transformations from natural language into features as input for these kinds of models but deep learning models are often able to inherently learn structure of features from text in ways that those more traditional or simpler machine learning models are not.

50
00:10:12,800 --> 00:10:20,560
This book is now complete and available as of this month as of november.

51
00:10:18,480 --> 00:10:30,720
Folks are getting their first paper copies and also this book is available in its entirety at smalltar.com.

52
00:10:28,560 --> 00:10:43,839
If you're new to dealing with text data understanding these fundamental pre-processing
approaches for text will set you up for being able to train effective models.

53
00:10:41,760 --> 00:10:58,000
if you're really experienced with text data and if you've dealt with it a lot, already you've probably noticed the existing resources or literature.

54
00:10:54,480 --> 00:11:17,519
Whether that's books or tutorials or blog posts is quite sparse when it comes to detailed thoughtful explorations of
how these pre-processing steps work and how choices made in these feature engineering steps impact our model output.

55
00:11:15,600 --> 00:11:28,800
so let's walk through several of some of these like basic feature engineering approaches and how they work and what they do.

56
00:11:27,040 --> 00:11:31,920
Let's start out with tokenization.

57
00:11:28,800 --> 00:11:52,320
Typically one of the first steps in transfer information from natural language to machine learning
feature for really any kind of text analysis including exploratory data analysis or building a model.

58
00:11:49,040 --> 00:11:55,360
Anything is tokenization.

59
00:11:52,320 --> 00:12:02,079
In tokenization we take an input some string, some character vector and some kind of token type some meaningful unit of text.

60
00:11:59,440 --> 00:12:12,480
We're interested in a word and we split the input pieces into tokens that correspond to the type we're interested in.

61
00:12:09,519 --> 00:12:19,040
Most commonly the meaningful unit or type of token
that we want to split text into units of is a word.

62
00:12:17,839 --> 00:12:30,880
This might seem straightforward or obvious but it turns out it's difficult to clearly define what a word is for many or even most languages.

63
00:12:29,519 --> 00:12:40,959
Many languages do not use white space between words at all which presents a challenge for tokenization.

64
00:12:38,240 --> 00:13:04,079
Even languages that do use white space like English and Korean often have particular examples that are ambiguous
like contractions in English or more accurately considered two words the way particles are used in Korean.

65
00:13:01,519 --> 00:13:13,839
How pronouns and negation words are written in romance languages like italian and french where they're stuck together and really maybe they should be considered two words.

66
00:13:12,399 --> 00:13:32,720
Once you have figured out what you're going to do and you make some choices and you tokenize your text, then it's on its way to being able to be used in exploratory data analysis or unsupervised algorithms or as features for predictive modeling which is what we're talking about here.

67
00:13:30,320 --> 00:13:45,199
What these results show here, so these results are from a regression model trained on descriptions of media from artwork in the Tate collection in the UK.

68
00:13:43,279 --> 00:14:02,079
What we're predicting in what we are predicting is when what year was a piece of art created based on the the medium that the artwork was created with and the medium is described with a little bit of text.

69
00:14:00,079 --> 00:14:29,920
We see here that artwork created using graphite watercolor and engraving was more likely to be created earlier that though. That is more likely to come from older art and artwork that is created using photography screen point or sorry screen print screen printing and and dung and glitter are more likely to be created later.

70
00:14:27,760 --> 00:14:37,519
There this is more likely to come from contemporary art modern art.

71
00:14:33,440 --> 00:15:17,279
The the way that we tokenize this text we started with natural human generated texts of people writing out the descriptions of the the media that these art pieces of art were created with and the way we tokenized that natural human generated text that we started with has a big impact on what we learned from it. If we tokenized in a different way we would have gotten different results in terms of performance like how accurately we were able to predict predict the year and also in terms of how we interpret the model like what is it that we're able to learn from it.

72
00:15:15,279 --> 00:15:31,279
This is one kind of tokenization to the single word, but also all another way to tokenize instead of breaking up into single words or unigrams we can tokenize to n-grams.

73
00:15:27,440 --> 00:15:37,920
An n-gram is a continuous sequence of N items from a given sequence of texts.

74
00:15:35,199 --> 00:15:54,959
This shows that same piece of little bit of text i'm describing this animal divided up into
bi-grams or n-grams of two tokens so notice how the words in the bi-grams overlap.

75
00:15:51,519 --> 00:16:11,120
The word 'collard' appears in both of the first bigrams the 'collared', 'collared peccary' also
referred to so n-gram tokenization slides along the text to create overlapping sets of tokens.

76
00:16:07,040 --> 00:16:14,480
This shows tri-grams for the same thing.

77
00:16:11,120 --> 00:16:23,360
Using uni-grams one word is faster and more efficient but we don't capture information about word order.

78
00:16:20,560 --> 00:16:41,440
I'm using a higher value two or three or even more keeps more complex information about word order and concepts that are described in multi-word phrases.

79
00:16:37,680 --> 00:16:56,480
But the vector space of tokens increases dramatically that corresponds to a reduction in token counts.
We don't count each token as very many times and that means depending on your particular data set.

80
00:16:55,120 --> 00:17:00,399
You might not be able to get good results.

81
00:16:58,160 --> 00:17:14,079
Combining different degrees of n-grams can allow you to extract different levels of detail from text so uni-grams can tell you which individual words have been used a lot of times.

82
00:17:11,439 --> 00:17:23,360
Some of those words might be overlooked in bi-gram or tri-gram 'crowns' if they don't co-appear with other words as often.

83
00:17:23,520 --> 00:17:39,679
This plot compares model performance for a Lasso regression model predicting the year of supreme court opinions the United States supreme court opinions with three different degrees of n-grams.

84
00:17:37,840 --> 00:17:51,840
What we're doing here is we are taking the text of the writings of the United
States supreme court and we're predicting when did it when was that text written.

85
00:17:49,760 --> 00:17:58,799
Can we predict how old a piece of text is from the contents of the text?

86
00:17:55,360 --> 00:18:13,600
Holding the number of tokens constant at a thousand using uni-grams alone performs best for this corpus of opinions from the United States supreme court.

87
00:18:11,280 --> 00:18:25,760
This is not always the case depending on the kind of model you use the data set itself we might see the best performance combining uni-grams and bi-grams or maybe some other option.

88
00:18:23,679 --> 00:18:43,039
In this case if we wanted to incorporate some of that more complex information that we have in the
bi-grams and the tri-grams, we probably would need to increase the number of tokens in the model quite a bit.

89
00:18:40,160 --> 00:18:51,039
Keep in mind when you look at results like these that identifying n-grams is computationally expensive.

90
00:18:48,960 --> 00:19:00,640
This is especially compared to the amount of like a model the
improvement in model performance that we often see.

91
00:18:58,720 --> 00:19:15,840
If we see some modest improvement by adding in bigrams, it's important to keep in mind how much improvement relative to how long it takes to identify bi-grams and then train that model.

92
00:19:13,200 --> 00:19:27,120
For example, for this data set of supreme court opinions where we held the number
of tokens constant, so the model training had the same number of tokens in it.

93
00:19:25,919 --> 00:19:46,799
Using bi-grams plus uni-grams takes twice as long to train to do the feature engineering and the training than only uni-grams and adding in tri-grams as well takes almost five times as long as training on uni-grams alon.

94
00:19:44,320 --> 00:19:56,559
This is a computationally expensive thing to do going in the other direction.

95
00:19:53,760 --> 00:20:05,440
We can tokenize to units smaller than words so like these are what are called "Character Shingles".

96
00:20:03,280 --> 00:20:13,120
We take words the collared peccary and we can instead of looking at words we can go down and look at sub word information.

97
00:20:10,880 --> 00:20:20,880
There's multiple different ways to break words up into subwords that are appropriate for machine learning.

98
00:20:18,480 --> 00:20:31,360
Often these kinds of approaches or algorithms have the benefit of being able to encode unknown or new words at prediction time.

99
00:20:29,039 --> 00:20:41,200
When it's time to make a prediction on new data, it's not uncommon for there to be new vocabulary words at that time.

100
00:20:39,200 --> 00:20:49,360
If we didn't see them in the training data, what are we going to do about those new words?

101
00:20:45,600 --> 00:20:58,400
When we train using subword information, often we can handle those new words if we saw the subword in our training data set.

102
00:20:55,760 --> 00:21:13,360
Using this kind of subword information is a way to incorporate morphological sequences into our models of various kinds of this is something that applies to various languages not just english.

103
00:21:11,919 --> 00:21:22,960
These results are for a classification model with a data set of very short texts.

104
00:21:20,000 --> 00:21:27,120
It's just the names of post offices in the United States. It's  super short.

105
00:21:24,240 --> 00:21:41,360
The goal of the model was to predict the post office located in hawaii in the middle of the pacific ocean or it located in the rest of the united states.

106
00:21:38,159 --> 00:21:49,200
I created features for the model that are subwords of these post office names.

107
00:21:46,480 --> 00:22:00,240
We end up learning that the names that start with h and p or contain that ale sub word are more likely to be in hawaii.

108
00:21:57,440 --> 00:22:11,440
The subwords a and d and ri and ing are are more likely to come from the post office that are outside of hawaii.

109
00:22:09,360 --> 00:22:19,600
This is an example of how we tokenized differently and we're able to learn something new. We're able to learn something different.

110
00:22:18,559 --> 00:22:29,919
In Tidymodels we collect all these kinds of decisions about tokenization and code that looks like this.

111
00:22:26,880 --> 00:22:38,640
We start with a recipe that specifies what variables or ingredients that we'll use, and then we define these preprocessing steps.

112
00:22:35,679 --> 00:22:50,400
Even at this first and arguably simple and basic step the choices that we make affect our modeling results in a big way.

113
00:22:47,120 --> 00:22:56,240
The next pre-processing step that I want to talk about is stopwords.

114
00:22:53,600 --> 00:23:11,840
Once we have split text into tokens, we often find that not all words carry the same amount of information for a machine learning task.

115
00:23:09,520 --> 00:23:18,880
Common words that carry little or perhaps no meaningful information are called "stopwords".

116
00:23:16,080 --> 00:23:22,720
This is one of the stopword lists that's available for Korean.

117
00:23:20,400 --> 00:23:36,640
It's common advice and practice to say hey just remove these stopwords for a lot of natural language processing tasks.

118
00:23:35,039 --> 00:23:44,320
What I'm showing here is the entirety of one of the shorter english stopword lists that's used really broadly.

119
00:23:41,919 --> 00:24:00,400
It's words like "I", "me", "my" pronouns, conjunctions "and", "of", the", and "these" are very common words that are not considered super important.

120
00:23:57,679 --> 00:24:15,840
The decision though to just remove stopwords is often more involved and perhaps more fraught
than what you'll find reflected in a lot of resources that are out there.

121
00:24:12,000 --> 00:24:22,400
Almost all the time real world NLP practitioners use pre-made stopword lists.

122
00:24:20,000 --> 00:24:32,960
This plot visualizes set intersections for three common stopword lists in english in what is called an "upset plot".

123
00:24:31,200 --> 00:24:40,159
The three lists are called the "snowball", "smart", and "iso" list.

124
00:24:37,440 --> 00:24:51,760
You can see the the lengths of the list are represented by the length of the bars, and then we see the intersections which words are in common on these lists by the vertical bars.

125
00:24:48,799 --> 00:25:00,720
The lengths of the list are quite different, and also notice they don't all contain the same sets of words.

126
00:24:58,320 --> 00:25:20,480
The important thing to remember about stopword lexicons is that they are not created in some neutral perfect setting but instead they are they are context specific.

127
00:25:16,960 --> 00:25:28,559
They can be biased. Both of these things are true because they are lists created from large data sets of language.

128
00:25:27,039 --> 00:25:37,440
They reflect the characteristics of the data used in their creation.

129
00:25:34,000 --> 00:25:45,600
This is the ten words that are in the English language smart lexicon but not in the English snowball lexicon.

130
00:25:43,039 --> 00:25:52,240
Notice that they're all contractions but that's not because the snowball exchange doesn't include contractions.

131
00:25:50,000 --> 00:26:05,600
It has a lot of them also notice that it has that "she's" is on this list and so that means that that list has "he's", but it does not have the list "she's".

132
00:26:03,360 --> 00:26:07,679
This is an example of that.

133
00:26:05,600 --> 00:26:13,520
The bias I mentioned that occurs because these lists are created from large data sets of text lexicon.

134
00:26:11,440 --> 00:26:22,480
Creators look at the most frequent words in some big corpus of language.

135
00:26:19,760 --> 00:26:33,840
They make a cut off and then some decisions about what to include or exclude based on the list that they have and you end up here.

136
00:26:29,919 --> 00:26:54,799
Because in many large data sets of language you have more representation of men, you end up with a situation like this where a stopword list will have "he's" but not "she's".

137
00:26:51,840 --> 00:27:07,679
Many decisions when it comes to modeling or analysis with language. We as
practitioners have to decide what is appropriate for our particular domain.

138
00:27:05,279 --> 00:27:12,960
It turns out this is even true when it comes to picking a stopword list.

139
00:27:10,720 --> 00:27:21,760
In Tidymodels we can implement a pre-processing step like removing stopwords by adding an additional step to our recipe.

140
00:27:19,200 --> 00:27:35,360
First we specified what variables we would use, then we tokenized the text, and now we are removing stopwords here using just the default step since we are not passing in any other arguments.

141
00:27:32,640 --> 00:27:42,399
We could though use a non-default step or even a custom list if that was most appropriate to our domain.

142
00:27:42,880 --> 00:28:02,080
This plot compares the model performance for predicting the year of that same data set
of supreme court opinions with three different stopword lexicons of different lengths.

143
00:27:59,279 --> 00:28:06,960
The snowball lexicon contains the smallest number of words.

144
00:28:04,480 --> 00:28:10,320
In this case it results in the best performance.

145
00:28:06,960 --> 00:28:16,399
Removing fewer stopwords results in the best performance here.

146
00:28:12,799 --> 00:28:29,919
This specific result is not generalizable to all data sets and contexts, but the fact that removing
different sets of stopwords can have noticeably different effects on your model that is quite transferable.

147
00:28:26,799 --> 00:28:41,360
The only way to know what is the best thing to do is to try several options and see so machine learning in general.

148
00:28:39,279 --> 00:28:49,919
This is an empirical field. We don't know and we don't often have reasons a priori to know what will be the best thing to do.

149
00:28:47,440 --> 00:29:01,039
Typically we have to try a different option to see what will be the best thing all right.

150
00:28:58,799 --> 00:29:07,760
The third pre-processing step that I want to talk about for text is "stemming".

151
00:29:05,279 --> 00:29:18,559
When we deal with text, often documents contain different versions of one base word often called a "stem".

152
00:29:15,440 --> 00:29:31,279
If we aren't interested in the difference between animals plural
and animal singular and we want to treat them both together.

153
00:29:27,919 --> 00:29:33,200
That idea is at the heart of stemming.

154
00:29:31,279 --> 00:29:40,320
There's no one right way or correct way to stem text.

155
00:29:37,279 --> 00:29:44,880
This plot shows three approaches for stemming in English.

156
00:29:42,480 --> 00:29:57,520
Starting from hey let's just remove a final "s" to more complex rules about plural handling plural endings. That middle one, it is called the "s stemmer".

157
00:29:54,480 --> 00:30:01,360
It's a set of it's like a little set of rules.

158
00:29:58,640 --> 00:30:09,919
That last one is the one probably the best-known implementation of stemming in English called the Porter algorithm.

159
00:30:07,279 --> 00:30:19,840
You can see here that Porter stemming is the most different from the other two in the top 20 words here from the data set of animal descriptions that I've been using.

160
00:30:16,799 --> 00:30:25,600
We see how the word species was treated differently.

161
00:30:24,240 --> 00:30:40,559
Animal predator this sort of collection of words "live", "living", "life", "lives" that was treated differently so practitioners are typically interested in stemming text data.

162
00:30:38,960 --> 00:30:53,360
Because it buckets tokens together that we believe belong
together in in a way that we understand that as human users of language.

163
00:30:51,600 --> 00:31:09,600
We can use approaches like this, which are pretty like step-by-step rules based this is typically called stemming.

164
00:31:07,039 --> 00:31:14,720
Or it's fairly algorithmic in nature like first do this then do this then do this.

165
00:31:12,080 --> 00:31:31,120
Or you can use lemmatization which is usually based on large dictionaries of words and it incorporates like a linguistic understanding of what words belong together.

166
00:31:28,559 --> 00:31:47,600
Most of the existing approaches for this kind of task in Korean are limited lemmatizers based on these dictionaries and that are trained using large data sets of language.

167
00:31:45,679 --> 00:32:04,080
This seems like it's going to be a helpful thing to do when you hear about this you're like sounds good, sounds smart especially because with text data we are typically overwhelmed with features with numbers of tokens.

168
00:32:02,320 --> 00:32:09,440
This is typically the situation when we're dealing with text data.

169
00:32:05,840 --> 00:32:18,240
Here we have these animal description data and I made a matrix representation of it.

170
00:32:16,080 --> 00:32:31,600
Like we would typically use in some machine learning algorithm, look how many features there are. There are 16,000 almost 17,000 features.

171
00:32:29,679 --> 00:32:36,640
That's the number of features that would be going into the model.

172
00:32:33,519 --> 00:32:53,760
Look at the sparsity 98 percent sparse that's high very sparse data, so this is the sparsity of the data that will go into the machine learning algorithm to build our supervised machine learning model.

173
00:32:51,600 --> 00:33:02,240
if we stem the words, if I use here an approach for stemming, we reduce the number of word features by many thousands.

174
00:33:00,159 --> 00:33:16,880
The sparsity unfortunately did not change as much, but we reduced the number of features by a lot by bucketing those words together that our stemming algorithm belong together.

175
00:33:15,200 --> 00:33:29,039
Common sense says reducing the number of words features is going to perform improve the performance of our machine learning model.

176
00:33:26,640 --> 00:33:51,039
But that does assume that we have not lost any important information by by stemming and it turns out that stemming or lemmatization can often be very helpful in some contexts but the typical algorithms used for these are somewhat aggressive.

177
00:33:48,320 --> 00:33:59,279
They have been built to favor sensitivity or recall or the true positive rate.

178
00:33:56,880 --> 00:34:07,200
This is at the expense of the specificity or the precision or the true negative rate.

179
00:34:04,640 --> 00:34:25,760
In a supervised machine learning context what this does is this affects a model's positive predictive value the precision or its ability to not incorrectly label true negatives as positive. I hope I got that right!

180
00:34:22,800 --> 00:34:43,200
To make this more concrete, stemming can increase a model's ability to find the positive examples of
say the animal descriptions that are associated with, say a certain diet if that's what we're modeling.

181
00:34:40,720 --> 00:34:52,240
However if text is over stemmed, the resulting model loses its ability to label the negative examples, say the descriptions that are not about that diet that's what we're looking for.

182
00:34:49,760 --> 00:35:08,400
This can be a real challenge when training models with text data kind of finding that that balance
there because often we don't have a dial that we can change on these stemming algorithms.

183
00:35:05,119 --> 00:35:17,440
Even just very basic pre-processing for text like what I'm showing here in this feature engineering recipe can be computationally expensive.

184
00:35:15,280 --> 00:35:42,560
The choices that a practitioner makes like whether or not to remove stopwords or to stem text can have dramatic impact on how machine learning models of all kinds perform; whether those are simpler models more traditional machine learning models or deep learning models.

185
00:35:39,040 --> 00:35:58,560
What this means is that the price the prioritization that we, as practitioners, give to like learning teaching and writing about feature engineering steps for text really contributes to better more robust statistical practice in our field.

186
00:35:56,320 --> 00:36:14,320
I mentioned before the sparsity of text data and I want to come back to that because it is one of text data's really defining characteristics.

187
00:36:10,400 --> 00:36:25,119
Because of just how language works, we use a few words a lot of times, and then a lot of words only just a couple of times only a few a few times.

188
00:36:22,640 --> 00:36:27,520
With a real set of natural language, you end up with relationships that look like this.

189
00:36:25,119 --> 00:36:38,320
That look like these plots in terms of how the sparsity changes as you add more documents and more unique words to a corpus.

190
00:36:35,680 --> 00:36:53,520
The sparsity goes up real fast as you add more unique words. The memory that is required to handle this set of documents goes up very fast.

191
00:36:51,920 --> 00:37:13,119
Even if you use specialized data structures meant to store sparse data like sparse matrices, you still
end up growing the memory required to handle these data sets in a very non-linear way. It still grows up very fast.

192
00:37:09,920 --> 00:37:23,839
This means it can take a very long time to train your model or even that you outgrow the memory available on your machine.

193
00:37:21,760 --> 00:37:31,200
You have to go to the cloud to an expensive big memory situation this can be a real challenge.

194
00:37:27,680 --> 00:37:43,520
This challenge is what has behind the motivating of vector languages for models.

195
00:37:40,240 --> 00:37:58,160
Linguists have worked for a long time on vector languages for models that can reduce the number of dimensions representing text data based on how people use language.

196
00:37:55,280 --> 00:38:04,960
This quote here goes all the way back to 1957.

197
00:38:02,960 --> 00:38:15,440
The idea here is that we use like the data is very sparse, but we don't use words randomly.

198
00:38:12,800 --> 00:38:17,359
It's not independent.

199
00:38:15,440 --> 00:38:25,920
The words are not used independently of each other, but rather there's relationships that exist between how words are used together.

200
00:38:23,359 --> 00:38:40,160
We can use those relationships to create to transform our sparse high dimensional space into a special dense low dimensional space lower.

201
00:38:37,359 --> 00:38:49,839
We still has like 100 dimensions but much lower than the many thousands, hundreds, tens, hundreds of thousands of space.

202
00:38:48,160 --> 00:39:05,839
The idea here we use statistical modeling maybe just word counts plus matrix factorization maybe fancier math that involves neural networks to take this really high dimensional space.

203
00:39:03,680 --> 00:39:21,839
We create a new lower dimensional lower dimensional space that is special because the new space
is created based on vectors that incorporate information about which words are used together.

204
00:39:18,560 --> 00:39:26,480
You shall know a word by the company it keeps.

205
00:39:24,079 --> 00:39:35,119
You need a big data set of text to create or learn these kinds of word vectors or word embeddings.

206
00:39:32,000 --> 00:39:53,839
This table that I'm showing right now, it's from a set of embeddings that I created using a data set or a corpus of complaints to the United States consumer financial protection bureau.

207
00:39:51,680 --> 00:40:13,119
This is a government body in the United States where people can complain and say what is wrong with something to do with a financial product like a credit card a mortgage a student loan, something to do with like a financial product.

208
00:40:11,119 --> 00:40:24,000
They're like something went wrong with my credit card, something went wrong with my mortgage that company is not being fair so you come and you complain to it.

209
00:40:20,000 --> 00:40:27,839
I took all those complaints and built a model.

210
00:40:25,440 --> 00:40:32,400
It's our high dimensional space and build a low dimensional space.

211
00:40:30,160 --> 00:40:44,079
We can look in that space and understand what words are related to each other in this space.

212
00:40:40,240 --> 00:41:09,680
In the new space defined by the embeddings the word "month" is closest to words like year months plural monthly installments payment so these are words that are that makes sense in the context of financial products like credit cards or mortgages in the new space defined by these embeddings.

213
00:41:06,720 --> 00:41:26,240
The word "error" is closest to the words like mistake clerical like a clerical mistake problem glitch or there was a glitch on my mortgage statement.

214
00:41:23,440 --> 00:41:30,880
We see these kinds of miscommunication or misunderstanding.

215
00:41:28,319 --> 00:41:33,599
These words  are used in similar ways.

216
00:41:31,599 --> 00:41:42,240
You don't have to create embeddings yourself because it requires quite a lot of data to make them.

217
00:41:39,520 --> 00:41:54,079
You can use word embeddings that are pre-trained, i.e created by someone else based on some huge corpus of data that they have access to and you probably don't.

218
00:41:51,359 --> 00:41:55,599
Let's look at one of those data sets.

219
00:41:54,079 --> 00:42:04,560
This table shows the results for the same word error but for the glove embeddings.

220
00:42:02,800 --> 00:42:27,280
The glove embeddings are a set of pre-trained embeddings that are created based on a very large data set that's like all of wikipedia, all of the google news data set, just like huge swaths of the internet have been fed in to create these embeddings.

221
00:42:24,800 --> 00:42:40,839
Some of the closest words here are similar to those that are before but we no longer have some of that domain specific flavor like clerical discrepancy.

222
00:42:38,640 --> 00:42:54,319
Now we have like miscommunication you know but and now we have calculation and probability which people were not talking about with their financial product complaints.

223
00:42:51,599 --> 00:43:10,400
This really highlights how these work here before we we created our own and
we were able to learn relationships that were specific to this context.

224
00:43:06,800 --> 00:43:16,720
Here we go to a more general set that that was learned somewhere else.

225
00:43:13,119 --> 00:43:27,359
Embeddings are trained or learned from a large corpus of text data and the characteristics of that corpus become part of the embeddings.

226
00:43:24,240 --> 00:43:35,839
Machine learning in general is exquisitely sensitive to whatever it is that's in your training data.

227
00:43:32,240 --> 00:43:40,319
This is never more obvious than when dealing with text data.

228
00:43:38,160 --> 00:44:01,440
Perhaps word embeddings is just like one of these classic examples, where this is true it turns out
that this shows up in how any human prejudice or bias in the corpus becomes imprinted into the embeddings.

229
00:43:59,040 --> 00:44:14,640
In fact when we look at some of these most commonly available embeddings that are out, there is bias.

230
00:44:12,200 --> 00:44:23,280
We see that african-american first names that are more common for
african americans in the United States.

231
00:44:19,680 --> 00:44:30,240
They're associated with more unpleasant feelings than European American first names in these embedding spaces.

232
00:44:27,680 --> 00:44:38,640
Women's first names are more associated with family and men's first names are more associated with career.

233
00:44:36,079 --> 00:44:44,079
Terms associated with women are more associated with the arts and terms associated with men are more associated with science.

234
00:44:42,640 --> 00:45:05,040
It turns out actually bias is so ingrained in word embeddings that the word embeddings themselves can be used to quantify change in social attitudes over time.

235
00:45:01,440 --> 00:45:11,119
Word embeddings are maybe an exaggerated or extreme example.

236
00:45:08,160 --> 00:45:31,599
but it turns out that all the feature engineering decisions that we make when it comes to text data have a significant effect on our results both in terms of the model performance that we see and also in terms of how appropriate or fair our models are.

237
00:45:29,520 --> 00:45:43,520
Given all that when it comes to pre-processing your text data creating
these features that you need, you have a lot of options and quite a bit of responsibility.

238
00:45:39,680 --> 00:45:59,040
My advice is always start with simpler models that you can understand quite deeply, be sure to adopt  good statistical practices as you train and tune your models.

239
00:45:56,720 --> 00:46:10,400
You aren't fooled about model performance improvements when you try different approaches.

240
00:46:07,119 --> 00:46:20,240
By using model explainability tools and frameworks,  you can understand any less straightforward models that you try.

241
00:46:18,000 --> 00:46:30,319
My co-workers and I have written about all of these topics and how to use them with Tidymodels if that's what you like to use and we will continue to do.

242
00:46:28,720 --> 00:46:38,960
With that I will say thank you so very much and I want to be sure to again.

243
00:46:36,560 --> 00:46:45,119
Thank the organizers of the R User Group in Korea.

244
00:46:42,560 --> 00:46:51,240
I want to thank my teammates on the Tidymodels team at Rstudio as well as my co-author EMIL HVITFELDT.

